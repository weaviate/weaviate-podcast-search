[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone. Thank you so much for checking out the Weaviate podcast. I'm super excited to welcome Jonathan Frankle back to the Weaviate podcast. Jonathan is one of the co-founders of MosaicML, a company that is just doing amazing work in the space of making deep learning training more efficient, faster, cheaper. And they've recently produced an estimate of language model costs across all these different sizes and price points and just overall helping people get a better understanding of what it takes to have your own large language model. And I've been so interested in large language models and thinking particularly with Weaviate and the role that they might have in search. So I'm so excited to welcome Jonathan back to the podcast and talk about what he's been up to at MosaicML. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Thank you so much for having me. It's always a pleasure to get to talk to you and I can't wait to talk about this new project. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So yeah, can we dive right into it? So what's the latest update with the Mosaic ML cloud and training large language models? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Definitely. So just to give you a sense of what we do at Mosaic very briefly, our goal is to train models efficiently, just in general. That's what we want to do for everybody. And there are a lot of ways of making it possible to train a model efficiently. And obviously a lot of models that people want to train efficiently. So I think the last time we chatted, we just released Composer, which is our open source library with all of our efficiency methods built in. And I think we had shortly thereafter released our ResNet recipe where we'd gotten like a 7x speed up in training ResNet50 on ImageNet over the standard NVIDIA baselines, which I'm kind of jealous. I'm not seeing a bunch of people doing lottery ticket experiments I humanly couldn't do because it was taking too long to train. And now people are able to do it overnight and I'm super jealous that people get to do that work now. But our latest release is for large language models for GPT-3 type models specifically. And it's been a lot of work on our part, but we've put together our own software stack for training these models. We've put together underneath that an orchestration stack, which is part of our Mosaic cloud that I'm sure we'll talk about a little bit. And the end result is we put out some prices. You want to train GPT-3 today, you can call me today. It's $450,000 to get to GPT-3 quality. We can talk more about where that number comes from and how it compares to other numbers. And I'll tell everyone that's the starting point. That's the baseline to beat. And first of all, if you beat that baseline, let me know. We'd love to obviously make it better. But second of all, our goal at Mosaic is going to be to drive that cost down as close to zero as possible over the next while. I set the goal informally to the team of getting to 100K sometime in the next few months. I think it's eminently possible given the kinds of speedups we've gotten elsewhere. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Wow. So can we get into this stack, see things like the distributed data parallelism, the chinchilla, the optimal compute laws, kind of like what goes into the package of the language model training? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Yeah. So there are really three big ingredients. I mean, the first ingredient to be completely honest is we put a number out there. This is kind of ingredient zero. I think a lot of the numbers you've seen previously for GPT-3 have just been overblown, to be completely frank. I think our number looks very impressive based on the insane estimates some people have put out, but those estimates just don't have really any basis in reality. This is a good number, but it's an honest number. And I think anybody else who sat down and tried to do this today would get to the same number. So item zero is just trying to be as direct and clear and honest as possible with the numbers. We don't want to scare anybody off from trying this. Quite to the contrary, I think everybody should be training their own models here. There's no reason why you should rent a model when you can buy it. But there are really three big technical ingredients that go into this. I think the first is the Chinchilla scaling laws. So for those who aren't familiar, first of all, we have these neural network scaling laws, which are kind of, I feel like scaling laws are a very strong word. These are our beliefs about the right amount of compute to use to train on a certain amount of data in order to get a model that is the best possible model that you could get there. If you have a, or you can look at it a different way. If you have a certain budget of compute, how should you spend it? How much data do you need to really make the most of that compute in order to get the best possible language model? What architecture should you even train? There's nothing fundamental or natural about this. This is probably a human description of a phenomenon that may be more sophisticated and more complicated. And I could talk your ear off about how much we should trust scaling laws. This is to be fair, one of the only places in deep learning where we have a scaling law on this GPT style language modeling. You never hear about scaling laws for BERT, for example. So, this may not work everywhere, but at least here we can kind of make these guesses. And what happened was a couple of years ago, the folks at OpenAI, Jared Kaplan and collaborators came out with a paper proposing some scaling laws. And it was awesome. It was the basis for predictions about what size GPT-3 should be. The problem was actually that it was a little bit off. And as soon as you extrapolate to bigger and bigger models, and this is, keep in mind, we're dealing with, you see these nice linear plots of scale on a log-log axis. So, being off by a little is being off by a ton. And so, especially at these larger scales like GPT-3, the predictions just weren't as accurate as they could be, which is totally expected when you're training on small models and trying to extrapolate. If you're off by a little, you're off by a lot. And the folks at DeepMind went through and basically fit the scaling law a little bit better and fit it with more data, with more compute. I mean, years later, they had many more resources. It's no flaw of the Kaplan work. It's really just, it's like we developed the James Webb telescope and now we can observe stars better than we could with the Hubble telescope. And what they found was that for the amount of data GPT-3 was trained on, the model was way too big. It just didn't have to be 175 billion parameters. It could have been closer to 33 billion. And that dramatically reduces the cost of doing this. It's a huge difference because the cost increases quadratically as you get to these bigger models, because bigger models, you need more data as well. And so, you can get away with doing this much cheaper that way. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "So that's why in the cloud, I think it's like 60 billion parameters is the 450,000 mark, because 60 billion is just as good as the 175 billion in terms of performance and that kind of thing. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "It's actually 30 billion, which is to me kind of mind blowing. We're dropping the size to 20% of what it was before. When it comes to efficiency, that's about as good as it gets. So I don't think that's the end of the story though, to be completely frank. We're seeing so much interesting work on data pruning and curricula and questions of how we should change what the attention operation looks like. In the PaLM paper, we saw this interesting parallel attention that came out. We've got flash attention. There are just so many new ideas floating in the space now about how to train these models effectively that we got 5X from just measuring something a little bit better. How many other 5Xs are there? So there's probably a lot more here beyond this, but for anyone who's doing research right now and really wants to get into this stuff, I think the sky's the limit on making these large language models more efficient. This was just kind of the low hanging fruit. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "That's super interesting. Can we dive into what makes it more efficient? Obviously you're the king of sparsity with the lottery ticket hypothesis and things like sparse attention. I learned about the Ali-B attention when I was first going through Composer. What are the efficiency things that have gone into the GPT-3 training?",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "For Mosaic, nothing. That's the honest answer is that we're using flash attention from our colleagues at Stanford, but otherwise we haven't even put in our speed up methods yet. I mentioned before, this is our baseline. This is ResNet50 from the NVIDIA examples. This is the way that I think about it. This is the starting point for making things more efficient. The first thing you have to do is stand up the stack and compute what your baseline costs. The answer is, wow, the baseline is already a lot cheaper than we thought, thanks to the work of the Chinchilla folks and some other things we'll mention. This is just our starting point at Mosaic. I expect you'll see in a few months from us, that number drop precipitously. Keep in mind, this is the number that allows us to have a healthy business as well. There's a lot of room for us to keep pushing down that number. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "I read another quote in the article around something around the vision of training millions of models compared to say the zero shot generalization kind of way of thinking. Can you tell me more about your vision for customized models? Say it's like biomedical domain, financial domain, legal domain. Do you see that being like, how has your experience with Mosaic been on finding all these specific use cases and then helping people say, convince them what a language model is, why it's worth it, and then how to say prepare their data? What's that experience been of getting people to train their own language model?",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Definitely. I think this is honestly something on the Weaviate side I'd like to chat about as well. People have a lot of data. That sounds like a dumb thing to say, but honestly, there wouldn't be Weaviate if people didn't have a lot of data that they need to organize in some way. I think it's the same argument at Mosaic that I've yet to meet a potential customer who wasn't sitting on a treasure trove of unlabeled data in their particular domain. The question is really, well, shouldn't you be able to take advantage of that? Or to put it a different way, right now we're asking BERT or GPT to do transfer learning, essentially. We take BERT, we pre-train it on Google Books, or we pre-train it on C4, and then the hope is, well, you fine tune it on your data and then it'll transfer. Why should we even bother to transfer? Don't you want to pre-train your model on in-domain data? You may not have it labeled, but one big hypothesis we have at Mosaic, and we're seeing this worn out in practice with customers right now, is that you should pre-train on your own data. And in fact, pre-training on your own data makes it much easier to get better performance because you're in domain. You don't have to force the model to go across domains and across tasks. So it sounds very obvious, like why do transfer learning if we don't have to? But I think in the academic literature, we think of BERT, we think of BERT two ways. One is, what do you do with all this unlabeled data you have? But the other is that we somehow put the constraint on ourselves that BERT has to be generic, that you want to have, given that it's so expensive to pre-train your own BERT, we have to make a BERT that anyone can use for anything. If we're successful at Mosaic, we push down the cost of BERT pre-training to the point where it's not expensive. And I'll argue to you, it's actually not very expensive. I think we've pushed down the cost of BERT pre-training to the point where it costs what ImageNet training used to cost. So for those academics out there who want to train on BERT but think it's too expensive, it's not, reach out. We haven't released our recipe yet, but you should expect that next month for BERT. But you don't need a generic BERT if you want to build a BERT for your one downstream task or your two downstream tasks. And in that case, you really do want to pre-train on your own data. In fact, you want to do even more than that. You want a tokenizer that was built using your dataset because the tokens that were used for C4 on Reddit or for Twitter, or they have lots of emojis, they have lots of words or characters. They may not show up in your data. You may have characters from a language that isn't included in your data that are wasting your vocabulary. And so everything from the tokenizer to what you pre-train on, you can build a domain specific BERT or a domain specific GPT. And I think that's pretty exciting. Like if you have medical data, you don't want a model that's pre-trained on Reddit as your basis. I don't think you want to use an API where you're just querying a model that was pre-trained on Reddit. What kind of medical data are you getting? Why not pre-train on PubMed? If you're working in a legal context, again, do you want legal advice from Reddit or do you want legal advice from whatever internal data set you have, or from the actual laws from various countries if you've downloaded those laws for some reason in a particular domain? So I think over and over and over again, or even further afield, if you're working on code or proteins, you probably don't want to start with C4. It probably makes a lot more sense to start on GitHub. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's super interesting. And I thought a lot about in search how a lot of the BM25, the keyword scoring is so popular because it helps for those specific words. Say you're training it on Mosaic ML Slack and you want a language model to talk to you about Mosaic ML, the word composer has never been seen in Wikipedia. So then these keywords scoring can help you adapt to that compared to this. So kind of in my thinking around language models and large models, I guess the reason I always thought that Reddit and Wikipedia were so popular is because it helps the overfitting sort of with a billion parameter model is predicting the masked token. If you only have 300 paragraphs, I always thought it would overfit to that. Is that kind of like the regularization techniques help overcome that or what's your thinking around? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Oh, I think this all comes down to data volume. So the assumption is made that people don't have enough data. People have tons of data. Oh my God. People have hundreds of billions of tokens of language data on their domain internally. Think of a company that runs like a, I don't know, a chat service for customer service agents or something like that. Can you imagine how much in domain data they have? Everybody has tons of data. It's been shocking to me how much data is floating around. So I think the important part to recognize is like C4 is at a trillion tokens, but plenty of people have around a trillion tokens of data or more. The amount of data that I've heard people have, it's just like, it's astounding how much unlabeled data labeling is expensive. You want to do it as best you can. You need it for the downstream tasks, but how do you leverage all that other unlabeled data? Train a GPT-3, train a BERT, train a SimCLR model on your data and use that as your starting point. Not an ImageNet pre-trained SimCLR, not a C4 pre-trained GPT. I think we have plenty of in-domain data. It may not be in the public sphere. If you wanted to go and find a data set of 200 billion tokens of legal data, that's hard. But a company that specializes in doing legal work probably has that in spades. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's incredible. And the language modeling algorithm is such a beautiful self-supervised, predict the masked token. It's very easy to just drop your text in. And I think a lot of, and SimCLR also, if you have images, you can just augment the images and then positive pair. And then we have some good heuristics. I was recently looking at this paper called Spider from Ori Ram. We had him on the podcast, a way to similarly do the positive negative bootstrapping. Kind of one other thing I was curious about with these large, like say the argument for Wikipedia for Reddit as the pre-training corpuses, I always kind of thought like this prompting thing, like the way that you can sort of template the input was a result of it seeing this kind of thing on the internet. So like, you know, it's because it's modeling HTML. If you do like image source equals, or like, I don't know how you prompt it, but like, like prompting it with like, you know how they'll like do like the title for an article by having title HTML tags, and then that's where it generates. So maybe do you have any thoughts on that kind of prompting and how that might be different when you're doing it for a specific data set? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Definitely. I think it again comes down to transfer learning. In some sense, prompting is not transfer learning. You're training the model to make predictions on what comes next for specific tasks, or in this case, you know, a lot of different things. And then prompting is just asking it to do more of that. If you want to prompt your model for something that is like what's in your data set, if you're, you know, doing medical work and you have a data set of medical facts and you want to prompt it for medical diagnosis information, if it's in domain, it's in domain. And so you don't need to, you know, you don't really need to worry about that. And I think, you know, the happy medium may eventually be a mix. One of the things we're doing a lot of playing with right now is mixing, you know, open source data plus customer data and seeing whether like what the right mix is should you like amplify the customer data more to kind of balance out what's happening. But, you know, maybe it is good to have a little bit of open-endedness. Like, you know, if you have a medical data set, it's not going to be able to write your HTML for you. You're completely right. Like that, I would be shocked if that happens. In a medical context, maybe you don't care. Maybe you do care a little bit about the model's ability to just kind of generalize a little bit. And then it's a matter of mixing the data properly or doing the pre-training in a specific order. And I think these are things that like this paper don't stop pre-training that like looks at this a little bit, but I don't know if we have the tools in academia right now to really look at this closely. Maybe something like the pile, which does have a bunch of separate domains would give us some ability, but I'm not even sure we know how to measure success. Like measuring prompting is hard. And I, you know, there are harnesses out there and there are benchmarks out there, but I don't know how good they are right now. So we're kind of in this tricky place where I'm not entirely sure how we measure and how we make progress in this front scientifically. We need the benchmarks. And, you know, this is kind of a call to action for the academic world and people who may be listening. This is a great project to do. You don't need to have a million dollars in compute. In fact, you don't need to have any compute in order to play around with a little bit of prompting on different kinds of data sets and building benchmarks. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "You mentioned one point that I thought was incredibly interesting. This idea of mixing open source data with in-domain data. Can you tell me a little more about the approach that maybe you would use your in-domain data as a query and then get like the top 10 nearest neighbors from the pile and then, you know, then blow it up 10X with that kind of thinking. What's been that kind of strategy? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Maybe. I mean, as a scientist in me, my reaction is always what's the dumbest thing you can do first. For me, the dumbest thing is, I don't know, you've got a hundred million tokens of your data. We've got C4. C4 is a trillion tokens. So if we were to just sample from both the data sets combined, your data would be underrepresented. Let's, you know, over sample your data by 5 or 10X and kind of make it 50-50. That would be the dumb first thing I'd try. And the other thing you can try is doing stage pre-training, like pre-train on C4 for a while and then pre-train on your data. I don't know if catastrophic forgetting would kick in. This is all new science. Again, for any people listening, we're trying to write a paper right now. There's so many questions. Oh my God. I wish I were in academia at the moment to chase after these questions. There's so much great science to do. But then you can get into questions of like, how do you figure out similarity between things? This is where honestly having some kind of, if only there were a service that would let you find the similarity between different kinds of objects in a semantic way, you know, figuring out what that similarity is and being able to use it. So it's, we have so many tools at our disposal right now. There are a lot of opportunities for us to play around with these things and really the limiting factor there does become cost, but you'll probably see a lot of the same effects in small scale that you will in large scale. So you can study this at a hundred million parameters and then test it at a hundred billion parameters and see what happens. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Super cool. And so I want to kind of get into the applications of large language models. I think so like kind of transfer learning for some other task is a very popular one, say few shot learning. And then you've recently touched on a really interesting idea, which is language models as databases. You, you know, query the language model directly. Can you tell me more about your thoughts on that idea? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Yeah, I think this is kind of, I don't think it's too early to call it an emerging consensus, but I would call it an emerging zeitgeist. That these language models are really kind of shims or interfaces. You can kind of honestly look at it two ways. Language models are databases in and of themselves. You can query a language model for the facts that it has contained in it. You know, who was the first president of the United States? Like, you know, the language model will say that, George Washington, maybe you need to prompt it a little bit with examples of how to take question and receive answer. You've seen all this work on chain of thought, kind of same idea, like how do you get information out of the model? How do you make the model do math? In some sense, this is all about the model having information that it's figuring out how to relate or connect in interesting, emergent, fuzzy ways that wouldn't have been possible in a relational database. The other way is to have the model literally interface with data or with some kind of data source. And I mean, Weaviate is an example of that in that sense. My other favorite example is the folks at Adept.ai who are building models that know how to interface with various web sources. You saw this paper from Ofir Press recently where he had a language model set up such that, you know, it would ask question, it would try to deduce some information by asking question, you know, it would provide a question and then, you know, it would either provide the answer itself or Ofir did this cool thing where he would just call out to Google and pull in the answer from Google and then have it ask follow-up question and then, you know, just keep using that information. But it was querying to Google. It was using a data source. And then I think there was an example I saw on Twitter today of having a model like write IPython queries that would use the Wikipedia package to query Wikipedia for information and then use that as input. So we're getting to this really interesting place where the model itself knows things and the model can interact with data sources in and of itself. I think these are really two separate things and I do wonder if in the future we'll train them differently. Maybe this is what the folks at Adept are already doing. But if you want the model to do retrieval and interact with data sources, you'll train it completely differently from how you train it if you want it to know things in and of itself. But I do wonder whether one cool application may be, let's suppose you have a big SQL relational database. You just train a language model on that content and have that alongside the database and, you know, develop ways to query that model for kinds of relationships or kinds of information that are in your database, but that you might not be able to suss out from just relations or writing SQL queries. You'll find new relations. I mean, this is what Weaviate is in some sense. And in many ways, this is turbocharging it by literally training the model with the data in addition to that. So I think the applications are endless in that respect. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "I really want to go back to self-ask and I, you know, seeing your endorsement for the paper and then I checked it out and I just thought it was amazing. And, but I do, I want to stay on this idea a little more. So is this the idea of say, you, you know, you, you take tabular data and you translate it into text, like just by kind of parsing it. Like I read this paper called language-interfaced fine-tuning, where it's like, you know, if and then the feature name equals the feature value, like you have this template to parse tabular into text. Is that kind of what the thinking of maybe we move all our data into this text interface for the language models? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Yeah. I mean, that would be, again, you, you know, me, I only do dumb things. That would be the first dumb thing I'd try. What if you just literally take the row of the database, format it, basically treat it as a sentence, like put a little symbol between each of the items in the database, maybe give it the field name, like put it into a little record format. And then those are the sentences you train the language model on. I wonder what would happen. I wonder if that would be enough, given a sufficient amount of data for you to be able to query the model. Maybe you do need to pre-train it first on something that looks like C4 and then you train this. But really the question is just how do you shove the data into the model? And at that point, maybe new relations show up. Maybe the model has some pre-conceived notions if you pre-trained it on C4. But really at the end of the day, the question becomes in my mind, don't you want your data in your database in some sense? I also wonder whether you can have a language model that learns to do SQL queries on a database to extract data out of that the same way we're doing with Google or Wikipedia right now. We could kind of go either way, but the important part is allowing the model to find these new emergent soft relationships between data. Like I'm totally convinced now of the Weaviate mission in that sense. Like this is, you know, there's so much we can unlock from our data that we couldn't before by just letting the model figure out how things are connected in ways that are hard for us to write down. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's super interesting. And so with this idea of things that are hard to write down, I want to come back to the self-ask thing and chain of thought prompting and this general idea of language models with tool use. I've seen that phrase like tool use. I think that's a pretty nice one where it's like, say the language model queries a calculator or it writes some code and then it runs the code and says, okay, what's the output from that? And with these search engines, I think it's just incredibly interesting because we have all this kind of like search pipeline stuff and query formulation things where you might be like a temperature in Germany and then you query it and then you get the results back and you're like, oh, actually I meant this particular city or like you are average, like you add keywords as you kind of iteratively search. So where does this self-ask thing, this chain of thought, is it in the training data? Is it something that is maybe learned from the language? How has it learned to be like, have a prompt that breaks up the question into like the compositional facts and then how does it learn to do that? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "That's a great question. Honestly, you should invite Ofir to chat about that. And it's a great question. I hadn't thought of it that way. I don't know where it is in the data and it may simply be that the model is mashing up all this other data that it has in an interesting way, the same way that you can get stable diffusion to give you like people sitting around a campfire on an airplane, I think I saw go by, or like salmon swimming in a stream where it was literal pieces of salmon. That was never anywhere in the training set, but it managed to mash it up and figure out what to do with it. And I wonder if something similar is happening here. What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part. It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward. We develop new querying methods. I don't love the idea that we'll go through and manually come up with these querying methods, because that just, to me, seems a little unscientific in some sense. Maybe it's good enough. Maybe some of this prompt tuning stuff will give us ways to discover better prompts or something like that. It's still pretty early, but we've raised really two or three huge questions here. One is, how do you get the knowledge into the model or where does the knowledge come from? Or even is the knowledge in the model or does the model just know how to interact with data sources? Number two is how do we query the model? Number three is how do we measure the efficacy of that query? In SQL, you know if you got the right answer. You can go through and manually search the database or what have you. If there's a bug in your database compiler, here, how do we know how to measure what the model knows? Is there a world in which actually the innovation that allows us to dramatically reduce the cost isn't a bigger model or a better training algorithm, but a better querying language that allows us to find that actually GPT-2 had all the same knowledge or could do much of the things that we can do today with GPT-3. We just didn't know how to ask it for it. We didn't know how to get it out of the model. I don't know. That makes me dream. I kind of wonder if a billion parameters may be enough in two years because we'll get better at the querying part. But to do that, we need better knowledge measurement. And if I were a grad student right now, those would be all the cool questions I'd want to work on because you don't need a huge model to do it. Ofir told me he used $900 worth of compute on that entire paper, which is just insane given that I have graphs that are 10 times more expensive than that in some of my papers. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Wow. Ofir came up with this compositional celebrities benchmark. I think it's things like who was president the year that Justin Bieber was born. So you chain together these facts and then it does that multi-hop compositional question. And Jonathan, you brought up the I think it's salmon swimming up a river and it's a diffusion picture. And it's literally like the food salmon.",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "It's like salmon filets in a river. It makes no sense. And I'm sure that picture was not the training set and people have pushed it enough. It's not just memorizing. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "So I'm very curious about this compositional generalization. I think the Gary Marcus angle is it's like a cup of coffee with holes in it. It shouldn't be able to exist, right? It's because the coffee would come out of the cup or the ball on top of this. You chain together all those adjectives like a round green rectangle on top of a purple cylinder. What do you think about this compositional generalization? Is this like kind of the ultimate generalization test? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "I don't know. I try to stay away from the Gary Marcus conversation to be completely honest. You can listen to old people talk a lot if you want to, or you can do real science. That's all I'll say about Gary Marcus right now on Twitter. But I do think it's an interesting question as to what kind of compositionality we want. When it comes to video, you could get this kind of behavior. I think really what may be missing is that we're working on pictures and not on video. Then when you see video, you may say, well, there are still things that can't really figure out. Okay, fine. We need to move to some kind of better world model. So it's really, I think the question of like the coffee cup with holes in it is a little bit separate from the answer of kind of compositional reasoning. On the compositional reasoning side, I mean, I think Ofir may have pushed these kind of compositional questions to their limit. Once you get into three-part questions, like who was president in the year that the person who wrote Blah Blah Blah Song was born? When you get to third and fourth order questions, it just gets like humans wouldn't be able to answer those questions. So I think Ofir has shown us a way forward and there's going to need to be more innovation on how we measure the kind of knowledge that's in these models, how we find other things that require logic to work their way through. It really is a throwback to some of the 1970s and 1980s style AI where we might need these giant knowledge bases where we can come up with facts or deductions that we know to be true based on these expert system sorts of databases. And then we need to ask the model to try to reason through and figure those same things out deductively. And I don't know, I think that's a really cool world to live in. And Ofir's paper, it points the way toward a future where we have benchmarks that look like that, where Ofir doesn't have to manually generate those questions, but maybe we dust off some scheme from the 1970s and that may already be there for us to pull on. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, wow. That kind of thing about how we do this aggregate query in these language model databases is very interesting. If we have a collection of articles about hiking trails in New England, let's say, and then we want to ask how many hiking trails are in New Hampshire, and then it needs to have that kind of intermediate. And the language model is the interface over the aggregation. There's this other paper called Neural Databases is something that had this symbolic aggregate step explicitly set. And I think all of that is just incredibly interesting. I wanted to also ask you about a different way of thinking about language models, the retrieval augmented language models, and the whole idea of you retrieve something and then maybe it's like an encoder decoder, where what you retrieve is fusion in decoder style rather than just being, say, prepended to the input and then decoder only GPT. Do you have any thoughts about things like RAG, Atlas, these kinds of models, RETRO? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Yeah, I love where we are in the field right now. We have models that we can train to memorize data. We have retrieval augmented models. We have the ability to fine tune models we already have. We can prompt models. We can do in-context learning. We can have the models use external tools. And we can actually literally train the models to use the external tools or we can prompt them to use the external tools. Probably one or two of these are going to stand the test of time. I have no idea which and I love it. I absolutely love it right now. This must have been how it felt in 2015. For those of us, the 99% of us who weren't in this field in 2015, when vision models were just exploding and every week it was a whole different way of doing things. VGG, ResNet, Inception, these were all such different models that were trained in such different ways. Adam was floating around at that time. You had different optimization strategies. Batch norm pops up, layer norm pops up. It's just this whole sea of ideas and it was hard to know what the best thing would be. DenseNet, Wide ResNet, like it was just an area of even more diversity and just completely different paradigms for interacting with these models. It is a great time to be in the field. Like, I don't know, people keep complaining like all the cool stuff was done. This is the cool stuff. Like, this is why I'm here because we have no idea what the right answer is and there's so much science to do. And for the people who sit down and systematically develop ways to measure this and systematically develop real-world scenarios, and honestly, that's a lot of our work at Mosaic. Unfortunately, it uses a lot of customer data and we can't release that data, although we try to find ways to share the findings and we can reproduce it on public datasets. But what a time to be in this field. So much happening. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. How much of this do you think is from the transformer and just text as being sort of the interface with multimodal domains? I feel like a lot of the... Well, I mean, yeah, there's a lot of breakthroughs to look at. But I feel like the sort of text and NLP to me seems like sort of maybe the biggest... I mean, images, of course, but I think this text interface is just such a unifying way of doing it. And then also for creating user interfaces for people to interact with these things. So maybe pivoting topics, I heard batch normalization, layer normalization. Can you tell me about the state of Composer and all these regularizations? I'm so curious about the training recipes and making it easier to train models. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Yeah. So I'll walk you through a bunch of things. So at Mosaic, the way that we think about this is at the end of the day, we have kind of converged around four or five big models, so much a lot's built. Those are things like ResNet, which is not only for image classification, more academic, but also is the backbone for a lot of object detection, segmentation models that are used in practice today. We talk a lot about the big vision transformers and everything, but by and large, what we've seen, good old ResNet is still the old standby. Then you have your segmentation, your object detection models. We've again converged around a few different heads for doing segmentation, for doing object detection. So there are a few different pipelines. NLP, it's BERT and GPT. That's the show. There are all sorts of things kicking around. There is RETRO, kind of a tweak on GPT. There's things like... Shoot, it's completely slipping my mind. But there are a bunch of other paradigms out there, but by and large, it's BERT and GPT that run the show. And at the end of the day, those standbys really are what works. At Mosaic, a lot of what we're focusing on in the scientific side is developing really great recipes for training those models. So we released our ResNet recipe back in June. That was our proof of concept to basically convince ourselves it would be possible to speed up models this way. And we got a 700% speed up over the NVIDIA baselines, which I'm so proud of. To be fair, it's an old benchmark, but even over some of the newer stuff like the TIMM models, we still were looking at a 2x or 3x improvement and accuracy levels that had never been achieved on ResNet-50 on ImageNet without help from outside datasets. So we're really proud of that. And we're doing it for everything. So honestly, I'll just... I'll go ahead and spoil it for everybody. We're going to be releasing our segmentation model next week, I believe. That we're looking at... The team has told me anywhere between a 5 and a 20x speed up, depending on your baseline. It turns out even the baselines for this model weren't very good. And we spent a little bit of time improving those. We had a blog post about that over the summer. And then once we'd improved that baseline, we were still getting between 5 and 20x. And we hope this will be really useful to the computer vision community. Those who need segmentation, it's a smaller, more niche group, but it's a workhorse and it's really valuable. And then on the BERT side, we'll be releasing our recipe probably in the next month or so. You should expect again about a 4x speed up on BERT pre-training on things like C4, but also probably on any dataset. For academic purposes, this is going to be huge. Well, honestly, I can't talk about one other thing that we did with it because I'll just say we're still in the MLPerf quiet period. But you'll see more about BERT soon on that front as well. And we're really proud of that. Some of this is not even with the algorithmic speedups, but with really good systems optimizations as well. GPT, you saw our baseline release. You can see this pattern, baseline, speed up, baseline, speed up, baseline, speed up. Did that for ResNet in June, the speed up for baseline was back, I think in July for segmentation. Three months later, speed up. You just saw our LLM baseline a couple of weeks ago. You know what to expect toward the end of the year, early next year. That's what I'm hoping we'll have GPT-3 for 100K. And then BERT baseline, we didn't really push the baseline too hard this summer, but you'll see the speed up out in the next few weeks. And so, if our track record keeps up, we're just going to keep doing this object detection, baseline coming out in two weeks. You know the drill. If we start working on self-supervised vision, you know the drill. We may work on some diffusion models, you know the drill. And it's just, keep cranking away at this. Honestly, on LLMs, I think there's more than 5X there. I'd love to get that cost under 100K. I'm not going to promise it because it's hard, but I'd love to get it there. And what that also means is that a 1 billion parameter model gets down into a couple of hundred dollars. For an academic researcher, that's what ImageNet used to cost. That's huge. I'm still the second year PhD student who didn't have as much compute as his friend, at least in my heart. I want all people in academia to be able to do real scientific research that the folks in industry aren't doing. So, you know, you can see what we're doing on that front. And then a lot of the big promises we're building it on, this Mosaic Cloud, which is a platform that is meant to make it really easy to do this stuff. You know, multi-node training where you don't have to deal with any of the issues, good integration with our recipes, where we can automatically profile your network and figure out which speed ups to put in, depending on your dataset, depending on your model, those sorts of things. We can catch, you know, cost spikes. Basically, it's trying to imagine what do I wish I had had when I was doing my PhD and hopefully it's useful to more than just me. But the hope is that'll allow us to stay in business long enough to keep giving cool things to the community. And my hope is that if you train with us, it'll be cheaper than training with any other cloud because with our speed up methods, you know, our A100s are magically 5X cheaper than everybody else's. So hopefully that'll be a nice gift we can give to the community in terms of dollars, not just in terms of using the speed up methods. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, well, it's obviously, you know, baseline speed up is a nice trend to be following. Can we, I mean, yeah, like you mentioned, you know, detect mosaic will detect a loss spike and then I think it resets it and maybe orders the data differently. Tell me a little more about how the recipes are a composition of like, you know, batch normalization or different attention layers and maybe like clever ways of doing dropout. Like, can you tell me how the recipes are unique to each data set or network? Like how is it not just the same recipe or that kind of thing?",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "It's a good question. It's something we're always studying. And especially right now, we're doing a lot of probing for our old recipes to figure out how broad they are. Like, let's take ResNet-50 and ImageNet because it's a benchmark most people are familiar with. What happens if you run that recipe on the places dataset? Does it completely fall apart? Have we way overfit our recipe to ImageNet? Have we overfitted? What if you change the resolution? Have we way overfit? What if you change from ResNet-50 to ResNet-101? So we're working right now, not just on these great flashy recipes that give you a 7x speed up, but on nice generic recipes that give you a consistent 3x on a wide range of different things in an area. So, you know, for BERT and GPT, that's a lot easier because language is language to a large extent. For vision, that's a lot more complicated because there are many other angles to take. So especially if my vision team were here right now, they would tell you that's the biggest worry on their mind is how generic are these recipes and how do you make them more generic? But at the end of the day, you know, a lot of what we're doing, if you start to look inside it, there's some stuff that's a little bit bespoke. You'll see this with our recipe for image segmentation where, you know, if you're measuring DICE as your metric, you know, we've added DICE to the loss because, you know, that's a really good way to improve your dice. And it's something that people have proposed. And once you do it, it does help a lot. But there are other things we're doing like exponential moving averaging to the model, which is pretty generic. It works across all of our computer vision tasks and it's hugely helpful. In LLP, it doesn't necessarily help, but it doesn't hurt. So there's no reason not to turn it on. There's something like sharpness-aware minimization where there are some hyperparameters you have to set, but it's been generally helpful for computer vision. So a lot of this, or NixUp, a lot of this is actually pretty generic. And I have no reason to believe it won't work on pretty much any computer vision task. It's up to us to prove that, you know, the burden of proof is on us, but so far so good. And I'm really pleased, like, you know, it's, I hope we'll have more results on that in the next month or so on how generic these recipes are and whether you're released, like, you know, the generic recipe and the one built into our cloud will be the generic recipe. That'll be what you get unless you really want to, you know, go for it. But, you know, if you can get 3X instead of 5X, but it works across the board, I think you're pretty happy too. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Well, and it's a very interesting point from ImageNet to places or, you know, satellite images, manufacturing images, and the thinking of like, have all these data augmentations just been developed for say ImageNet or does this really work for any kind of computer vision is a super interesting idea. I don't know. Yeah. I mean, it definitely seems like maybe like some of the cropping. Yeah. I mean, the whole like thing about like the difference between satellite images compared to image and I think is very interesting personally. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "If you were to ask my vision team, they would tell you they're specifically worried about that right now. And so they're working on it and they're getting a bunch of datasets and we're going to find out what the generic recipe is. And I think we'll have, you know, maybe that'll be the third blog post in each series. You know, it's our baseline, our best recipe than our generic recipe. And I think that would be fun. There's one other thing I wanted to throw in here, which is really one of the big aspects of LLMs is actually this fully sharded data parallel library. And I think it's important to discuss for a couple of reasons. So when we train large language models, they're too big to fit on a single GPU. In fact, they're too big to fit on a single node oftentimes. And not only that, but the activations are too big. So you have to do what's typically called 3D parallelism, which is where not only do you split the model across multiple GPUs, but you also split the data across multiple GPUs, which is what you usually do data parallel training. You also do pipeline parallelism where you send each example through the model in a pipeline fashion. You accumulate the gradients as you kind of pipeline the examples back. And then at the end, you apply the gradients to the model and then you repeat. So you don't even do it all in one step. And you're constantly streaming activations from, you know, first part of the layer to the next. You might even have to do tensor parallelism and split a tensor across multiple GPUs. These things are huge. And there's some fantastic frameworks for doing this. So in the past, one of the very popular ones was DeepSpeed. DeepSpeed is super popular for doing large language model training. We've worked closely with the DeepSpeed team and they're awesome. They produce some awesome stuff. In fact, they produced this other cool thing called ZeRo. And it's a series of sharding operations for sharding the state of things like your optimizer and your model across the GPUs. And even in some of the fancier implementations, you know, even doing activation check pointing to the CPU and even doing activation check pointing to non-volatile storage attached to your model. This is really cool. That's something I'll talk about in a minute because I think that's just the coolest thing in how they do it. Because if you train a large language model, you do so much reading and writing that you'll actually burn out your SSD in the course of one training run. But that's fine because the run is so expensive. What's one SSD? But the nice thing about these is that these sharding techniques actually reduce your need to do all these data, all these parallelism techniques. And so this fully sharded data parallel library, this is out of the folks at FAIR. But what it does is it's just data parallel. But all it's doing is it's using these ZeRo techniques to shard the model state, the optimizer state across all the GPUs in the cluster. And so instead of needing a ton of GPU memory on a single node to do data parallel training, and again, data parallel training is just where you have the model, you know, you have multiple copies of the model on each GPU. You run different examples through the models, forward propagate, backward propagate, and then merge the gradients. So it's kind of, it's the dumb thing that you would do if I gave you eight GPUs and told you to train. It's what we're doing on literally everything else. If you use PyTorch distributed data parallel, for example, it's a naive thing. FSDP takes us back to a world where we're just doing data parallelism. We don't have to do model parallel, we don't have to do tensor parallel, we don't have to do pipeline parallelism, which is the nastiest, but we're doing this sharding. So it requires a lot more network bandwidth in order to do the synchronization, but it's way simpler. Megatron is notorious for being hard to customize. The models that we released for FSDP, it's a one file, and you just have to wrap some things in FSDP, but it's one file, which means you can customize it, it's pure PyTorch. So you can imagine for us at Mosaic, we want to do speed up methods. We need to be able to modify the heck out of the model in various ways. That's really hard to do in Megatron. It's a piece of cake in this implementation. And I think it's, what I liken it to is kind of a caffe versus TensorFlow versus PyTorch sort of thing. We wouldn't have gotten to PyTorch if we didn't have some frameworks that preceded it that got better about coming up with the right APIs. I think it's the same thing with FSDP. In some sense, it's superseding Megatron or DeepSpeed, at least for the moment, which are superseding hand-rolled frameworks that you had to come up with yourself, because we've learned how to get the interfaces right, and we've learned what is and isn't necessary to make things run well. And so my argument to all of you is if you like to PyTorch over TensorFlow, you're probably going to like what we've put together in FSDP over Megatron. And it's customizable and easy to use if you want to tweak things. And I think that's resonated with people. So maybe we'll have the Jax that will eventually replace this, but at least for now, this is so much cleaner and so much easier. And you don't have to use things like Megatron that are really unwieldy and difficult to work with. And I've heard nothing but complaints, the same way that in the TensorFlow days, at least in TF1, it was the best game in town, but it was still something everybody complained about. I think we still complain about PyTorch, but a little bit less. And now people seem to love Jax. I like to think we're getting to that place with LLMs. And again, they aren\u2019t for everybody. But we're still getting the same utilization you would get with Megatron. You're not giving up any speed. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, well, that's so fascinating. And personally, I don't have enough experience with these things to really keep the debate going. But I do completely appreciate the abstractions and the APIs. It's just all incredible hearing about all these things. And so maybe a pivoting topic. So can you tell me about your experience with building Mosaic as a company? It's going like crazy. It's so interesting to see this company emerging. Can you tell me about your experience building this? ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "It's been hard. I think our CEO, Naveen Rao, will always say building a company is one of the hardest things you can ever do. There are a lot of hard things in life. This has been the hardest thing that I've ever had to do. Definitely takes the cake on my Ph.D. A Ph.D., you have to do good science. That's really hard. You have to create something that's never been created before. That's really hard. At Mosaic, not only do we have to create something that's never been created before, we have to create something that's actually useful to people and useful enough that people will pay for it. We have to understand what people want. We have to build one big thing together. A Ph.D. is very much an individual pursuit. Building something together is hard. Trying to figure out, you would see, to get to the base, we do the baseline blog post. Three months later, we do this feed up blog post. And to get those baselines, it's probably another three months of work, which means that we have to predict what our customers are going to need six months in advance if we want to have it ready. That's hard. This is all really hard in a space that's changing so quickly and in a financial environment that's changing so quickly. 2021 was a great year to be raising money. 2022 is not. It's been a tough year for startups. It's been a tough year for everybody in terms of tech companies. And so all of that is really difficult. Keeping a team focused, keeping a team with our eyes on the ball and what matters, trying to serve our customers appropriately, it is so hard. And I'm sure you can speak to the same experience. It is so difficult. And I love the challenge. I hope we'll succeed. Who knows? But the experiences I've gotten and what I've learned have just been incredible. I really didn't plan on doing a startup. We've talked about this a little bit more privately. You know that that was not my plan in the least. I'm really glad that I had this experience. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "That's super interesting. And you've always amazed me with how many things you're able to do in parallel. I think when I first met you, you were at Georgetown, you were at MIT. And now you're a professor at Harvard and Mosaic. How do you manage? How is your time thinking? I'm very curious how you are managing your time in your new role at Mosaic and also obviously your role in science. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "It's tough. It's genuinely tough. I'll tell you two things. One, I haven't started at Harvard officially yet. I got one more year. But Harvard has been very effective at finding ways to get free labor out of me. So advising students, which is a blast. And I'm also managing their compute infrastructure because I'm doing that in my day job. So it's really useful there. It's been a lot of work. And then between you and me, I still haven't finished my dissertation. I'm working on it. But it turns out Mosaic has been a huge distraction. My advisor will ping me periodically and make fun of me. Your students realize you don't have a Ph.D. yet, right? It's the biggest running joke at Mosaic. Oh, Jonathan, he doesn't have a PhD yet. Yeah, my friends find it so funny that I have a Ph.D. and my boss doesn't. It's hard. And I'm not sure I'm that successful. I've also chosen to prioritize work-life balance lately, which has been really nice. But also startups are demanding and they will take all the time you can give them. And I work really hard with my team to make sure they're taking vacation. I yell at them if I see them on Slack on nice weekends. There are times for that and there are times when we have to push, but it shouldn't be all the time. And if you're in a place where you have to burn yourself out to be successful, I don't know if that success is worth it, to be completely honest. There's much more to life than suffering for your work. And so, yeah, the answer is I don't have the balance I should have, but I have a hell of a lot more balance than the last time we talked or the time before that. And I'm really glad for it, but it's hard. At the end of the day, the people I'm most responsible to are my team. They're counting on me to make sure they still have jobs. They're counting on me to make sure that they're doing the right work. And when push came to shove and it's a matter of taking the evening off or making sure that my team is in a good place and is safe and is happy, you know, got to be people first. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Incredible. Jonathan, thank you so much for doing the podcast. It's so amazing getting to pick your brain about these things. You have so many interesting ideas, so caught up with it and you're building such an incredible company. And yeah, it's really cool to hear how the people first, the balance and all that good stuff instead of some kind of like, let's work ourselves to death culture. But yeah. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "I'll probably give you the same feedback back to you. There's a life outside of work. Don't you forget that either. You lead a pretty busy life. ",
        "podNumber": 26
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Well, thanks again, Jonathan. Thanks so much for doing the Weaviate podcast. And it's awesome. So excited about these things. ",
        "podNumber": 26
    },
    {
        "speaker": "Jonathan Frankle",
        "content": "Thank you so much for having me.",
        "podNumber": 26
    }
]