[
    {
        "speaker": "Connor",
        "content": "Hey, everyone, thank you so much for checking out the Weaviate podcast. I'm beyond excited for this episode. We have Dmitry Kan, one of the most influential speakers in search technology. Dmitry is the host of the Vector podcast, he's a senior product manager at TomTom, and he's recently given this incredible keynote at the Haystack European Conference 2022. So firstly, Dmitry, thank you so much for joining the Weaviate podcast!",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Hey, Connor, thanks for having me. It's always it was always a pleasure to talk to you. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Awesome. So could we maybe kick things off with the big the keynote where Vector Search is taking us and some of the key ideas behind that? ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, for sure. So the keynote, I had an honor to give this keynote at the Haystack conference, as you mentioned, in Berlin in late September last year. And there have been a bunch of actually people from the Vector Search community, including Weaviate and Pinecone and Qdrant and also users, it was especially interesting to see how far ahead users have gotten. And they've been asking very precise questions, you know, like, how do I choose a model and which database to prefer and so on and so forth. This was very interesting. And some of them have been already trying things. And so in that sense, I wouldn't say like a year ago, maybe I felt like I could share more and educate more and even explain the basics. Nowadays, it's more like, okay, we already understand the basics. Okay, can you explain what practical step I can take to actually implement Vector Search? Or I have tried, you know, doc2query, it doesn't work that well. What should I do? You know, and one of the key points in the keynote, key points in the keynote, I have had was this work that was done by McKinsey and Google in 2021. So almost like two years ago now, where they call out the issue of search abandonment costs for the US retailers. So it's only US retail, which kind of like loses $300 billion a year. And why they do this, why this happens is because of the search abandonment issue. So you know, practically speaking, users start typing a, you know, a query, they probably see some autocomplete, some dynamics going on on the website, then they check the results and they cannot find what they look for. And we discussed with you in the Vector podcast just recently, that the query language, the user language is so different, right? User to user. I have a bunch of examples on this front as well, when search and e-commerce doesn't work. And so what's interesting also is that 64% of retail website managers do not have a clear plan for improvement. So they kind of lose the money, they don't know what to do with it, how to fix this. And that's like more than half of them, right? It's like a big, big number. And another issue that kind of surfaces now that I have this product management hat on, it's interesting to see that 85% of global online consumers view a brand completely differently after the search is unsuccessful. So in some sense, the brands themselves have nothing to do with the search engine, which is basically like, let's say an aggregator or whatever, like Amazon type of thing, right? But if search doesn't work, they think it's brand's problem because they cannot find the item they are looking for, right? And so they change their perception of the brand. This is how important it is to make the search work. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, and it's super fascinating. It reminds me maybe like these kind of numbers, there's something about like maybe 98% of data is private, the motivation of sort of indexing and searching through private data and as well as this retail thing, like having these kind of numbers. But I'm going to think like, so with the retail and fixing it, is it the responsibility of the brands? Like whose responsibility is it to implement these cutting edge search engines? I mean, we think like with Weaviate, it's our responsibility to build it and make it available. And you see it as being like more independent e-commerce retail stores. So like not just Amazon, being sort of the front layer, but more and more individual retail sites taking on this pipeline of, as we'll talk about neural search frameworks, like having their own Weaviate, Jina AI kind of setup. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, I think that's an excellent question. And I think it kind of like depends on the situation, the practical situation with engineering and sort of assets and resources that this company has. And also like the focus that they want to have, right? In some cases, companies might want to like outsource this and just ask somebody to build it for them. And this is an interesting opportunity for players like, for example, SeMI or Vespa. If you guys have a cloud offering, which is kind of like end to end, solving a particular issue starting from actually creating the embeddings, all the way, all these backups, every infra element is taken care of. And then on top of this, you can actually prove that, you know, I can actually retrieve the items with higher relevancy than I would do myself. Because what does it mean for me to make it myself? I wouldn't start scrambling, reading Stack Overflow, hiring engineers, and that's like a long path, right? And maybe I'd rather pay money and just get to the market sooner. And so I think that's an excellent question. And there's another facet to this, I guess, is it like in your question, is it brands concern? Or is it like, let's say a neural search framework or vector database concern? Or is it like a retailer concern? In some sense, I think it's like a, it's a joint ecosystem, right? It's not like you can just say, hey, please fix me the house and I'll pay you. But they might fix it their own way. They might also break some things while they do this, but you are paying. So money don't always buy everything. So I think I still believe that it's better if these players would collaborate in some ways. For example, you know, if we're talking about neural search implementation, you know, what's important is like, for example, how you catalog an item, what metadata you have, right? And so, and also how up to date it is, right? So somebody needs to be supplying this data to you. And so maybe the brands could be supplying this data to you as they change or introduce new products. But then everything else, you know, the pipeline should be already in place and it should be easy to use, you know, be it an API or some other way. Like Amazon, I remember they have an office, they have a service of delivering data on trucks, right? Because it's much faster to deliver on trucks than to upload it because it's a lot of data. But if it's like a medium sized store, like maybe you don't need a truck to deliver your data, you know, on batch level or whatever, like you could build an API. I hope I answered your question. But there's another perspective also to keep in mind, even if you have the engineering force in place, like in-house, building, like if you take like an open source database or open source neural framework, it might take you a while to gain the, you know, to accumulate the knowledge, to gain the momentum, to realize, okay, this is what we can build from POC to actually productizing this. Maybe it's easier if you just go and kind of like outsource this resource, this cost to a managed database, like in case of Weaviate, in case of Pinecone, and focus on something else, right? Focus on that specific, you know, cataloging issue or maybe bringing a classifier that will do the product classification on the fly and things like that, right? Kind of like put your effort and focus on what matters. And then later, maybe you can change your mind. You know, the moment you grow, maybe you will decide to build your own vector database. But before that happened, you need to kind of like pay your bills, right? And so don't get ahead of yourself. And also, if you partner with a clever, you know, player, which is like we have a lot of them now in the market, you know, you might win and they will also learn, you will learn together. So that's an interesting perspective to also think about. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, it's a brilliant way to open up this, as we're also talking about neural search frameworks and you've written this great blog post that outlines the different neural search frameworks. And as you mentioned, I mean, you did a brilliant job just covering it and this idea of, you know, the overhead for learning it, the learning curve of something being something that's important. And at Weaviate, there's a huge focus on UX and being developer friendly, creating content to try to educate, especially with the new releases we try to have even our Weaviate Air Show to try to explain how to use all this stuff and what it is as much as possible. And I think this is going to be such an interesting discussion because there are parts of it that should live outside of Weaviate, I think parts that I think maybe should be built into Weaviate. I also want to just quickly set this up before we get into it, that like this, what I say on the podcast about the relationship of Weaviate and neural search frameworks is my personal opinion. It's not like an official statement of Weaviate with respect to like how I see what should be built where. So I really want to kind of transition this into maybe we could kind of do the tour of the neural search framework, starting off by maybe just the high level of how are you currently defining a neural search framework? ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Oh, yeah, that's an excellent question. And maybe I can also intro, like I created this, you know, mental diagram for myself and I kept selling it everywhere and seems like people get it and some of them even like reach out on LinkedIn and say, hey, I got your vector search pyramid. Thank you for creating it. But really, I, you know, my goal was just to put things in their place, like on a bookshelf. So you can like reach out and give context and ground your discussion. And so in this vector search pyramid, which we probably can also share, you know, essentially like on the base level, you have the KNN and ANN algorithms. And to some extent, I also covered some of them, not all of them. You know, I know that Pinecone, for example, did a great job publishing a lot of material on this, you know, how each algorithm works. We also happen to invent one algorithm, BuddyPQ, which is just a modification of product quantization. And then the next layer, and I did publish a year ago now, a blog about not all vector databases are made equal. And so you have Milvus, Weaviate, Pinecone, you know, GSI, Qdrant, Vespa, Vald, and also those players that kind of like, yeah, I didn't find yet the proper phrase. I kind of say catch up, but on the other hand, it's not like catch up, it's like being bold and actually going into this space for existing databases like Redis and Elasticsearch and Solr, and they add vector search functionality as well for their users. And then there was this next layer that I couldn't like quite wrap my head around. And I was thinking, what exactly is this? And of course, Tina and Haystack are sort of like, in some sense, the pioneers of creating the terminology. And so I think at some point they were calling themselves neural search frameworks. Recently I saw that Jina is calling themselves something like MLOps for neural search. So of course, these things morph, but I kind of like decided to stay with the same term, like neural search framework. And I kind of labeled every system I could come across. But there are some exceptions as well, by the way, there, and I'll get there. But basically the way I define it, and I guess we can share the blog post itself, is that basically reading from the blog, neural search framework is an end-to-end software layer that allows you to create a neural search experience, including data processing, model serving and scaling capabilities in the production setting. So like just to unpack this, so let's say if you take a SQL database and you want to build a website, right, you will still need to figure out, okay, where do you get the data? How do you process it before it gets in the SQL database? Then how do you normalize the tables if you do that, right? Like foreign keys and all this thing, which index type to choose and things like that. So that it actually breathes and works and can scale. But like this definition may sound to some people as MLOps, machine learning operations. But there is a key difference that first of all, MLOps, it's like a wide area. It's a wide field. You know, it concerns itself with a lot of things like, I don't know, model training, you know, model versioning, deployment, serving, monitoring, you know, data drift, fine tuning, a lot of things, right? And the application is also quite diverse. Like I don't know, I could be building, I don't know, face recognition system or something like that, right? But like neural search framework focuses only on neural search. So it's like, and also another question that frequently comes up is like, what's the difference between neural search and vector search? In a way there is no difference. It's just like how you take which angle. Let's say neural search, you could think of it, okay, I have a deep learning network. And so that's probably why it's neural, because it's like deep learning, you know, neural network. But if you take the stance of, let's say, geometric space, right? So you embed your object into a multi-dimensional geometric space. And now you need to, and each point is a vector. So now you need to basically find a vector which is relevant for your query vector. So you are doing a vector search, right? But this is kind of like mechanics of it. So yeah, I think this is how I define it. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, and I think I'd want to start with connecting with our earlier conversation of the lost money in retail and more and more brand stores trying to build their own retails. And I'd even maybe extend that to people with their own blogs, looking to have searchable things on their blogs, and also paying the bills and not worrying about this thing. I kind of want to start off with say, like the data pre-processing layer. Like I see kind of with these neural search frameworks, they define these pipelines. And I think pipelines is sort of the key term here. And I'm going to talk about what I, what are the pipeline I think should live in Weaviate and what I think should live outside of Weaviate. I think maybe starting off with just like the data ingestion part, like maybe you have some kind of API that you query to get the data. You have maybe like a PDF parser with some kind of OCR. How are you thinking about that first part of the data ingestion layer? Because it may be if I could just add one more thing to transition the question. And coming into like running things in production, Jina AI, they have these executor pattern. And I learned a lot about this on the Weaviate podcast with Maximillian Werk. I highly recommend that if you're curious about learning more about this pattern for listeners. But this kind of way of scheduling like a cron job that say, you know, every two hours is going to query this API, like, or say every day it's going to hit the arxiv API to get the new batch of machine learning papers, parse out the text and chunk it in the PDF, then vectorize those chunks. And then maybe put that to Weaviate. And then, yeah, I don't want to, let's start just on that first part of.",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, I think it's another excellent, you know, topic to think about because I've been consulting a few startups that are trying to build vector search, right? And they sort of get quickly far ahead, you know, because they have, let's say, a database, a vector database, they have their data, obviously, they've chosen the model to vectorize with. They're already scratching the relevancy side of things. But then someone comes in and says, hey, we just received another batch of objects. We have half a million, you know, objects to index. Can you please index them? And by the way, I have a demo tomorrow, right? So what options do you have, right? And literally, some of the developers would reach out to me and say, hey, any options, anything to save this situation? And you know, like, of course, naively, what you can do is that you can start writing a Python or Java, you know, concurrent app, which will start unpacking this, you know, datasets, you know, reading from S3 or something like that. And then vectorizing, you hear the problem that, oh, you need to use GPU to speed things up and that's quite costly. So we need to kind of like, rethink a lot of things and not make a mistake if we launch this in batch mode, and it will run for two weeks. So you go back to your manager and say, it's going to take two weeks and they're like, what? And how much is it? $5,000. Oh my God. So like, each time we crawl the data, you're going to need $5,000 in two weeks. That's like going out of hand, right? And so, especially what you mentioned in Jina, and also there is a framework called txt.ai, which I learned just, you know, by kind of like Googling inside GitHub, if I can say that. So they have this notion of kind of this workflow. So for example, as you said, in Jina, you can have an executor that will read kind of like an archive and basically iterate PDF files and then transform, parse the textual content out of a PDF file, and then it proceeds to the next stage, right? And txt.ai, for example, also has, you know, some connectors for other types of data, like in computer vision space or automatic speech recognition space. And so they have different like workflows that will help you set things up really quickly. They have one demo app on Hugging Face Spaces, which essentially what it does is that it goes to Hacker News front page, it scrapes, you know, the top links, and then it indexes the titles, you know, of that top page, the top on that front page, and then it shows a search box. So it basically indexes all these titles, you know, embeds them on the fly. And then basically, when it's ready, you have the search box appearing on the screen, and now you're ready to query it. And so it is that easy. And they show, you know, how easy it is. Of course, it's not what you will use in production setting, most likely, you know, like in production, you want to have like a workflow that routinely goes and checks that front page, you know, like a cron tab, and then indexes that. And you still have the index that is serving the query. So you have some offline index, which is being prepared, and then you do the swap, you know, and things like that. But they also simplify things like deploying on Kubernetes, so you can scale things up. Because like, literally, if you if you write your own Python app, and one of the startups, by the way, there was a bottleneck, specifically in this component that would read the objects, one by one, and then it would try to classify the object and then embed it. And then it proceeds to the next step. And it was like, I think it was like single thread application, which would kind of like take forever. And it was very convoluted, right? Because if you don't have the framework in your hands, you start reinventing the wheel. And so most likely, you will kind of cut the corners. And you will be basically wasting time, unless you have a lot of time, which usually is not the case in startups, like you need to build something really quickly. And so this is this is something that I think is probably, you know, hasn't been in the minds of the makers, maybe like more than a year ago. But I think it becomes more and more important that you don't just bring the vector search core functionality, you don't just sell the statement that, hey, move to neural search, and all your problems will be solved. But you can actually show the path to get there, right? And with these workflows that process the data, access the data quickly, and allow you to do this repeatedly, is going to win more customers. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, and I think, well, yeah, I'm sorry, maybe I'm slowing it down too much. But I think maybe from the next step, we've gotten the data, and now we're vectorizing it, I kind of want to if we could talk a little more about the decisions with vectorizing the data, I think it's so interesting, you mentioned like a single thread approach where you're not taking advantage of like batching on the GPU or parallelization on the CPUs. And we've recently added things like ONNX support for the text2vec, thanks to Marcin Antas who got this done. And like this kind of model inference for the vectorization sake, also, I mentioned on the vector podcast that I'm really excited about our partnership with Neural Magic and what they're doing to sparsify these models so they can run super fast on CPUs. And so, and maybe one more reference is in our Weaviate podcast with Sam Bean from you.com, he describes how they combine the Spark big data technology with the ONNX acceleration for the CPUs and how they vectorize that way. I know you've done a podcast with Max from Mighty. Can you tell me how you're thinking about the vectorization layer? Sorry, one more thing is with Weaviate, I think kind of another interesting thing about this is how we have separate Docker containers for Weaviate, as well as these deep learning model inference containers. So you can kind of scale them up differently with I think things like the Kubernetes Helm chart and it's a little more complicated, like you can scale them up differently. Or you can just use Weaviate cloud service, as you say, you want to pay your bills some other way. So can you tell me about how you're thinking about the vectorization space? ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, for sure. And I think it's great when you describe how you guys build it. I take the stance of looking at it as an outsider. And in some cases, I'm basically the middle level between the customer and then sort of participating in the decision making. And I'm not fully aware of how things are implemented inside the vector database, for example. But I know that somebody has got to pay the bill in the end, right? Even if you use a vector, sorry, a cloud solution, still the bill will come your way, right? And guess what? It's either you or your manager that will have to pay it. And also, guess what? Because of the low margins, and I just had a podcast with the GSI product manager, where he says that a lot of these things are now on the radars of even big players like Amazon or any big player that you think is a big player, they still might have very low margin, like Google might have very low margin on their web search, right? So it's very important for them to optimize things. And so I think this article in the budget, if I can say so, is very important as you build the neural search experience. And so if you need to pay, as I was saying, $5,000 each time, it's going to be prohibitively high and so it will slow you down, right? Like eventually you will say, hey, maybe we don't evolve as frequently, maybe we will just do it once a half a year and it may also die out. So it's kind of important to address this heads on. And in one of the startups, I actually recommended Max's work. And so I said, hey, can you port the model to ONNX and basically make it available as part of Mighty? And what Mighty does is that it basically moves your computation from GPU to CPU at a comparable... So the quality is exactly the same, you get the same embeddings, but you pay less, right? And the speed of doing this on CPU is comparable as well as it would be on GPU. So in that sense, maybe you can dedicate GPU cluster more to things like model fine tuning where it matters, but to that production side of things, like when you compute the embeddings already on the existing model, and then how you productize, how you serve things, you don't need GPUs necessarily. In some edge cases you do, but then you know, like, okay, why we do this, probably it pays the bill and also bring something on top. So it makes sense to do this, right? So I think this is interesting that I've also learned in the past year that, you know how things kind of get created, right? So first you get that hype of new tech, like on a big data a few years ago, and then comes this realization that, oh yeah, yeah, we do have a lot of data. Oh, sounds like Hadoop is the way to go. But then all this data shuffling, or how do I upload the data into HDFS? Like, you know, and all these connectors arise and like all of these things. And then somebody comes over and says, hey, no, no Hadoop anymore. Like let's do something else. So but I think every step in this journey is important. You know, if there was no hyper, so to say, in the beginning saying like, you know, like Bob van Luijt was saying, I was just in the airport after the conference with Google and I realized, hey, there's something here, you know, let me build this model and maybe it can reason semantically about text. And even if it wasn't doing 100% perfectly, it was already showing the way to move forward, right? So all these other items, which are more like mundane, infrastructure level, unsexy, no one talks about them on sales presentations, or like product level, maybe even, right? So you don't say this to users, hey, you know what, I spent $5,000 to embed the items and now you can find them. You don't do that, right? It's usually DevOps people, it's usually, you know, heads of units that will say, hey, did we spend $5,000 again? Can you do something about it? So then you go back and say, which options do I have? And so I think then it becomes important to focus more and more. And I think Max, by the way, gave an excellent presentation and he showed also how he scales. So it's not just a Docker image of mighty that, you know, you can send an object and get an embedding, but he also shows how to scale it out and basically still save on money and make it efficient in terms of time. So it becomes like a trade off of how soon you need it, how much money you have, you know, how much money you can burn on this and things like that. But basically he's working on a very efficient implementation. So I can recommend that. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, it's so interesting. I think like you could completely abstract it by sending the embeddings to the OpenAI embeddings API or Cohere's embeddings API. And there's like that model as a service model on the cloud API thing, or there are these, you know, do it yourself options. And I think one thing that I think the frameworks contribute a lot is with the ease of like Docarray embed or, you know, the deepset cloud with Haystack and how you can just have your Python code to embed and then put it in the database layer. So I think I'm skipping over a little bit over that little layer of you have the vectors and now you need to import them into Weaviate because I think, you know, and we have like a new Python client and Dirk Kulawaik is the expert on this and hopefully he'll be back on the podcast. He was on our 1.15 release podcast. We can talk about that. But passing that import layer, let's talk about the ANN index part. And I'm curious, like how you see the entanglement between the ANN index and the database part. Like is there a chance that it could be separated? I know with Weaviate, like the, you know, HNSW product quantization, it's written in Golang. It's very like in the core of Weaviate. Do you think these two things could be separate layers of the pyramid one day?",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, that's also a great question. And the podcast that I mentioned with Yanif Vaknin from GSI is one example where the ANN index can become its own entity because GSI is basically, GSI offers an APU component, right? So associative processing unit. So think of it as the same, you know, system in the family of processing units like CPU, GPU, and it's kind of like next stage in a way targeted specifically at neural search, but not only that. And so in principle, Weaviate or like Qdrant or Pinecone, what have you, or Elasticsearch could be basically that computation layer. So in some sense, like a middle layer, which receives the data, it knows where the data is, it knows everything about relations between objects and so on, but it doesn't need to, so like on certain scale, it could also do the vector search for you, right? So for example, using quite efficient HNSW algorithm or product quantization or something like that. And I know that you guys also implemented diskANN, is that right? So like, I mean, so this is when companies will concern themselves with the cost issue and they say, Hey, for our use case, it seems like, you know, RAM is way too expensive, can we actually move closer to the disk and we have plenty of SSDs that are cheaper. And diskANN, which I think is kind of like a derivative from Zoom paper, essentially basically moves closer to the disk. It does that expensive sorting of full precision vectors as the last step, but everything before that happens on other layers, like with lower granularity of vectors. And they also solved the problem in HNSW. And again, sorry, Yury Malkov, but like he might disagree for sure. But like in that paper, they claim in diskANN paper, they claim that thousand nodes in the HNSW graph that they have built were unreachable from any point, from any entry point. And so they have fixed this problem. So they increased the connectivity of the graph. And so, you know, these fundamental issues being solved and you can then focus on things like, okay, how do I quantize my vectors? To what extent? And this again, is a trade off between granularity and sort of like later issues that you will have, right? So for example, with precision and vectors overlapping to the single point, so you need to disambiguate them in some way. And there are ways to do this, but it's also may become expensive in some cases. So this is interesting that I think, and this was also one of the questions on my keynote as well. Is there a place for hardware companies and maybe startups even to appear on this scene? Of course, we know that Nvidia and Intel are working on this. They've been competing heads on on billion scale ANN competition challenge last year. But also of course, GSI does it. And I think, yes, of course, building a hardware startup today may sound scary, but at the same time, I think this could be very interesting to tap into. And so maybe in the future, already today, you know, like there is a connector between Elasticsearch, OpenSearch and GSI hardware. So basically the way I think about it is that in Elasticsearch, you can compute facets, right? So you can build, I don't know, nightly job that builds facets and then displays them in some dashboard or sends them to the users. So all of this computation could happen in Elasticsearch, but that other expensive computation with neural search could happen outside. So you don't need to kind of like suffer from the fact that now you need to balance between these two fairly expensive processes at scale. But you could kind of like distribute them, you know, how we do with vacuum cleaners, we charge them and then they go around the house and clean, right? So they don't use the electricity and you can use the electricity for some other purpose. So exactly the same idea I think may kind of like enter the scene at some point. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "So really I thought of just so many things as you're talking, so packed with knowledge. I think, well, I really want to kind of understand the APU a little more because I don't quite understand. I think I want to quickly touch on some other things and then we can come back to that. So I think a very interesting thing is about the incremental nature of the ANN index in database entanglement. The key distinction between like vector database and vector library, amongst other things in my opinion, is that you have to incrementally update the vector index compared to like build once and then you have the static index search. And I think that's, I think HNSW is amenable to that, but like product quantization, making it like online, because what product quantization is, is you have the vectors and you're clustering them for each of the dimensions to reduce the precision by representing say 32 bits with instead the centroid ID of that index in the vector, and then you can like couple it and slide it that way to like have the k-means in it. And I know that you understand that. I'm just kind of saying that for the listeners. But so the kind of incremental thing is how I see that distinction. But yeah, could we come back to the hardware and what makes it different? I know that like Cerebras is building this big chip with the thing of, you know, where, you know, like GPT3 is limited. I think 4096 is the token limit at the time of this. And I know it's, I was recently reading DeepMind Sparrow paper, and it's fascinating to see these massive prompts. Like when you see a prompt that's like, you know, a 1200 token prompt, you're like, wow, that's quite a prompt. Like this idea of building custom hardware to overcome the quadratic attention, or I know there's like sparse attention, but like generally to have massive inputs to Transformer. So that's kind of my frame of reference for understanding custom hardware for deep learning. And I think the TPUs are a similar idea where it's like a big GPU. So what are the details behind the APU? How is it optimized for vectorization?",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, that's a great question. I think we can share a couple links on that. We've also happened to build a demo, which we presented at Berlin buzzwords last year. So we took, you know, 10 million images and from LAION data set, and we have used clip model to vectorize them and then build a demo where you can compare how, you know, in image vector search compares to, let's say, in title vector search versus complete keyword approach, right? So no vectors involved. And so we've used APU as our back end, you know, far back end for storing these vectors and actually, you know, computing the nearest neighbor set. And so basically the way APU looks all sort of like on the inside is that, first of all, it's kind of like this paradigm of compute and memory. So if you take kind of like a traditional server in a way, traditional way, you have a CPU and RAM, when you will run the, let's say, HNSW algorithm or some other algorithm, whatever, it will have to kind of like go back and forth between memory and CPU, right? Because like some things are in memory that you keep as a state, but then in CPU, you still need to do the computation, right? And so you basically constantly like change the context. And so you kind of lose some time in doing that. In the APU, they pushed as far ahead as possible to go after massive computation. Like basically, they do it entirely in memory. So they don't involve any CPU or any other units, right? And so they have like 48 million RAM cells. So it's highly packed. Maybe we can provide an image, you know, like one chip that you can insert into the server. And I think you can insert a couple of these to, you should insert a couple of these to scale to 1 billion items, right? And if you have less, you can insert maybe one. And so basically, each of these units contains like 48 million RAM cells. And then they have like, so they roll up to like units that basically do the programmatic bit logic computation. And in principle, if you quantize the vectors to the bit level, then basically what you need to do is kind of like bit logic, right? So like bit computation, like multiplications or what have you as you do the vector search. And so basically, like you can send, let's say 20 queries at one single time, and you might have 1 billion items to check against. And they can massively basically run this parallel computation for all 20 queries at the same time. So you will have to precompute the vector representation for your vectors. But beyond that, they will do this massive computation, and then they will return you results for each of these query vectors. And then you can do whatever you want on the front end, right? So for example, one use case could be you have stored queries, right? And you want to like see what new data items are going to be hit by these queries every single day, right? So for example, for financial industry, this might be very important to stay on top of things. What's happening? Do I need to sell my stock or buy stock? Or do I need to do nothing? And so this basically then becomes suitable not only for similarity search, but also for image processing itself. So I think they're using the same units even in space in some cases. So if you don't have, I don't know, I'm getting beyond my knowledge here. But if you have this complex hardware with CPU and all these magnetic fields involved, like in space, you might get hit by the radiation, which shines directly on that device. And so it melts or whatever. But if you have less of that, so you only have memory, you can seal it in a certain way and then you don't need a cooler maybe even, because you don't have the CPU itself. But of course, you do need some cooler, I guess, but whatever. So I think they're using it also in space. So it's kind of like a versatile tech, but it's also now purposed for vector search. And we were quite surprised. I don't want to oversell because I don't work for GSI, but I had the exposure and kind of like firsthand experience. And what surprised me is that it goes to millisecond level. And you have indexed like 10 million objects. So if I was using, let's say, Elasticsearch without scaling, without sharding and indexing 10 million objects, it wouldn't probably go to millisecond. It would probably, I don't know, it would probably be like tens of milliseconds or hundreds of milliseconds, I'm guessing. It depends on query, of course. But here I was super, super surprised. It was going, I don't know, 10 milliseconds or something. What? It didn't do anything or recache everything, but no, it did compute from scratch. So that was interesting. Yeah.",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, that's mind blowing. And that kind of frontiers of computation thing is so fascinating. It reminded me of this little joke we had where when we decided to name Weaviate Air, Weaviate Air, Etienne goes, now we can't build airplanes. Sending the Weaviate satellite over. But yeah, so I think it's really fascinating. I think kind of coming out of the approximate nearest neighbor, the data structures, the vector compression, such an interesting topic. I feel like this is kind of the most technical aspect of it, in my view. So coming up one layer, then we would have, say, how you vectorize the queries generally. And you mentioned with the custom hardware, how you could have some particular way of batching the queries and with the offline query use case. So coming up maybe one more layer, and this is sort of the part with the neural search frameworks, or maybe I'm gonna get myself in trouble because I think that this kind of stuff should live in Weaviate. We have, well, it already kind of does. We have the Q&A transformers library where you loosely couple the vector search with then passing it to the extractive question answering model. We have summarization and this kind of thing. So how do you see that next level of the pipeline where you have the search results and now you want to process the search results a little more? So you might, well, I guess maybe one other little thing, not a little thing that I missed, was the combination of vector search with symbolic filters and how that would work with the ANN index. So just as a note for the completeness of the coverage. So now we have the results and we want to process them with maybe question answering, and we'll kind of, if we could start from there and then talk about other things we could do.",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, actually, maybe I can take a step back. When I was working on this blog post about neural search frameworks, it took me several months because I didn't see the picture coming together yet when I got the idea of publishing this. So to give examples beyond Jina and Haystack, you also have managed neural frameworks like Vectara, also Relevance AI and Muves, but I also included Hebbia. And this is where my head was blown away. I was like, hold on a second. So basically, if we are looking at the vector search pyramid, as we take a step upwards, basically we are moving closer to the user, right? So because we don't concern ourselves anymore, we kind of assume that ANN layer is solved to an extent. We can fine tune it, but it's solved. Vector search database is solved because it exists and I can pick a variety of them. So what is unsolved, right? So why do we need another layer? Why can't I just sit down and have all these components in my hands and kind of use it as a Lego building blocks and come up with my app? In principle, I could, right? I could take Weaviate, I could take Qdrant, whatever, and I could then build everything. And Weaviate offers a lot of these opportunities, as you said, for vectorization inside the database, so I don't even need to worry about that part, right? And then you have these components for summarization and things like that. And things, of course, blend now that we moved to neural search framework level, but it still was very logical. If you take a look at the blog, you will see each card for each of these neural frameworks will have a specific field saying, does it use any existing vector database? And if not, what is being used if it's kind of publicly available? And so if you look at, for example, Jina, Jina is using Weaviate, Qdrant, Elasticsearch and Redis, using as in, you can choose one of them, right? Depending on your setting. Haystack, kind of the same, but they also offer an opportunity to implement directly with FAISS. And coming back to your earlier point, why you need to choose a vector database over ANN algorithm is because ANN algorithm might not support the symbolic filters, right? So like if you take HNSWlib of the shelf, it won't support the symbolic filters, right? And the same goes to FAISS, it doesn't support symbolic filters. So it means that you will have to have some external database, maybe SQL database, where you will have to do this post-processing step. So after retrieving the nearest neighbors from the semantic perspective nearest to your query, now you need to post-filter them, using the metadata filters, but that can actually kill the whole list, right? So you will have to either over query and then hope that after post-filtering some items will remain, or you'll have to repeat it several times, effectively delaying the response to the user, right? So you don't want to do that. So like, you know, databases like Weaviate, qdrant and so on have the support to do this in place, right? In the same stage as they say it. So I think this is important, right?",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah. And that actually did change my perspective a bit because I could see like with the doc array having a connection to like a graph database separately from Weaviate where you aggregate the things and then those go into the re-ranking layer, the question answering layer. And yes, I can see that, I think, but then coming back to our not worry about too much and just your core value, if Weaviate can offer that kind of like how hybrid searches now in Weaviate and you can have the keyword search and the vector search and that kind of flow into your ranking thing, it's very interesting. So kind of one emerging thing with the neural search frameworks, and it comes back to this idea of looking at different indexes, a graph database, a vector index, maybe the FAISS static, and maybe you have some application where your data doesn't change. And so then the FAISS index makes a ton of sense. So there are these new things called LangChain, where the idea is it's the search framework is around prompting the chat. I think I like to say ChatGPT instead of GPT3, because GPT3 is impressive, but chatGPT is super impressive. So maybe we could call it GPT 3.5 or four or whatever. So basically the idea is it's like the orchestration layer where you tell the language model. So one way of doing it is this chain of thought prompting where you get the few shot examples are showing you how to decompose questions such as to illuminate the compositionality. If you're asking, did Thomas Edison use a laptop? So you'd want to break the question down first to like, okay, when did Thomas Edison live? When were laptops invented? And then you've taken the two facts. So it's like this layer on top that tells the language model, like, here are the different data sources you have access to. So what do you think about that kind of like how this chat GPT technology will influence the neural search frameworks? And then after that, I want to get into like the generalization to images, audio, like genes, maybe like other kinds of data types. But I think maybe just sticking with text would be a good way to sort of set the stage for this. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, for sure. And I wanted to still blend also with kind of like what the value prop is in this neural frameworks and maybe as a segue to chatGPT, how chatGPT could change things. So like if we take the example of Haystack, so for example, what they allow you to do is that the query comes in, you can have a node and the way they model this is they have a DAG type of thing, right? So they have a directed acyclic graph. And so the query gets classified, let's say, with a query classifier, that's one node. And then after this query is classified, it can, depending on the class that is predicted, it can either go to dense retriever, or it can go to a keyword retriever, right? Let's say maybe it's based on length or some other features that you know work. So you have a boundary in your classifier. And then, but maybe in some cases, even it could go to both of these. And then you will have a further node that will read the results from these retrievers and will merge them and then present them in some way using, I don't know, RRF method or some reciprocal rank fusion or some other method. So and you can like play with this, you can have like different nodes do different things like one node could be, if you classified the query as a question, you could do a question answering. But if it was like a table related, like SQL table, so you classified it as a SQL compatible query, so you could go to that node and say, hey, can you also query the table? You can also do like document similarities, new documents come in. So it doesn't need always to be, that doesn't always need to be like on the retriever side. It could be as part of your backend pipeline somewhere where you need to do document similarity and then decide whether or not to even compute an embedding for this document. Maybe it didn't change or maybe it didn't change enough to warrant a new embedding. And so you might discard it and so on. But you also have this other nodes, which we talked about earlier about document extraction process. So you extract things and proceed to the embedding layer. Coming back to your question about chatGPT, I had an exposure to it, of course, I actually, well asked it, can you name my blog post? Because I was a little stuck there and I grounded it and I said, hey, I wrote another blog post about vector databases and this is how it was called, not all vector databases are made equal. Maybe you can play on those words or something. But it decided not to use the same sort of words and just gave me a neural search frameworks. I had to add comparison. I was like, oh, boom, cool. You cannot imagine, you can do the work and then it leads up to the posting and you're like, how should I name it? Then you go to your friends, your wife, and they're like, how should I name my blog post? How do I know? So in these really strange situations, you can reach out to systems like chatGPT. If I went on duckduckgo or Bing or Google or something and I asked the same question, I probably wouldn't get an answer. I would get a bunch of links and like, what should I do with these links? This distracts me more than it gives me value. But in chatGPT, I got an instant answer and I was like, I like it. I spent maybe five minutes thinking about it and I liked it and I slapped it on the title. That was fine. So I think in some sense, maybe to me, this wasn't even a search experience in this kind of basic definition or sort of the way we used to it definition that, okay, I need to type something and then I need to examine links or examine some output, go check the results and then decide myself, like, am I satisfied or not? In chatGPT, you don't have any URLs coming back to you. Not yet, at least. Maybe they will be added. Who knows? But like today, it's more like a companion that you can talk to. In some sense, I was dreaming of such a companion. Maybe, you know, when you study, you have all these books and papers and everything, but can you really quickly make sense of, or can you find an answer to that specific nagging question like you had during the lecture? It's super hard, right? So of course you can go to search engine and start typing all these queries, but here you can have kind of like a sensible discussion in a way. Of course, I know some people were even hysterically laughing at the results and so on and so forth. So maybe it's not purposed for all situations and also for all audiences. It was actually a discovery for me. There was one linguist that I was following. He said that he cannot use general web search engines because every time he types something, they don't understand what he, they don't have the data. It's not even about understanding. They don't have the data. And so he needs to go to libraries and like read books that are not indexed in this search engines and things like that. So for these very specific niche use cases, maybe chatGPT might not work. It depends on the data again. But I think it was surprisingly clever, right? If I can say so about AI. It wasn't always static and you explained it well that it takes different paths in the tree when it computes the answer. And the other question I asked, like, can you find a bug in this code that I wrote and it just leaks memory at some point. It gave some sensible suggestions. And I felt like, I know that it's kind of like a silicon there. Like I cannot maybe like, I still need to examine some caution and sort of not fully trust maybe for life sensitive situations or something like that, you know, or medicine or insurance or something like that. But like things that I know it has indexed and humans have written that, you know, and it has been sort of vetted multiple times. And so also upvoted a bunch of times on Stack Overflow if you were talking about coding. And so there is some evidence that this might be the answer. But I think it was still surprising that how it changes the perception of search, even if we can talk about search in this case, that it actually generates the answer. You know, search engines don't generate answers today, like beyond maybe, okay, you.com and Google, you.com I think is more advanced than this, but like Google has this snippets, you know, where it says, you know, probably the answer is this. And so they commingle it with URLs. But like in chatGPT, you don't have any URLs, it just talks to you and then you can continue the discussion. I don't know. It was fascinating. But I still don't know if this will make it into the necessarily search experience. Like so in search, I think it's very functional. You know, if I walk down the street and I see something on like on the shop window, I take a picture and I say, I want this. And so it finds by the image. So that is still a search experience for me. So in some sense, maybe in the future, you know, we will have control F on everything in the world. Right. So like as I walk everywhere, I can kind of mentally press that control F maybe in some device, maybe on top of me, like glasses or something. I don't know. VR. But like today, a lot of places miss this. And still, there are a lot of contexts and situations when you ask yourself, what is this? Do I know this? You know, and you have some other like subsequent questions, but there is no way to ask them because you can pull up the phone and start typing and it's freezing weather and you're like, oh my God. It's kind of like a deteriorating experience. But I think it could be so much more interactive and multimodal. And I think neural search especially enables multimodality situations, right? And experiences so that you can actually like not constrain yourself to the point that am I asking like a textual query or I just have a query. I have something on my mind, right? Or maybe I saw something. Can you tell me more about it? So I think maybe chatGPT might push us in that direction that not only it will find things but it will also reason about things and help you reason. But the creativity part, I don't think it will disappear. I don't think, at least not now, I don't see how AI can solve creativity part, like create things for you. Yeah, it did create the title, you know, but maybe a more creative person than me could actually create a better title, right? And things like that. So yeah. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "There's a lot that I want to unpack and I do think this, the creativity is kind of like a characteristic of compositional generalization and novel. But I want to just kind of tell you about one other idea that relates to how you had chat GPT come up with the title for your blog posts. And so I want to credit Bob van Luijt and Jerry Liu, the creator of GPT index. They included me on this call where they were, you know, hashing out their understanding of the GPT index top level indexing. And I just think this idea is so profound on how we use chatGPT. And it's the idea of when we search and we get like 15 results, as you mentioned, we need to like parse through the result. And so like one thinking was like, how about we use a crossing, like a high capacity cross encoder, which is like going to be another, like, let's say it'd be like maybe like an 80 million parameter transformer. Their paper is where they use like big T5 models, like billion parameter T5 models to re-rank. And there's like this paper where you have the density on yes, no. like query, and then you put the query document, document, and then yes, no. And you re-rank with that and you use high capacity models, similar to log prop, that kind of idea. But so this idea of like, how do we parse through a bunch of results? And then another idea was like, okay, well maybe we use a question answering model and we'll re-rank it based on the confidence of the extractive question answering model. And we'll try to calibrate the question answering model to demonstrate uncertainty, maybe Bayesian networks, something like that. But this new idea of having GPT summarize the results by having the original question and then saying, please summarize these results. You'll receive it one by one. And then it receives it one by one, updates its summary. Maybe as you mentioned, like you would want to have the reference. It could maybe say like, oh, and also please like, you know, keep a queue of the most influential results as you've been parsing through it. And it's like ability to reason and do this, I've been playing around with this a little bit, I think is just super profound. And that, so that kind of summarization across results, what do you think of that idea? Because I am mind blown by it. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, this is amazing. I'm not as deep in this topic yet, but just on the surface of it, listening to what you just explained, you know, there is still this trust component as well that, and again, this trust is just how we perceive it. It was designed that way. And we think this is the trustworthy way to vet the information that I'm getting. So if I get the URL from the search engine, I can click it and see it with my own eyes and see when it was published by whom, and maybe I can even reach out to that person and ask some questions. So if I'm not provided with this information and evidence, how do I know that this is true? Right? So, or maybe does it even apply to my specific situation? Maybe it gave me way too generic answer. And so I think it will be interesting to see if I have a list of people that I follow on Twitter, let's say, and I trust them on specific topic, you know, let's say the way I trust how you, Connor, publish, you know, things about specific papers or some breakthroughs and recent implementation in vector search space. So when I have a specific question, maybe I could say, Hey, let me first check what Connor thinks about this. Right? So if chat GPT could kind of like bias the answer and include some of the hints from you, from your timeline, not from your timeline, but from the things that you publish on Twitter on that specific topic. And then include that as a supplementary material and maybe like a chapter sort of, you know, in the answer. So I can, I don't need to, you know, go and check Twitter now. I can actually go directly to the information, right? Barebone and I can read it. And then if I have a question, maybe if I reach out to you, I might ask a very specific question rather than saying, saying like, can you point me to a paper, you know, in this topic? So, so I think if I think maybe chatGPT made that first important step. It created also a lot of like, maybe controversy or some people I remember like one case on, on stack or on hacker news was so this guy's parking his car in, in wrong, I guess in the wrong lot or like in the wrong spot in the, in the parking lot. And then he gets a fine. And then he's like out of options and he's thinking, okay, what if chatGPT can help me here? And so he asked chatGPT to write an email to this official, you know, say, yeah, I put on my app and I paid for it, but apparently I, I put my car in the wrong, you know, slot and spot. And so, and so it was like, like, you know, like how in certain cases, certain situations, like we, when we zoom in on a situation and we are a little bit like stressed, we'll lose words and phrases. And that's probably why psychologists and consultants exist because you run towards them and you say, Hey, I have a problem. And they will calm down, you know, what happened and that they will help you to walk through the situation. And then they will say, say like this, you know, dear official, whatever, I happened to park my car in the wrong spot. I understand this is a big mistake. However, I paid for it in my app. Here is the receipt. Do you think, and then like, you know, it's like the computers, they don't, I think this is also a good thing. Like to some people it's bad thing and it can be, I guess, developed towards that. They don't feel, they don't have emotions, right? They may pretend to have emotions like in ex-machina, a movie, right? But like, but I think they are very calm and like, okay, your problem is this, you know, like Sheldon Cooper. Okay. Here is the answer. And sometimes it may help you. And actually in that case, the official came back and said, yeah, I understood the situation that happens sometimes. You know, I will just issue a warning this time. You don't need to pay fine and you are good. And do it next time. That's fine. Right. So probably I got carried away a bit, but I think that coming back to your question, like if I can, if I know that there is a very interesting book or blog post and it's in my browser history, it's accessible and I can give access to it, to chatGPT, it can go and personalize to my interest, however biased it is. I don't care. Maybe I do want to be biased. I don't want a random blog post published somewhere. Well you can add it if you want, but still, can you bias to what I have read and I have already forgotten, you know, in the several decades that I live, I've forgotten that I read that book. Can you remind me of that? So yeah, I think there is a way to take chatGPT to the next level in terms of personalizing the answers. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah. And I think, well, personalizing the answers, I think it comes, well, coming to our other podcasts where we talked about ref2vec and the personalization vector and how that can filter the search results. And then those are the results that go into chatGPT as context. But I think the other thing you're saying, one, I think like this prompting is so interesting. Like one thing I've played with is write an argument between Ilya Sutskever, CEO of, I don't know what his job title is at OpenAI, but write an argument between Ilya Sutskever and Clement Delangue from Hugging Face about closed versus open source models. Right? Like you prompted like that, you give it like a background about each of them and the argument. And I think also what you say about polite writing, like one of my favorite ways to use chat GPT is I'm writing something, I give it a sentence and I say, could you suggest seven writings of this? And it's funny you mentioned that it's just Silicon, because sometimes I'm like, oh, seven, that's kind of a tall task, maybe just three. I think that kind of comes into also like when it's no longer free, it might change how we use it a little bit too. Like the way I use the GPT3 API, I'm like, well, I'm paying for the tokens generated. So let me not prompt it to give some long generation. But so I think this has been a great coverage of chatGPT. Continuing on the pyramid, I kind of want to hit what I see at the top level, which is just the user interface, like the, you know, like someone with CSS skills, how they contribute and how they fit into this. And I've seen like Jina now, you know, I know we have some stuff that's not out yet, but like on this kind of the search interface. And I know like, usually, you know, you just have like a search box, right? Like, it's just the bar is like kind of interface. But say you're doing like image search, and you want to like click on the images, you want to fuse that with the search box. And I don't know, do you see like a lot of in I know, like you.com, and it kind of comes into what you're saying with how you have the evidence as well, like you.com, they have this interface where they split the search results, and then the chatGPT, like this interface, do you think there's opportunity for innovation at that layer? ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, I think one one part is innovation. And another thing that I've been thinking about is, let's say, there is an existing player, they have a search engine was recommend recommender system or something. And they're thinking, okay, can we experiment with vector search, but we're probably not like 100% sure yet, is it going to fly? Or, you know, can we expose it only to sort of like the power users in some sense? I think Jina had this kind of like, early stage demos in some sense, early stages in, you don't go with this in into production, but it helps you to reason about and test what's more important, I think, the influence of neural search on your, you know, search engine experience. So they had, I think they had this search engine where, like a demo where you type a query. And then you have this slider, I guess, so you can say, you know, have more weight, put more weight on keyword results. And then you slowly blend this into the neural search. And then, and then you can choose like, okay, it's going to search directly in the images, let's say using clip model. But still combine the results. So still ask the keyword retriever, and then combine the results from the dense retriever from the clip model, and sort of like, show me where these results land, right? So if, because for some queries, it doesn't make sense to check the image, because it's succinct enough. And it looks like it's going to match a specific metadata, you know, item, filter, or maybe title of that object. So there is no reason really to go and examine the image because it doesn't contain this information. And vice versa too, like in some cases, when I say, I don't know, like, can you give me a picture of a bear eating a fish in the river? You know, there is no such title. But you do have an image of a bear eating the fish in the river. And you're like, yeah, maybe if the model, and we have done this in the clip search engine demo, that essentially, it surprises you, blows your mind that the clip model can summarize its understanding, so to say, to this level that it will match anything you type like that. So you say bear, and it understands what the bear is. It's not an otter, things like that, right? So this is very interesting. And of course, it goes back to, you know, contrastive learning and you can have enough negative examples, which are semantically negative and not just random negative examples and things like that. But I also came across, I wonder if I can pull it up. And when I published this blog post about neural search frameworks, I came across this company called Nuclea. So I didn't yet include them in the blog post, I need to study this a little more. But there was some interesting thing that they offer a way to compose the UI as well, right? So not only, this is coming back to your question, you know, can we sort of make some breakthroughs there? Of course, if it's an established player, they have the dedicated front end team, they will probably figure out what they want to do, and they have an existing product. But if you are on the verge of experimenting, right, so you're still there, you have your like mind open in many ways, you don't know what UI will be in the end. They offer beyond, you know, a database for unstructured data, they also offer you a number of components on the front end side, so you can basically compose the UI the way you want. And I think like you.com, I think you mentioned also experiments with chatGPT like answers, right? So not only the URLs, but also kind of like an answer, which is more interactive, and maybe you can continue the discussion there in that box. I also want to like, give an example, which was like pre neural search era in many ways, at AlphaSense, like, when I look at document search engines, you know, let's say it's an article search engine or patent search engine, usually you will have like, several stages as you go through the UI as a workflow, so first, you need to type the query, then the screen changes to the list of documents sorted in some way, then you click on the document, again, the screen changes and it opens the document, right? What AlphaSense did really, really early on, like 2010, was to have what we called a three pane UI, so you have like, you have a search box at the top, you type the query, press enter, you get a vertical column, you know, a column with the results, not too narrow, you can still figure out what is what, you can read titles, you click on that, it gets the snippet pane, so you can actually quickly gather, okay, is this relevant to me or not? So you have like, several stages, but it's all on the same screen, almost like an Amazon checkout page, right? Kind of, and then as you click on the snippet, it pulls up the document, however big it is, maybe let's say PDF document, thousand pages, it will load only the necessary chunk of that document and it will jump from that clicked snippet to the relevant section of the document, so now you basically have all these, you know, tools in the same view and I think this is very powerful because it saves a ton of time, you know, because you need to always kind of walk in the shoes of the user, what does the user want to achieve with your, not with your product, they hire your product to get some job done, right? And so is your UI efficient enough in, you know, in satisfying that specific request? And so I think this was very interesting and then some competitors tried, not only tried, but copied, you know, this view and then the history goes on, but I think this was a very interesting breakthrough, even like before neural search, but with neural search, a lot more doors open because, and also it brings that complexity layer that now product managers, engineers got to simplify, like you need to simplify the complex always because the users will not have time to figure out really complicated designs or UIs, however flashy your UI is, if it's not functional, it's not going to fly. So you need to kind of like simplify the complex and now these open doors with multimodality, right? So like all of a sudden your query can go directly into inside the image, now it pulls up the image, you need to explain to the, you cannot highlight like a snippet inside the image, right? And say, Hey, there is an arrow here, that's the bear you were asking for. Like inside the patent, like in patent search, I spent some time, I was part of the board in one big enterprise, basically examining the patent to be the patent applications and a big chunk of work for a patent presenter would go into examining the prior art. And so they need to examine a ton of patents and figure out whether or not they overlap with that specific patent or not. And so this means that the search workflow changes from get me something on the screen and I'll decide if it's good enough to, can you give me everything there is on this topic? So it's like a long, long search. You paginate like how until like 200, page 200 or something, and you've got to have like a hundred results on the per page or something like that. So they spend like days examining just one query. And then you go back and say, Oh, my query is missing this term. You know, let me change my query. Boom. I have again, the new list of results. And now it would be cool if the system showed me the difference, right? This was another feature we had at AlphaSense, you know, as new documents come in, let's say you have a, let's say if it's a public company and they publish a 10K report, which is like a yearly SEC filing, it might include portions of a quarterly report from a quarter prior. And so, and also like that 10K, which was published a year ago, they don't actually rewrite it. They just change some numbers, right? You know, like our performance, our top line, whatever. You don't want to reread 700 pages again to learn, has something changed with this company? You just want to see, Oh, the top line changed. And also, you know, they spend a lot more on production of that sort. And like, okay, now I need to go back to my Excel and input these numbers and see what happens to the stock price prediction. Right? So I mean, always think, I don't know if I answered your question well enough, but like always, like forget about vector search, forget about learning to rank, however sexy those are and those are super cool. I mean, I'm excited myself. Always go back to what user wants, right? Like if they're driving to that destination as we compute in TomTom, always remember I'm sitting in the car, it's freezing, it's dark. I want to get there. So show me that top result as soon as possible so I don't need to type that long, you know, sitting in that freezing weather or whatever. So I think it's a good exercise to always kind of go back and maybe talk to the users as well. In some cases, it's more maybe complex, but I think still that pipeline could be established and you can start asking questions, okay, what are you trying to find? What is your normal sort of workflow and use case? What are you trying to optimize for? And they will give you very interesting, sometimes confusing answers maybe, so you can drill in and find sort of a more detailed version of what they wanted to say. But then it comes to the, rolls up to this bigger picture. Ah, I got it. We are just missing one button here, something like that. And then.",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah. Bob had taught me about this jobs to be done framework for thinking about this. And I think the jobs to be done, it's like this business school thing about like, why did you hire the donut to do the job of something to eat on the way to work? I think of the user interface layer, you think the most about the job being done. I think that was a great, and yeah, the job to be done has cascading effects to the whole pyramid and the different requirements all the way down. I think this is a great summary of the pyramid and then a transition into a really fun topic to wrap up with, which is this idea of renaming vector search to maybe relevance.",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, no, I actually wanted to pick your brain on that because I know you, the moment I posted that podcast, you have responded and that was like a long answer. And I was like, Oh, I got Connor and he's like really passionate about this topic. I think this is important. Like I will tell this disclaimer, it's not my idea, but I like to throw in some thoughts which are of higher level and maybe the goal is to not change the course of the industry, but more like inform and give another perspective. And I think this perspective came from Doug Turnbull on one of the episodes of Vector Podcast where he said, if I was to give an advice today to the vector search engines, to the vector databases, I would say, stop calling yourself a vector database. And it was like a cold shower already, like stop calling yourself a vector database and call ourselves what and why? But he spent a lot of time in search, right? He wrote a book, Relevant Search as well, co-wrote it. And he's co-writing AI-powered search as well. And so he said, well, think about the nutshell of what you're building, right? It's like what new area you are discovering, right? On one hand, we are all building or participating in the construction sort of like rise of new industry. But in the end of the day, it either solves or doesn't solve some use case or plethora of use cases, right? So we claim that we are moving to semantic search level, right? Something that didn't happen before, something that was hard to achieve and maybe with custom synonyms, synonym tables, or with bootstrapping another shard to handle that language. Now we have multilingual models. So they handle multiple languages in the same representation. And so it's the same shared geometric space, which is kind of like super cool. I can ask my question in one language and get it searched in another language. Then you still need to deal with the answers. Like do I need to translate them back to the source language or something? But that's another story. And so he claimed or he kind of like suggested that in a way we are solving towards relevancy. And so he said, why don't you call yourself a relevance-oriented application? And he wasn't stubborn on that specific term or that specific phrase, but I guess the key word for me that stood out is relevance, right? And then I also wear my hat of a product manager and thought, okay, if I go to my boss at TomTom and I say, hey, let's bring this semantics into the mix and guess what, we need a vector database. So let's acquire a vector database license or something. I think the first question will be, what the heck is a vector database? Is it like vector clocks? I used to have some queries or something. No, I'm just joking here. But I'm saying that, you know, or like it becomes like this kind of engineering lingo or like they say Greek, right? So speak English. I don't understand you. And so then you say, oh, you know, vector databases, it's like a new breed of databases. It's like the next stage SQL. I was like, SQL? Wait a sec. Just forget all this. It's basically like moving everything using deep learning, moving everything to deep learning. Okay, hold on. So semantics, you get the word semantics, right? So like we move to semantic level searching and by the way, we can ask natural language questions now. Oh, natural language questions? What is that? So it's like, well, just normal questions instead of typing keywords. You can basically, you see what I'm doing? I'm basically like stepping back and back and back. And I'm kind of like degrading the terminology from, oh, this is like a super cool aircraft. You know, and I've used all these materials to build it, all these glowing buttons. What can it do? It can fly you to Mars. Mars? I don't want to go to Mars. You see what I'm saying? Like step back as far as possible back and say, you know, remember we have this problem in TomTom. Sometimes people ask questions. So they say, can you drive me to a lake or something? So it's like, they don't type like an address because they don't know the address of the lake. Lake doesn't have an address. Maybe it has the coordinates. And so we have certain percent of these queries and maybe we can tackle them with this new tag. Then you can maybe say vector databases are the new breed of databases. But like, yeah, I guess when you enter a discussion with an unprepared customer, you might not start, maybe it's not a good idea to start with vector database or some technical term instead, you know, talk about semantics or relevance or something like that of that sort. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah. Let me see if I can do, can you drive me to the lake? I think maybe let's see if that example can show the different intents that that could entail. So maybe you would want to be asking it, like, are you capable of this task? And it would say like, yes, I could show you how to drive to the lake. Maybe it would find another question where someone else had asked something like that. Or you know what you want probably is like the Google Maps directions, current location to the nearest lake. So I've been seeing this like intent ranking papers like task aware retrieval with instructions and the instructor model where, yeah, like there's different intents for different search tasks. Like so with Quora duplicate question detection, like the academic data set, you're looking for another question, not the answer to the question. And so it's, I don't know too many cases, like it's kind of like the idea where you encode the domain in the task. Like you say, search me a paragraph in scientific literature compared to in Reddit. Like it could have like find me a, you can say like, find me a conversation on Reddit, which is a different intent than find me the answer from Wikipedia. Is this helping? Like this kind of, so I don't, so that's kind of how I'm thinking about it. I mean, I still think, I still think the vector search database, I like that so much because it kind of comes all the way back to the pyramid where it's like this coupling of the ANN and then the database stuff. And just maybe it's particularly to how it's done in Weaviate, but that those two are so tightly coupled, sort of, I see it as that, that's sort of like what I see as a novelty, but yeah, it's really fascinating. I think relevance podcasts, welcome to season three of the relevance podcast. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yeah, actually I was surprised or maybe it confirmed the bias in a way that when I was writing this blog post about neural search frameworks, one of the players just called relevance AI. And when I was talking to Daniel Vasiliev, who is the CEO of this company and co-founder, he was advising me to even stop using the word vector and embedding. And I was like, what do you mean? And he was saying, well, our user base and our target user base is not necessarily engineers, is not necessarily deep learning researchers. It's anybody. It's someone working in HR and they have a bunch of CVs and I guess they want to like take a look at them from a different angle, quickly find a candidate, maybe plot some characteristics quickly, cluster them, things like that. And so the progression of thought is not like this. I have a bunch of CVs, what I'll do, I embed them. And then based on the embeddings, I will compute the clusters and then I will run vector search. You know, they don't even know what these things are. They go on relevance AI platform, they upload CVs or they point to some archive or like cloud URL and then the system pulls them down and basically extracts all of these things that we described in this episode, you know, the workflows. It does it behind the scenes, so you don't need to worry about it. And then it embeds the documents and then it basically gives you the search prompt so you can search them, right? So they target, I guess, a completely different user base, right? So like if you contrast that to, for example, what Haystack is doing, and I was also talking with Malte a lot as I was prepping the blog post, he was saying that this is kind of like a development, kind of like an IDE in a way, right? So like Deepset Cloud is kind of like an IDE for integrated development environment for an engineer or researcher, or maybe both of them collaborating. And they can even chat so you can find references of the discussion that happened earlier and like go through that and things like that and make decisions together and then plot metrics. So it's a different user base, right? Like the target user, ideal user is somebody who understands coding and understands the concerns of scaling and cost management and whatnot. And so they are much closer to the actual platform creation, right? So but like if you're talking, like someone reached out recently to me on LinkedIn in product management capacity and they said, yeah, we know that you published all of this, but can you give us like a couple blog posts or maybe a podcast which introduces us to this space? So I don't think I will send them like Haystack URL, right? Because it will be way too technical for them. And Haystack has like excellent documentation, but it's purposed to engineers and researchers in a way, right? And then of course there is, and there is a product for every of these choice or sort of business model, right? So like a product to be created. So for in the case of Haystack, they have deepset cloud, so you can basically subscribe, pay for it and don't need to host and like worry about how I scale, if something breaks, right? Same with Weaviate, you know, you guys have a cloud as well and things like that. So I guess to wrap up this thought is that it kind of like depends on what your ideal target user is, right? And also what use cases you have tested your tech with, you know, that another use case like this, we know how to work through it. And so that's why in each card in the blog post, you will also find use cases list, right? And I tried to focus not on, if I could, not only on sort of tech level or kind of like algorithm level thing, but actually specifically on the end user facing use case. For example, in the case of Haystack, they have, you know, document information extraction, but they also have, you know, getting revenue numbers from a financial report. This sounds more like a specific use case now, right, or like reason for a legal claim. So it's not like, okay, I have this neural network. How do I plug in that into qdrant or Weaviate or Pinecone, right? Is there a plugin architecture that I can implement? You know what I mean? Like, so it's a different level of abstraction and it's a different level of discussion as well. And so I think when Doug, I believe, kind of like deciphering his thought process was to kind of stop talking only, maybe only about tech. And by the way, I love vector database myself, that term. My podcast is Vector Podcast. Many people think it's about vector search, but it's not only vector search. It's like I, in the, in the about on YouTube, I actually write that it's like a vector as in what vector you have in your profession and life in a way. But many people come back and say, Dmitry Kahn, the creator of Vector Search Podcast. I'm dumb. It's not Vector Search Podcast, it's just Vector Podcast. But that's okay. I like the sound of it too. Vector Podcast is very cool. So I guess to wrap up again, really close off on this, that, so first of all, A, it's, it's about the niche. It's, it's the, not the niche, but the sort of like your ideal user that you're going after. And B, think about, are you limiting yourself unnecessarily in the way of sort of how many users, what type of user you could reach with your, with your system, with your engine? Because as you said, for example, Weaviate could in principle eat away some functionality from neural search frameworks. So it kind of like begins to occupy these two layers in this vector search pyramid, right? Or blend them and that's fine. But like, is it limiting, too limiting to say that it's only a database and it's only a vector database? Maybe it's more than that, right? So that's just a couple of thoughts there. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah, it's really enlightening. And I want to give credit to Sebastian Witalec and the DevRel team at Weaviate that's helping me think like this. Like I guess like me personally, I have a background in like doing academic research in the PhD and you know, reading machine learning papers and thinking very along the lines of like vector search database makes a ton of sense. Right. But like, I think one example is Erica Cardenas on the Weaviate team also, she created a, like a dog image search demo. And so it's like, there could, there are like layers to how you want to present this. It could be like, just point us to a folder with images in it, right? That was like the most simple thing to do. Or it could be like, okay, well, we're going to actually encode this into a base 64 encoding. We're going to use a ResNet, we're going to use the ResNet18 is, you know, probably don't need the 32. It's like, how much detail do you want to cover? And I think it's super interesting. And like by communicating that way, you'll unlock the like creativity of the people who are thinking at the higher application layer and, you know, paying the bills the other way is because I wonder how many times we've said that phrase in the hour and a half. Yeah. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Yes. It's like decision makers, right? Sometimes you, sometimes it actually is going the opposite way. It's like, you may spare some time as an engineer or researcher, test some algorithm, show results, impress yourself, impress your colleagues. And then this rumor will travel to the level of product management, decision making above product management as well. And they will say, wow, this is super cool. Can we release this to prod now? Right? Does it happen frequently? I don't know. It depends on the company, I guess. It depends on the culture. And I guess we could also spend some time talking at some point, you know, how companies are structured and how you have this, I think it's Conway law, right? So that your product is a result of how you have organized people in the company because they will have like teams, silos, maybe at some point even, and some teams might not be talking to each other as much as you think they could. And so your product architecture will be a result of this information architecture, which is also very interesting. But for startups, it's probably not the problem. For startups, you can talk to anybody anytime, right? But you still need to get job done. So that's also another perspective. So yeah, I mean, this is very interesting topic. In general, a lot of angles to take. And if you always like remember whom are you talking to, and just to wrap up on that thought, like if the breakthrough doesn't happen from the engineering level, can it happen, let's say, in product management level, because you have resonated with how they think, and you know what type of issues they're trying to solve. And you have stepped back from the technical terminology, and you started talking bilingual, and they said, here is what vector search actually is. And then they will go back to the drawing board and say, ah, this is what it enables us to do. And now we can search inside the images. Wow. That's super cool. Yeah. ",
        "podNum": 34
    },
    {
        "speaker": "Connor",
        "content": "Yeah. I mean, I think this was an amazing podcast, beginning from talking about the opportunity cost of bad search in retail, and then taking it into, okay, so you're building a neural search framework, there's all these components to it, your famous pyramid diagram, and walking through every step in detail, then coming to thoughts on chat, GPT. Yeah, I think just the user interface, the jobs to be done, this whole thing, and the renaming of vector search has been such an incredible podcast. Dmitry, thank you so much for your time coming on the Weaviate podcast. And thanks also so much for hosting me on the Vector podcast. I'm such a fan of the Vector podcast and what you're doing. It's so interesting to hear all the different characters. And I think sort of the role you're playing in being like the mediator of the market this way and hearing all the diverse voices. Hopefully I didn't give up too much of what we're thinking, but this kind of diversity of hearing what everyone's thinking, it's just so fascinating. And thank you so much. ",
        "podNum": 34
    },
    {
        "speaker": "Dmitry",
        "content": "Thank you so much, Conor, for hosting me. And this was an amazing time. Always glad to and happy to exchange with you all these thoughts and almost like in a brainstorming fashion. And I'm excited for the future of this with chatGPT, and all the breakthroughs that you guys are working on and many players on the market as well. Thank you.",
        "podNum": 34
    }
]