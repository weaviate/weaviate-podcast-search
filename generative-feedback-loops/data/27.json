[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone. Thank you so much for checking out the Weaviate podcast. I'm super excited about this episode. We have Michael Goin from Neural Magic. Neural Magic is such an exciting technology. We've plugged it into Weaviate to see how you can do this, make faster predictions with your models. So they call this inference acceleration, this general space of trying to make machine learning models, make predictions faster. They use all this amazing technology around sparsity and quantization. It's just such an exciting technology overall. And I'm so excited to welcome Michael to the podcast. ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, thanks for the warm welcome, Connor. Super excited about what we're enabling here together, particularly in vector search and partnering with Weaviate. Yeah, we're pretty new to the space. In general, we have a lot of experience in computer vision and natural language processing in terms of question answering, classification, your usual sort of NLP. But the whole space of vector search, information retrieval, things like this are really exciting to us and seems to be a lot of space for optimizing the future of databases.",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's so incredible to just change out the text vectorizer and point it to one of these Sparse Zoo models and just see how fast it can turn text into vectors, turn images into vectors. And also these other upstream things like, as you mentioned, question answering, classification. So could we kick this off by describing what is neural magic? How is this faster model prediction achieved? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, so maybe if I just give a bit of a background on just where we're coming from and in our technology. So at Neural Magic, we're building what we consider to be the software or really the foundation for software delivered AI. And this is because as machine learning models grow larger and larger, and particularly in the last couple of years with transformers and large language models, we start doubling the numbers of parameters every, even multiple times a year, from four to eight months is just the usual cadence now of talking about doubling of parameters. And so this massive growth in model parameter size means that we've had to use specialized hardware accelerators in order to keep up with these state of the art results. However, using these specialized hardware accelerators can be incredibly difficult for organizations to manage. Because really, if you want to optimize these models and get the best price performance you can out of them, it can quickly become a big challenge to determine which models you can run on which hardware and how to optimize each model for that hardware. And this is the exact opposite of most application infrastructures. And where we've already gotten really good at managing CPU based infrastructure, you can simply elastically scale what virtual hardware you have based on the actual application demands. I mean, you know, can you imagine if every time you wanted to build like a mobile app or a website, you had to wait for your application server module, or an extra thing you need to plug into your data center or get access to from your cloud provider. There's a reason this doesn't happen anymore. And the industry always evolves into simple open commodity hardware and software. And so that's what we're bringing here with Neural Magic. We're providing the open software piece and enabling all this commodity hardware you already have available such as your CPUs to start thinking about AI inference for those platforms. So really excited about that. And through that, we're, you know, we do the usual sort of inference acceleration things of, oh, you want to quantize your models, bring them to lower precisions, and get that extra memory and speed benefit. But we also focus on sparsification. So this is a process where we're pruning away individual weights of the model while keeping its overall structure. But think of it as just another compression algorithm. You have all these zeros in your weights now, and you can just simply choose to not represent them and not calculate them. So this results in a memory and speed savings. Now, the research for sparsification has been around for several decades. People have been sparsifying the models mostly for size, you know, compression benefits. But in terms of seeing actual inference speed from it, particularly on the hardware side, just hasn't fully been explored. And so we focus on making smarter algorithms and heuristics for executing these sparse models in a sparse native way, rather than just throwing, you know, dense matrix multiplication at the problem and kind of brute forcing these models like traditional hardware accelerators do. Yeah. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's incredible. And I think there's a lot in that to unpack. I love this argument around, say, the CPU, the commodity computers and how, you know, having the GPU is this bottleneck compared to how you package software applications. Hearing about the pruning and getting into the science that we're definitely going to unpack that a little more as well. But we also hit a little more about this, just kind of like these numbers and some of these claims that I came across in Neural Magic. I saw these images where it's like BERT large, distill BERT and this BERT large running, a sparse BERT large running just as fast as distill BERT. And now that I've plugged this into Weaviate, I've seen it myself firsthand. And so can you tell me about just like these numbers that you've all been able to produce? Because I think these just jump out to people when you're scrolling through LinkedIn and you see one of these bar charts, it's like five seconds on CPU, six seconds on A100 and say 13 seconds on T4. ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, so, yeah, great pulling out all the results. There's a lot to go through. We're coming up with new results every day. But I especially find that first example you brought up really interesting. And this was actually research that we did in collaboration with an Intel research lab. So Intel themselves are interested in sparsity, even if they don't have, you know, inference acceleration for it. They're still interested in the research. It's another form of compression alongside quantization. And so actually, Intel research labs over in their Israel base have been working on greater and greater sparsity, achieving greater and greater sparsities and contacted us for for using our engine as a sort of runtime for these models they're producing. And they published this paper called Prune Once for All. And so it's basically this idea that, you know, what we call sparse transfer learning. But the essentially the idea that you can actually prune your pre-trained upstream model. So just like you would take a BERT base from Hugging Face that's been pre-trained on Wikipedia or some large text corpus in a sort of unsupervised fashion. Then you take this pre-trained model and fine tune it for your specific task on a much smaller set of data. So what we found out with them is that you can actually sparsify these models in the pre-training stage. So now you have a sparse architecture that's pre-trained and you can just maintain the sparse architecture we found of it and fine tune it onto all your data sets and results that they publish and results that we publish in our research lab as well is that the sparse transfer learning generalizes quite well across a variety of data sets. And sparsity can actually help the generalization of models, which is really interesting to think about in that maybe your first intuition isn't that. But the intuition we've kind of come across is not having, you know, those millions and hundreds of millions extra parameters means that the model has to make each parameter count a lot more and take a sparser, more general representation in order to maintain the accuracy of the whole model. So it's really interesting, this dynamic play of, you know, maybe starting with dense models that work with too many parameters that work really well with our very stochastic methods of training where we throw more data at it. But then getting to a good base accuracy and then working on sparse architect, finding sparse architectures to, you know, go through the step of optimization before we just consider deploying these dense models. So, yeah, in the case you brought up, yeah, we were able to get a BERT large model to 80 percent sparsity with quantization and get its speed to the same level of a distilbert model, which has, I think, eight times less parameters, maybe even 10 times less parameters. And at maintaining essentially the same accuracy of the original BERT large model, just staying within, we usually target 99 percent of the dense models accuracy as a as a as a place for sparsity to keep us. And, yeah, getting that 8x improvement on BERT large to keep its accuracy much higher than distilbert, but get to that same latency. So, yeah, really opening up what size models we can or what accuracy models we can really consider for deployments nowadays. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, and I want to like firstly congratulate you all on having this like science and product. It's so interesting. The BERT surgeon, this prune once for all, if I'm getting the name right. It's so cool seeing the science and the product together. And and so so I really want to get into the sparse zoo and how these models come to be the difference. I'm starting from the sparse transfer learning. I find that to be absolutely super interesting the way that, say, you sparsify a language model. And then, you know, we do in vector search a lot as we train with contrastive learning. We have maybe triplet losses or we have this multiple negatives ranking loss. So we can take a sparse model and then fine tune it with our kind of contrastive learning objectives to produce vectors. So what is the state of this sparse zoo? How do we sparse transfer learn, get new sparse models on the zoo? What does that all look like? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, so great question. So sparse zoo is a website you can go to sparse zoo dot com and you can check out the hundred or so models that we host, both their dense sort of baselines and then all of the sparse and sparse quantized models we produce since there, you know, for a variety of tasks across computer vision and NLP. So definitely encourage you to check that out. But yeah, within there, this is where we have these sparse upstream models or even these sparse downstream models that are that have been produced by our research, our ML researchers and engineers. And the key thing is that everything we push up there is reproducible. We have commands ready for you to make them with the rest of and we use this concept of recipes, which are really just YAML files, but controlling all the hyper parameters and pruning modifiers, quantization modifiers, basically laying out a whole schedule of training for this model or optimization, depending on where you're starting. And that all folds into our open source machine learning library called SparseML that plugs into and builds on top of common frameworks like PyTorch, Ultralytics, Hugging Face Transformers, things like this. So, you know, both with like, you know, in the Hugging Face Transformers case, you know, we have our own trainer class that we've inherited from the Transformers class and added sparsification to it. And so, you know, once you pip install SparseML, you can build your ML application with this trainer exactly in code or we have CLI commands like SparseML.Transformers.Question Answering, for example, and you can just point it to a model you want to train, reference the recipe in SparseZoo to follow the same path we've gone, choose your data set, choose your hyper parameters, use essentially like you would any Transformers call. And we really hope that helps people reproduce their results and bring them onto their own custom data sets. It's all open source, so you can contribute your own custom pipelines and maybe training losses, things like this. And so, yeah, sort of all scattered across READMEs and documentation and on SparseZoo itself, there's commands and guides for each of the models that is part of the process of uploading a model. You need to have the SparseML commands for training it, evaluating it, validating the accuracy and the DeepSparse commands for validating the speed that we measured as well, or, you know, acceleration. So, yeah, that's kind of an overview of our ML side. And then, of course, on the inference side, we have the DeepSparse engine, which is free to use for personal and research. So we're really excited about what people have been building with that, you know, in our Slack communities and on GitHub issues, people using it for varieties of models. We haven't even decided to sparsify ourselves yet. They've found success in medical imaging, super resolution, even, you know, subclasses of popular NLP tasks that we haven't gone for yet, like multi-label classification. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super interesting. And I definitely want to get into benchmarking as we've talked about how we, you know, clock these things in the difference between, say, 128 sequence lengths, and, you know, the difference between, say, 128 sequence lengths or 384 sequence lengths. And I want to get into that. But on this topic of recipes and the Sparse ML trainer, I first kind of became familiar with this kind of recipe thinking of what goes into a trainer when I was learning about Mosaic ML and their composer. And they have all these different regularizers that you, you know, package into a recipe to train your model with. So I know a little bit about pruning. You zero out some weights and there's like iterative pruning, maybe second order pruning is also something you do. What is the recipe for the Sparse ML? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, so yeah, recipes and Sparse ML can hold a lot of information. You don't have to use them for pruning or optimization in general. They have very simple building blocks with them. Simple things like learning rate modifiers. So how you want to modulate your learning rate over time or introduce ways to boost accuracy like distillation from teacher models or dense upstream or even dense downstream models. These are all things that help improve the accuracy, improve the robustness of training your model. And so, so yeah, really a recipe is just the base thing to it is you define a start epoch and an end epoch. And then you list out all of the modifiers, all these little modules. Essentially, you want to hook into your training pipeline over, you know, over the training time of the model. So yeah, and we focus a lot on PyTorch. We have some integrations with Keras and TensorFlow as well. But yeah, it's totally meant in that, you know, you can make a SparseML manager essentially and just hook this in as a callback function into your training framework of choice. And it can start, you know, overriding things like your learning rate, all sorts of things like this. Other integrations as well as uploading your experiments automatically to weights and biases and common tensorboard common ways to for MLOps to view the status of your experiments. And, you know, early stop them, continue them back up from checkpoints, things like this, you know, made for the ML practitioners, but also all wrapped up in command line scripts so that, you know, data scientists and data hackers can also get started on just trying a new data set, trying a new experiment they have in mind.",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Super interesting. And I, yeah, the whole, the way that this enables sparse models and getting away from needing these crazy expensive dense models and towards this more exploiting the inherent sparsity of neural networks to make this more economical run on CPUs, et cetera, but continuing in sort of the practical stack. So now we have our sparse model and we want to evaluate it and say how much faster it is. The things like say, moving the data to the GPU, all these things, what goes into bench marking these inference accelerations and saying this is faster than PyTorch, you know? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah. Yeah. Great question. It's and it's really difficult to, it's unfortunately pretty difficult to benchmark across all these different types of hardware and frameworks. They all want to do it in different ways. But we try to abstract this away as much as possible, especially on, since most of our models are based in PyTorch backends, we put a lot of this work into sparse ML, you know, in addition to the validation process, also measuring how fast are we validating the results? You know, what's our iterations per second, things like this. And so this allows us to measure at least, you know, some CPU and GPU results on PyTorch and measure that. But, you know, most of our benchmarks are comparing CPU, how much faster can we make CPUs? And so from that perspective, we like to run and compare against other engines that were built for inference, such as ONNX Runtime or OpenVINO. That and that's part of our process of actually getting to inference time. So once you've trained a model or taken a model from sparse Zoo or sparse ML in your framework of choice, we usually export that model into an open model format called ONNX. And this is just a graph representation where each node in the graph is a different operation, such as convolution, matrix multiplication, relu, pooling, things like this. So maybe just your one multi-head attention block of transformers is broken into 20 different nodes that are all connected. But the key thing, this is an open format, you know, spearheaded by Microsoft, Facebook, Google. So all the frameworks can export into this and can even take ONNX in as an input back in. And so that's the main thing that the deep sparse engine takes. And usually once we get this ONNX, then we can bring this in to all of the other inference engines as well and start to measure their performance. And so, yeah, getting into the details of performance and what are the different benchmarks that can really cover accurately the realistic scenarios that you're going to use once you deploy models, there's a couple of major ones. So the first sort of obvious one is when you have just a single image or a single query coming into the model, you want to run this as fast as possible and you want to get that result back to feed into your pipeline. And we call this a synchronous or latency based scenario. So we're talking about batch size one and a single image, a single sentence or set of tokens representing one request. And our goal there is to see how fast can the whole system or section of the system return this result back to you. So this is really common and a part where actually GPUs or hardware accelerators in general struggle a lot as they usually can't access the memory on the CPU, which is where this result usually starts from in terms of, you know, you are getting an HTTP request. Someone's giving you an image and asking for a result. That networking request lands on the CPU's memory. And then now if you want to run it on a GPU, you need to pass that image over PCIe Express onto the GPU, copying onto its memory. It then executes on that. Then you need to copy the result back over PCIe Express back onto the CPU memory. It can put together the network response and then send that back out. So when you're really talking about a handful of milliseconds per request and you don't want the ML model to be the bottleneck in your application pipeline, this is where latency is a really important scenario to look deeply into. So that's a basic one we do. There's also throughput or batch scenarios where, say, you're working on an offline task. Say you have, you know, a million things you want to embed before the end of the night. The question is just how quick or how cost effective can you run through this? And so here you find a good batch size that can fit within your memory available or utilizes the system well. And then you distribute all these batches across your available hardware as fast as possible. And so this allows you to amortize some of the communication costs. You still have a lot of data to move around, but it should be a place where you're really bound by compute as long as everything fits into memory. And so this is pretty good for offline tasks. And then another example that we hear a lot about from our partners and what they're interested in is sort of a model server or a multi-stream asynchronous case. And so think of it just like a web server. And we make it very easy to start up model servers. So just think of we're presenting a model to the world as a RESTful endpoint. We commonly use fast APIs. This is just a super easy to use thing that you can just ping its dock endpoint, get information about what format your image or tensor needs to be in, send that over. And you also have a schema to tell you how to use the output. And so in this case, you're serving multiple users at a time like you would a web server. And each request that is coming into your server, you need to dispatch this efficiently across your hardware. You don't want each of these responses running into each other and causing each other to slow down. And the goal is to serve as many users as possible, as many requests as possible while hitting the latency requirement that you have for your application and your users. So this is something we spend a lot of time on, is this is where a lot of application deployments end up in terms of either it's sort of latency or offline in terms of making very small vCPU or cloud instances. So this works well if you're going in, say, a Kubernetes cluster and you know, maybe like each node is working on a single inference, but often it can be a lot more cost efficient to just have a single larger node that this model server is hosting up a model or even multiple models on one node. And just depending on the requests coming in, dispatching those models and there's requests across the single system, which can really help drive down costs and also make your application complexity or your really, really your ML infrastructure much simpler. So hope that was helpful. Those are sort of the major three scenarios. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's incredible to the node server and how you can have each node going in. So I have my question to kind of build my understanding of this, and I'm sorry to listeners if this is a dumb question, but is it like you would want to do one by one inferences on the CPU compared to where the GPU, I think what you you batch it up into, say, like 64 predictions at a time because it's this big GPU. So you like, you know, take a lot of data at once. Is it with the CPU thinking, say, you have like 16 cores? Is it like inference, inference, inference, like a multi-threaded programming way of thinking about inference? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, definitely in the multi-stream case, it can be really it can be usually pretty efficient to have these like multiple streams where each one of these like batch size one inferences are pinned to individual cores to really help. So it's kind of like an embarrassingly parallel problem. Each core is working on different problems. But, you know, in terms of getting the best possible performance, this is where it's really in your best interest to use an inference engine that's designed for CPUs like DeepSparse, where we're aware of especially the unique topology, that hardware topology that CPUs have in terms of having multiple levels of cache, layer one, layer two, layer three cache, sometimes even a layer four cache and your main memory, of course. And particularly with these sparse quantized models, you can, for instance, on these large large models we produce, you know, uncompressed as a dense FP32 model, they're commonly at one or 1.2 gigabytes on disk and even more in memory. And once we've sparsified these large models, we've been able to lower them by at least an order of magnitude, getting them down under 100 megabytes, for instance, once sparse quantized. And so this now is really interesting as like, oh, maybe your L3 cache on the CPU is 70 megabytes. So maybe we can fit, you know, half of the network just in the L3 cache of this section of cores. And so let's just have this section of cores with this L3 cache just running this model. And so all inferences are going to go through this section of cores. Now, we don't require you to have to have this sophisticated understanding of the hardware you're running on. But these are some of the optimizations we think about when you're building your application with our engine and especially our model server that can intelligently make these decisions based on the hardware available and the models you provided as well, compiling the model down to fit the hardware available. So it's a really interesting optimization problem, and we have a lot of systems and math engineers that work here who find that problem interesting as well. But yeah, generally, you know, CPUs can see a benefit from throughput as well. It's not that it's just completely lost on them. You get more throughput and generally the scaling is better as you become more compute bound, obviously, because we're removing, you know, 90 percent of the compute from the model when we prune it to 90 percent. So it really is about efficiently using all subsystems of the hardware from the compute you have available to the memory, to the networking, and even the larger ML pipeline around the model. You know, we offer this, you know, similar to transformers, this idea of pipelines that will wrap the model around pre-processing and post-processing that's needed both for ease of use, but also with good defaults for speed. So you can just host up a whole pipeline instead of hosting a model, and you can just feed that straight text instead of needing which tokenizer do I need in order to tokenize the words into the embeddings and then provide these tokens to the model so that you can just send your text and get your sentiment out of the model directly without needing to know all the nitty gritty of ML pipelines and how to do it in the right way, efficient for the hardware. So it is really about bottlenecks at every single layer, both inside and outside of the model. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, those abstractions of pipelines are so useful, like from sentence transformer import sentence transformer, then model.encode, and then just the string, and then you've got the vector and that abstraction having that similar thing. I really want to ask about from the beginning, the CPU cache hierarchy and sort of generally the CPU, what it offers. But quickly, so to access is running it from pip install deep sparse from deep sparse import pipeline is, and then is that the best way? Or is there say like a Docker container that would put this inference engine? I don't know these things too well, but like, what's the difference between running it in Python or say, is it like some container? I think the question is. ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, yeah, yeah, no, yeah, totally understood. Yeah, I mean, I think definitely for developers that like to get their hands nitty gritty, understand the UX of the code itself and understand the code behind our benchmarks as well. You know, it's in your best interest. Yeah, pip install it, run it locally, use the pipelines, use the models without the pipelines and read all the code that's on the DeepSparse GitHub. Now, yeah, for people that are more thinking about how can this scale in my infrastructure, particularly things like Kubernetes? Yeah, we offer Docker containers, you know, most commonly. Yeah, we have this deep sparse server that the Docker containers usually built around. And then it's just the case of what config file do you give that Docker container? And the config file just defines, yeah, what model do we want to use? What's the pipeline that it's using? You know what IP address and port or what port should I host it on? Simple things like this. How many streams should I work on? How many cores should I use? This is all sort of in a text file that you can provide. But yeah, we actually have some examples up to date on some of the things that we're SageMaker and we're working on ones for AWS Lambda and Cloud Run to show you examples. Or even on just local Kubernetes examples. I think we're doing the minikube as well to scale across EKS or GKS. So that, yeah, you can just treat your Docker container as another web application that you have this RESTful API into. And so it's just the case of how many pods do I put this Docker container on, put a load balancer over it, and build your application without worrying about scale. It's just managed for you. And I think that's really interesting, just in the case that we are running on CPUs, you know, we are running on CPUs, we can run on vCPUs at the smallest level, because we're just looking at what instructions are available. You don't need to set up any fancy drivers like CUDA or special environments. It's really just running on Linux, looking at the CPU information, and able to scale down all the way down to a task or a pod on a Kubernetes cluster, all the way up to bare metal multi-socket servers that are hosting 20 models at a time.",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super exciting. I think right now, the way I've plugged in Neural Magic to Weaviate is through our Text2Vec transformers container. You just replace the Docker image with one that has the Neural Magic pipeline. Yeah, it's exciting because I think there's still even more optimizations we can do to get this even faster. And so, kind of the next topic, I think we've kind of really covered the how to use it, the practical application. And so, I was listening to Nir Shaveed on Yannic Kilcher's podcast, and it seems like a whole reinvention of the model architecture. Things like, say, you can access a terabyte of memory and then the human brain is sparse. Can you tell me about this kind of vision of sparsity? Is it more than just layer by layer? One quote from the podcast that stood out to me is something around thinking, generally, just thinking of layer one, layer two, layer three, that there's some other way. Maybe it's a path that doesn't wait for this one to finish, other ways of traversing this graph. Do you think about this kind of thing? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah. Yeah. I mean, we think about this thing internally, obviously, from our background, from MIT, you know, co-founders, Nir Shaveed, professor at MIT, really accredited, wonderful individual. And his postdoc, Alex Matiev, that they co-founded this company together. You know, they were working in a connectomics lab together and working with brain researchers as the computer science people to help them. And yeah, they sort of, from the problem they were working on, they were trying to image and pattern out brains. And these are obviously sparse architectures. So they came up with sparse algorithms to map onto the brain. And so, yeah, totally, you know, the road we're going down in terms of, or the road we are down in terms of training models and deploying them very much works on the idea of having dense layers, feeding into dense layers, larger and larger models that are at fully connected layers. And like I said, this works well because we found a golden bullet that works for now, stochastic gradient descent. You just throw more data at it, you regularize it enough, you show it enough data at a time, and it can just continue and continue to keep learning. But that doesn't mean that it is the best way. And I think like in the future, you know, this scaling, especially as Moore's law, you know, struggles to continue forward, we need to find more efficient ways of training and running these models. And this is what we hear exactly from our customer engagements. They, a lot of times they have models running in production, or they're gearing up, and then they're getting some load, and they're realizing, yeah, this doesn't make sense. But the value is there for their, for the people using it. It's just too expensive operationally to run all these models. And that's because most of the time, they're just training the model on the GPU. And then they're like, well, it's accurate. How do we run it? And then they go run it on the same GPU that they that they trained on. And, you know, haven't taken the thought of from training to inference, let's have a step of optimization in there. And that's really what we're what we're working on now, introducing that optimization step to more and more people. Now, of course, like, you probably shouldn't be training dense models in the first place, but it'll take a lot of research and progress in science to get there. But I'm sure we can get there. You know, purely from the idea. Yeah, the brain is a is the equivalent to a cell phone of compute with a petaflop of memory. It's not the other way around, where, you know, we don't have exaflops of compute in our brains. With it with it. Yeah, certainly we don't have teraflops of compute with just, you know, 16 gigabytes of memory attached. We we have a lot more memory and access it really efficiently so that we don't need to eat all the time. So, so, yeah, that that's that's definitely how we think about it. But also, you know, just in case of talking about this asynchronous execution that that that we built in our engine as deep tensor columns, it's not like we're the only ones working on this. You know, a great example actually is the the really famous Google paper from I think about a year ago, Pathways, or even their sparse transformers paper before that, where they trained, you know, I think a trillion parameter model even larger than GPT-3. But the key thing was, based on the input coming in, you know, you would have, say, multiple multi head attention or fully connected layers, multiple, essentially experts you could choose from. And based on the input coming in, it would route it through a different pipeline. So you have, you know, 100 times more parameters than you would actually use for each individual inference, but you're sampling them over time for different inferences. So this is obviously at a very structural, large scale, but it, you know, it makes sense. This is what the brain does. We're not accessing, we're not running all of the weights and parameters and neurons and synapses in our brain every second. It's localized to certain pieces that we need to run every single millisecond or pieces that we need to run, we need to remember a nice memory or something like that. So, so, yeah, it's, it's really interesting. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's super interesting. I guess I was thinking of that. Yeah. Those two angles where one, it's like you're realizing the lottery ticket hypothesis where, you know, you theoretically can train the sparse sub network from scratch and having this quantization aware training where you're pruning during training is like getting closer to that kind of idea where it's like, be mindful of the sparsity during training. And then like this pathways thing, which is like a total rethink of how deep neural networks are going to, I'm sorry, are going to process the data. How are they going to process data to make their predictions? So one other thing that I think is really interesting is edge applications, like predictions at the edge, putting it in phones or embedded sensors, things like this. Can you tell me about like your motivation around edge applications, say like maybe robotics or like apps that I guess don't have to network? Like what's the thinking around edge case applications of deep learning? ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, so, you know, the most obvious thing is power. You want to keep power on these things as little as possible. So it's obvious we want to make the machines as efficient as possible while also maybe paying the least amount to get them on there. But in addition to that, there's also the cost of the networking, of the bandwidth you pay. And, you know, we've heard this a lot from retail and manufacturing in that, you know, they don't want to send, you know, 200 cameras from every store out to AWS. They don't want to pay the ingress costs every day. And it would be much better if they could at least, you know, just send out the high level analytics every day instead of needing to process all of the data. And you need to process all of that in a data center hundreds of miles away. And also use that bandwidth of the store that their, you know, employees and customers would like to have good Wi-Fi at. So, yeah, it really is like a problem with more problems at every level. But, yeah, I mean, it's really from the case like, you know, you think about a camera or say like a, you know, a camera running at your front door, at your doorbell for, you know, to look at the delivery driver or to essentially replace your doorbell. Many people have these and they wouldn't really like every single thing being shipped out to the people that make them. And so each one of those cameras, I bet you 100% has a CPU attached to them, maybe with some, you know, video decoding stuff next to it. But the CPU has to be communicating that thing. And so if we can make the models and things that are useful for this data small enough and compressed enough, but still accurate enough to run on these devices at the edge and then the camera doesn't communicate with anything, even on your local Wi-Fi using that bandwidth, even outside of external bandwidth, unless there's a warning or something serious that should be looked at or every once in a while. That's just a better experience for everyone involved. And plus, the box that you have plugged into your wallet, integrated into your mainframe, you don't need to have that box. You don't need to have the compute there, have the power there, have it being noisy and things there. So definitely, we hear the asks every day. And that's why obviously a lot of people are interested in what we have to do. So, yeah, it's really just about figuring out the whole pipeline beyond just optimizing the model to make it easy to build these applications, fit them into smaller devices. That's part of the reason why the DeepSparse engine we focused on x86 CPUs to start with, so made by Intel and AMD, because those especially were the most common four years ago, which the company was founded at. But now ARM is running everywhere, both in the cloud, on the edge, on this MacBook I'm talking to you right now from. So we're working on ARM support right now and hoping to have an alpha by the end of this year and hopefully a demo at NeurIPS in December where we'll be presenting. So, yeah, looking forward to getting sparse networks onto more types of hardware. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's a super interesting example of the doorbell. Like, I think it detects the package deliveries and you might not necessarily want to have a doorbell of your front door, like streaming to the cloud service all the time. And all this motivation around, say, like federated learning and that kind of thing. And I think it's interesting, too, with WeeVid, if you want to have a vector-based approach to this where, say, it's a security camera and it's kind of like doing outlier detection. So it takes an image of the front of your yard, vectorizes it on the CPU with something like Neural Magic, and then builds up this vector index to do the outlier detection. I think it's very interesting. And also, say, this vision of the Tesla bot, where you're going to have a household robot that you get. I think a lot, other than Tesla bot, I think a lot of startups believe in this kind of thinking that we're going to have a 5'8 robot that you say, make me some eggs, and then it will go make eggs. And similarly, in that case, I can imagine that you'd want it to not do this kind of bandwidth sending and have it be like... ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, you don't want it taking up a person's worth of someone streaming Netflix on your Wi-Fi constantly. Right, right. Pumping it all up, yeah. Cool, cool. For every device, certainly. So yeah, we've heard that as well. ",
        "podNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, awesome. Well, thank you so much, Michael. I think the technology of Neural Magic, the way it fits with Weaviate is so exciting. And sparsity, the vision to this, the sparse zoo, sparse ML, all of it is just so captivating. And thank you so much for doing the Weaviate podcast. ",
        "podNumber": 27
    },
    {
        "speaker": "Michael Goin",
        "content": "Yeah, thanks so much for having me. And it's been great talking to you and learning from you over the time. Really excited to learn more about the use cases for Vector Search and contribute some cool demos.",
        "podNumber": 27
    }
]