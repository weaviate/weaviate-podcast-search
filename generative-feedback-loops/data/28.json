[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone. Thank you so much for checking out another episode of the Weaviate podcast. I'm super excited to welcome Maarten Grootendorst to the podcast. Maarten has created this incredible library, educational content around BERT topics. So this idea of topic modeling, using an analysis in vector space to extract high level topics to understand what is in these vector clusters and what makes them distinct from each other. So firstly, Maarten, thank you so much for joining the podcast. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Thank you. Thank you for having me here. I'm really looking forward to this. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So can we dive into the, I think it's about four steps to producing the BERT topic analysis. Could we kind of walk through the algorithm? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, yeah, of course. So generally four steps, depending on your view, go a little bit more and extend it to 10 or so, but let's keep it straightforward. So what often is happening is you get a bunch of documents, right? There may be a few thousand, hundreds of thousands, millions of them, especially in the context of Weaviate, for example, and it can be many more. We embed those. We need to have some sort of numerical representation. And I'll go into that a little bit later, but it really doesn't matter necessarily which type of embedding technique you use. That kind of depends on the use case. So when we have that numerical representation, it's typically in high dimensional space, right? And that's generally fine because it gives a lot of information about the document, but doing something in high dimensional space can be tricky. So before we go to the clustering step, which we'll talk about a little later, we first reduce the dimensionality of these n-dimensional vectors. That typically works out well. It can be a time-consuming step, so that's still something we need to take into account, but it makes sure that some of the global information is being kept in low dimensional space where we don't have the curse of dimensionality, where we can use even Euclidean distance measures, which obviously we cannot use in high dimensional space. So we can do something with that. After that, we do the clustering. The dimensionality reduction, of course, is done with UMAP, which is now currently mostly state of the art there. And the clustering is being done with the HDBSCAN. It has the nice property of generating outliers, but that depends on the use case, because not necessarily everyone agrees that it's a nice feature to have, right? So then you have clustering. So we have these three initial steps. We have clustered our documents into topics with semantically similar documents, and that's nice and all, but then we need to do some extraction of these topics to get some representation out of that. And there are quite a number of techniques that you can use for that, but from the context of BERTopic, at the very least, we wanted to have something that doesn't necessarily make a lot of assumption on what happens before that. Now, if you take HDBSCAN, for example, it's in the name, it's a density based algorithm. So these clusters, they can be circles, they can be lines, they can be squares, or anything you can imagine. And because they are so different in structure, we wanted to have a topic extraction method that doesn't make any assumptions on how those clusters might look like or might not look like, with the idea of later introducing modularity. But again, that's something for later. So then we resort to something that has been done for quite some time, and it's just a generic bag of words approach, with the small twist of not considering documents, but considering topics instead, clusters, so to say. And the same thing applies with the CTF-IDF that's done to score these words. Again, we're not looking at individual documents, because we don't necessarily care about those, but we're more interested in their abstract global representation. So also there, we focus on clusters instead of documents. And those four steps are essentially how BERTopic works. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, and I'm sorry to distill it into just four. I think this is kind of how we've been starting to wrap the foundation of Weaviate, where we have embed the data into vectors using, say, a sentence transformer, then compressed dimensionality with UMAP or TSNE, HDBSCAN clustering, and then concatenate the clusters into cluster documents, and then extract the, and then use, say, TF-IDF to get the keywords. That just kind of been my foundation guiding thing. I attended your talk at Cohere, where you presented this really interesting diagram of kind of the building blocks of this, and you touched on this density-based clustering compared to k-means. You mind quickly just touching on that a little more, say, the difference between, you know, after you do, you know, vectors, then UMAP, and then, say, HDBSCAN versus k-means, how they have different structures. You mentioned, say, you know, circles in your data. Could you explain that a little more, the difference between those two clustering algorithms? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, sure. So, an assumption of HDBSCAN, or not necessarily an assumption, it takes into account that certain clusters can have different forms and shapes and sizes, and, you know, it works rather well when you have a piece of data that starts here and goes all the way up to a certain other point, and it follows that density along the line, and using that, it can essentially make sure, okay, we still have that cluster, because it's difficult to capture those clusters that aren't necessarily in one tight, small, dense structure. K-means does something entirely different, and basically draws lines over all of these clusters, and assumes that everything within, you know, let's say, square or certain form, that the very centroid of that is the most representative of that cluster, and HDBSCAN doesn't necessarily have that assumption. So, when you, for example, extract the centroid from a cluster found by HDBSCAN, it doesn't necessarily have to be the most representative one, because, you know, if you have a circle, so all points around the circle, and there's one single point in the middle, HDBSCAN most likely will find all of those, maybe not the middle one, but if you take the centroid of that, it's not really representative of everything that happens on the outside, right? So, then a centroid-based method for topic extraction doesn't work. It might work if you use k-means, because that assumption is more built into the algorithm, but we don't care about, you know, all of the assumptions that all of these clustering algorithms have. That's way too much to take into account. So, we separate them. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's a super interesting insight. The, you know, say the circle shape with the dot in the middle, and this sort of describes this idea of representative documents, and what might be the problem with that, where, yeah, I also love the example of the line, and if you have k-means, it's going to try to, like, cut that up into four parts of the line with the centroids. Yeah, wow. So, the density thing is incredibly interesting. Can we maybe touch on the final product, the keyword list, and then come back to the hierarchical, the topic trees? But, so, as we, you know, concatenate each cluster into a long cluster document, and then we use TF-IDF to find the unique keywords, I've seen ideas like setting the topic labels where you go through the list, then you find a label through it. Can you talk about, sort of, what you're doing with that? What you get by doing that when you go through a list of keywords, like, say, medicine, healthcare, you say, okay, let me call this the medicine cluster, sports, basketball, baseball, the sports cluster. Can you talk about that kind of idea of setting the topic label and how that helps search through the keyword list? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, of course. So, the thing with topic modeling is that's a rather subjective technique, right? It's, first of all, it's unsupervised, so it's really hard to have the ground truth and objective evaluation metrics. And the second thing is, what you consider to be a topic, I don't necessarily agree with. Now, the entire idea of having all of these words in a topic description helps you understand, oh, this is what the topic really is about. It already provides you with some context about, you know, what you can find in that topic. Because if you would give it a single topic label, like sports, I still don't know what we're exactly talking about in that topic. So, there's a lot of human evaluation. Human in the loop is still very necessary in these kind of techniques, because I can give you some sort of description, but you still have to interpret it yourself and understand what the topic is about. And I think that's very important with these kind of techniques, because people tend to look for evaluation metrics and then do some grid search on that and find the so-called best methods. That really doesn't work in topic modeling. You can try, if you have a very specific use case with a very well-defined evaluation metrics, but even then, it's so subjective that the person really is necessary to do something with these words and these labels. And, you know, ideally, create them themselves. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, is that human in the loop? I've seen ideas use like a GPT-3 to try to guess. Do you like that idea? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, yeah. I've heard that one before. Of course. I mean, it makes sense. It's text generation. So, if you provide it with either some of these words or perhaps some of the representative documents, whatever those might be, then you can get some sort of description from there. The difficulty here in lies, you know, GPT isn't always necessarily that accurate in creating topic representations. It's not specifically trained for that. From an API and open source development perspective, you would need a very large GPT-3 model in various languages on top of that, which, you know, makes it not really that easy to use out of the box. So, hopefully, I can provide a basic pipeline where you can tweak and change whatever it is you want. And if you feel like you need GPT-3 to do some fine tuning, go ahead. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's really, I think maybe the idea is like with the few-shot learning, we could give it a few examples and maybe that would work. But really, I find this idea of labeling the clusters so interesting with, say, the way that we search through Wikipedia articles, we vectorize each of the paragraphs and then we still have the title of the article as like a symbolic filter. So, you could ask, say, what year did Barack Obama become president? And you can filter that by having article Barack Obama as the title. And then that helps reduce the search space a lot. So, do you see maybe labeling these clusters with the summarization of the keyword list, I guess, as being a way to similarly kind of say, just search through this cluster and unsupervised? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, you could definitely. The one thing to note here is that you need to have some idea beforehand what you're looking for with topic modeling that isn't necessarily always the case. You know, you have tickets for some software, hundreds of thousands of tickets, and you just want to see what's in there without any guidance. Okay, then it's completely unsupervised. With few shot learning, of course, you can say, okay, I have some idea. I don't necessarily know the entire thing, because otherwise, it's a classification task. But with few shot learning, you can then say, okay, I have some idea. Let's see if we can do some semi supervised few shot learning on top of that. But then again, you have to define those labels very well beforehand. Because if you don't, again, then it becomes difficult. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "I've always been I haven't quite wrapped my head around how semi supervised learning works in this sense. Do you mind explaining that a little more? And I think that'd be a great topic to go into. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "So there are roughly two ways to do semi supervised topic modeling. And the thing is, with our topic, I'm trying to keep it as simple as possible without and without making way too complex for the user. The first one is a dependency on UMAP. UMAP in itself does some semi supervised dimensionality reduction. And by leveraging that, you know, HDBSCAN can find those, those things rather well. The other thing you can do is essentially saying, okay, I have a topic that I know is in there. It's about sports or whatever. And then you create some keywords or sentence yourself. And you embed that one. And then it's simply a cosine similarity with all of these documents. And searching for the ones that are, you know, that are above a certain threshold, and simply say, Okay, I know these labels, those are these and these topics. And then you average them out by nudging it a little bit more to the topic that you have defined yourself. And by averaging out those documents towards that specific topic, most likely something like HDBSCAN will capture it. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "So sorry if I'm misunderstood. So you try to embed a single word to say, you know, I, I'm searching through my tweets, and I want to have like deep learning, let's say a single phrase. So I embed that into the same vector spaces, the sentence transformer is going to vectorize the tweets. And then I use the nearest neighbors of that to say, this is a topic and this is a topic and then HDBSCAN, like it can take that signal to kind of structure the whole space around it sort of. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "So what you're essentially doing is you embed them in the same space. And when you find documents, using some sort of nearest neighbor approach that are very similar to deep learning, you average the embeddings between that document and the deep learning embedding. And if we average them and use that instead of the original, you know, document embedding, then you nudge it slightly towards the deep learning input. And if you do that with a certain amount of documents, HDBSCAN, of course, will pick that up because you kind of have nudged it towards the same embedding. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think that's incredibly interesting. And it kind of is inspiring my interest further in this idea of say, kind of recursive partitioning, like where first we have all of our data cluster, cluster, and then we want to go with it. So say it's all of Wikipedia and we have like sports, history, medicine, and then we want to take the medicine cluster and then do it again. And now it's like different kinds of medicine. How do you think about that kind of recursive BERTopic, like keep going into the clusters when it's really big data? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah. So that's something that you can actually naturally do with, of course, HDBSCAN, but because with BERTopic, we're not trying to be too dependent on a specific clustering model. It's a post hoc analysis. So what you can do is you can say, I've found 100,000 documents. I want eventually some fine grained hierarchy in that. So what we do is we say with K-Means, for example, we're going to generate a thousand or 5,000 small topics. And after we have created those 5,000 topics, we simply do some sort of hierarchical analysis on top of that, the exact one that you want to do, that's up to you, of course, and build those 5,000 topics up until you get less and less and less and less. And then you can slice them at different levels. And from there, extract the hierarchy that's in there and zoom in whenever necessary. So instead of going from one topic to very small ones, we're going from very small ones to a single topic. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "So when you're concatenating each cluster to make the cluster document, that is how you run the TF-IDF algorithm to get the keywords. So what the hierarchy is each small slice, the inverse document frequency is compared against all the data we started out with, or sort of like it's neighbors in the tree on the same level, so to say. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "So essentially, what we're doing is we have with the CTF-IDF from those 5,000 topics, we just have a topic term matrix with some scores. And within that topic term matrix, we're essentially comparing which ones are most similar to one another. And we iteratively merge them. And at each step of merging, we get another layer and get another layer, and get another layer. And because we've already done the clustering, whenever you combine two topics together, the only thing you have to do is recalculate the CTF-IDF representation. You don't have to do anything with the clustering whatsoever. Because the CTF-IDF is based on the documents, not necessarily on the clusters. That's just split. And if we know which splits to combine, we just recalculate the CTF-IDF representation, and you have your new higher abstract cluster. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Could we step one... So the way the topics are merged, do you mind... Sorry, I'm not... The way that it's merged, just one more time. How do you... Yeah, yeah, yeah. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, of course. So what we're essentially doing is you have for each topic, you have a vector, right? In this case, it's a sparse matrix for the entire topic term matrix. But for each topic, you have some values. And we have 5,000 topics. Some are bound to be similar to one another. Especially if you use just cosine similarity between two topics, you get certain topics that are very similar. So if you do simple linkage, some hierarchical linkage, what you basically can say is, okay, I'm going to search for the two topics that are most similar to each other. And these two topics, I'm going to find the documents for both of them. And I'm going to combine them together into one new long document, and then recalculate the CTF-IDF representation. And if we do that plenty of times, then we suddenly get a little bit less topics and more abstract and more general representation. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Super interesting. And yeah, that way of the unsupervised kind of subpopulation discovery is so interesting. Can we talk also about generalizing this to image embeddings? Or say we have a vector that describes the image, but then we also maybe have some text for each of the image to like captions, or maybe it's linked to an article. Can you describe how we could use this algorithm to explore other kinds of vector spaces? Like, yeah, like image vectors, maybe audio vectors would have an associated text. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah. So, what we're topics essentially doing is saying, okay, we're gonna make the clustering step in some way independent from the topic extraction step. And what that means is that we normally say, okay, I'm gonna convert my documents into some sort of numerical representation through language models. But what if we have other metadata that we can cluster instead? Well, we can simply throw those in instead of, you know, the document embeddings and cluster whatever else is interesting. And whether that's metadata or images, for example, doesn't really matter. Again, we don't care how we do the embedding extraction step. We just want data to cluster. And whether that's document embeddings or images, that doesn't matter. And then when we have clusters, we can just say, okay, we're gonna and then when we have clustered all of these data, whether that's images or texts or streaming data, what have you, then we take the documents for each of those clusters and do the topic extraction step. So, what you essentially can do is you can say beforehand that you want, you know, images to be clustered. And those can be from, you know, booking.com or Amazon or whatever, you know, what have you. And you cluster those instead of the document descriptions. And the great thing about that is that, you know, an image typically represents one sort of concept, one sort of topic. And with very long documents, that's not necessarily the case. And then we get a lot of different topics and we have to split them into sentences. And if you do the images, you know, we get one thing and then you can use the documents and see which parts of that document mostly represents that image. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think that idea is super powerful, the decoupling of clustering and topic extraction slash kind of metadata analysis. Like if we have patients, we could put, say, their, you know, like images, like medical images would be a good one to vectorize or I don't know, maybe we have like clinical notes that we can vectorize. And then we'd also have the metadata like age, weight, height, pre-existing conditions. And so with also this cluster analysis, we can, you know, have these sort of like histograms, all the violin plots, all the kind of things for each of the metadata of each of the clusters as well. Do you think about that kind of thing also, like symbolic data visualization achieved through clusters of metadata on them? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Oh, that's interesting, especially the example you mentioned with clinical data. So if you have a number of patients, you cluster those instead of the text related to those patients, you can say, okay, we have found certain subgroups of patients and within those patients, they typically talk about these and these and these topics. And that might make it, you know, even more interesting to do things like that. It's more advanced techniques, so to say. So I haven't actually seen it being used like that. But yeah, I mean, when you do a fit transform, you provide docs, documents and some sort of data, typically embeddings, but throw in your metadata. I'm very curious to see how something like that would look like. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I sort of brought this up accidentally. But one of the questions I have written down to ask you about, and I think we'll come back to the technical questions after this is so you're also a psychologist. Do you use this in your work? How have you found like, has this kind of been inspired by that? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "To a certain extent. So there's a lot of psychology that indeed tends to get back into the package itself. And it really comes from making sure that it's easy to use, that you communicate things properly, API development, things like that, without necessarily looking at what a state of the art or what is best. It's really practical approach, so to say. And it really stems from my own needs when constantly needing to install different types of topic modeling techniques for different use cases, which can be difficult at some point. Because if you install, if you pip install, A, it has a different feature set than you pip install B. Especially when you compare, for example, hierarchical topic modeling with dynamic topic modeling, they typically end up with two entirely different topic sets, and then I cannot combine them or compare them, or, you know, that makes it difficult. And I then start to focus okay, what do users typically want to be using for for these kind of things? What makes it easy? What makes it simple? What makes it fun, maybe? So at some point, I figured, okay, when we create a topic model, I want to have included dynamic topic modeling, hierarchical topic modeling, semi-supervised topic modeling, online topic modeling, if it's possible. Just make it, if possible, a one-stop shop for topic modeling, so that you can create a baseline with everything you do. And then if you want, there are more specialized topic modeling techniques that you maybe can use for very specific instances, that's fine. But this should provide you with most things needed inside of a topic model. So yeah, I've mostly focused on things from a user perspective, what do users need? What do they want? If I hear about a feature in the issues page, typically, I write it somewhere down and see if I can create it. So yeah, it typically, it often tends to get back to the psychological background into development and communications and documentations and things like that. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "So is it like with the perspective of looking through psychological literature and trying to break it up to keep up with it? Because I know like with trying to keep up with machine learning literature, it's pretty exhausting. So I would love something like this. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, no, that would be perfect. But unfortunately, I haven't found a way to do exactly that. Because I've I agree fully with you that it's exhausting to keep up with literature. I mean, even in a very specific subfield, like topic modeling, it's it's, it's so difficult to keep up. And we're, of course, in a very large natural language processing domain, which gets even larger when you consider machine learning and the transition from transformers into computer vision, all these other types of fields. Now, I mostly focus and that's my, my pragmatic approach on things that I can use right now. So state of the art is awesome, state of the art is great. And at some point, you know, it's necessary, because that's those are the things that we're going to end up using. But I really like to focus on the things that I can use in practice, the things that are performant that simply work. But to get back to your original question, I've mostly used topic modeling, not in psychological domain, but now mostly in clinical research. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Hmm. Yeah, super interesting. And I want to come back to you mentioned dynamic and online topic modeling, which are two topics I don't yet understand that I'd like to ask you about, but kind of on this topic of applications. Also, I, I think this kind of subpopulation identification could be super useful for the general thing of generalization testing with deep learning where we want to know like, what is in distribution, what is out of distribution and say, we, you know, put our data through HDBSCAN, and then we only train on data that isn't defined as an outlier, and then tested on these outlier points are different kinds of ideas like that, where you things like say, you train and one hospitals medical images, then you test on a new hospital and the machine learning model fails. You think about that kind of generalization testing as an application for this? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "A bit. Generally, topic modeling is approached as in a very explorative way of getting to understand your documents. It really helps that you understand, okay, this is my data, these are potential clusters, we can, of course, use HDBSCAN to get more to the very core of these topics, but it often throws out a lot of documents that could be useful for yeah, could be or most likely are in in the cluster. So it's rather strict from from the default settings, which is fine, because it really depends on your approach. But there are more and more packages that I've seen developed that focus on on the explorative approach of essentially BERTopic. So creating a 2d representation, and do the do the labeling yourself and see if some things make sense. So that's, that's what we're talking, for example, also gives back to you there visualizations where you can have a 2d representation of the documents and the related topics. And there are some other packages that go beyond even that, where they say, okay, now that we have those potential topics, let's see if they make sense. Let's label them ourselves. So that that human labeling becomes more and more important. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's extremely interesting. And I think also, I hope this isn't too tangential, but like this exploratory data analysis, I think it's really useful for like data cleaning, deduplication. When I first saw BERTopic, I put the CORD-19 data set into it. And then I saw I was like, oh, I didn't even realize there were languages other than English in this data set until I did this. I didn't really like look through it. Because it like when you web scrape data, or you're talking about like, going to PubMed and getting 300,000 papers for your data set, it's like, you don't really know what's in it. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah exactly. And I think that's, that's also something that has gained popularity over the last few years, right? The quality of the labels that you have, and what what is exactly in all of those standard data sets that everybody has been using, and nobody knows, really, if there's something in it that shouldn't be in there. I think even the 20 news groups data set has some has some Dutch articles or some Dutch language somewhere in there that that I found recently, you know, these kinds of things are important, because we're using them constantly in research. And we don't always know exactly what's in there. So to give to use such a method, whether it's BERTopic or something else to have an idea, okay, but we're missing this information, or these labels, potentially, shouldn't be in this cluster, but or in this class, but should be in a separate cluster, or are they connected to one another? That kind of information is way more important than I think people realize. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, and with like this self supervised language modeling, where yeah, you have unlabeled data, but the quality of it. Yeah, it's not like the like the quality of your labels in the same sense of like an NER model where you're like labeling each thing, but you still want to have good data for the language model. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, yeah, exactly. And spending time on labeling a little bit more than you do on fine tuning your model, I think that you will get the main advantage of going through your data understanding what it means. Those labels are exceedingly important, because they're often, you know, they're subjective still in a way, because you look through a document and you give the label. Now, I mean, that's that's my interpretation, you know, and another labeler might disagree. So spending time on these kinds of things is just important. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's it. Do you see that a lot of disagreement within labeling? Like you have a list of keywords, right? And then two, two psychologists would disagree with what the list of keywords that what it would conclude to. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, so it's not necessarily that I see psychologists a lot with topic modeling. I see that more in policy and politics. But what you definitely see in psychology quite a lot is now if you have observers that observe certain behavior, certain data or certain things that humans do that that need human interpretation, yeah, then we need several people making that judgment to see if there's an overlap. And if two or three don't agree over the same instance, something is going on. At the very least, we can say it's not a very clear label or something that isn't clearly defined or cannot even be clearly labeled. And as such, we can say, okay, it's an outlier, or it's not something that represents the thing that we have in mind. And that's also something that indeed, you see more and more popping up in industry. So so prodigy is doing amazing things with the with the labeling tools. And I think that's where it needs to start looking through your data, seeing what's what's happening there. Ideally, if you have the time have two people look at the same data and see if they come to the same conclusions if they understand the use case well and understand what the label should represent. Well, because, you know, if we talk about certain labels, I might have have a certain concept in mind because of my history and experience with that concept, but your experience with that concept might be different. So, you know, you have your own subjective experience involved in in labeling. And in a way, we might need to throw that out or, you know, average them across labelers, people who look at the data and check it. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think the idea of across human labelers, and then there's also this interesting idea of having multiple model\u2019s embedding spaces. So this topic of like domain adaptation, like, if I embed my CORD-19 data set with all mini-LM-L6 trained on all sorts of data, like the general model compared to the one that's been trained directly on PubMed papers, and I look at the topic spaces of two different models, have you explored this kind of thing? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, so that's also often what you see happening with something like BERTopic is that there are a lot of embedding techniques that you can use. And there's a benefit to that, because, you know, you can use it for your specific use case, and you can check what what fits. But there's also disadvantage which that with that in that, you know, you can end up with quite different topics. And then the question becomes, of course, which one is correct. Now, the definition of correct, as we mentioned, is difficult, because who decides what is a correct topic and who doesn't. But the main thing here, I think is that you consciously and purposefully choose which embedding technique you use. And, you know, some might represent it better than others, but it's you who choose for very specific methods for very specific reasons. And that can be because you have fine tuned the model on your specific data, you have tested it with a pre trained general model that doesn't capture that domain, as well as you thought it would. You can also throw a lot of models, you know, in in in BERTopic or something else, and see what comes out. But then you run a little bit the risk of, you know, choice overload, you have so many choices, you don't know what what to do and what to choose. And then, you know, I give a lot of responsibility to the users, it's up to them to decide, okay, I'm going to use this model for this. And this reasons, I'm, you know, trying to motivate the users to really think about which models are you going to be using? And why are you using that specific model? Do you understand the model? Do you know that when you use, you know, TF IDF as your input documents that you're constrained with your vocabulary, but if you use fast as fast text, you have a little bit more, you know, flexibility with respect to those words. And if you use long former, then you can use a little bit longer documents instead of, you know, the regular transformer models that have small token limits. And if you use certain sentence transformer models are rather fast, and others are very slow. The ones are some are more accurate, and some are more are trained to more data. This is tricky, and it's sucky that there isn't a single solution. But that's just the way it is. And as such, it's up to the user to really dive deep into, okay, this is my data. And for this data, I think this type of embedding more would work best. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's extremely, it's such an interesting interpretability of the models to see where how they're clustering things, what they think are the nearest neighbors to things. And so something that I want to pick your brain about also is how do you evaluate the topic models? There are benchmarks, different things. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, so I've gotten this question quite a lot before. That's all right. Because whenever you, you know, develop a model, people are gonna ask, is it accurate? Is it performant? Is it doing what we want to be doing? And my answers all that always, no, maybe not the most fulfilling answer, but it depends. Because what does it mean to have a good topic model? Does it mean that topic coherence is as best as it could be so that the topics, you know, are easily interpretable by by humans? Does it mean that the right documents are clustered in the right clusters? Does it mean that it's performant? You know, we could maybe create a model that runs for two weeks or create one that runs in a few hours. That's also an evaluation metric, we can optimize for diversity for topic diversity, we can optimize for, it depends also on the underlying algorithm, right? If you use something like k means how many topics are we going to optimize for? And what does it mean to have 100 topics versus 50? Because, you know, for some use cases, you just want 20 topics or so. And with HDBSCAN, you can have to tweak to make sure you get roughly around that 20. And you can fine tune based on that. So it really depends on the use cases. But what I would generally recommend to do is talk with your stakeholders, talk with your audience, talk with the people who are actually going to be, you know, using this model reading about this model. They are the ones who need to do something with that. So for my use cases in clinical researcher, and clinical research, I talk with medical professionals, I show them, you know, something that I thought was logical, a number of topics, a number of granularity, also with respect to those topics that can also be evaluation metrics. And I often then tell me that's wrong. And that's fine, because that helps me reiterate and see, okay, it's not granular enough to have very specific demands. That's fine. So 50 topics, we're going to do 200 and see if that's makes a little bit more sense. So I really want to do it. But there isn't one topic evaluation metric that works best. Because also, what is the definition of best in the context of topic modeling? That really depends. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, so definitely like a human in the loop, strong component to it. And I think maybe like when we would present a human search system, and we say, hey, does this query document pair match? So it's like, well, relative to what else and it's like, maybe you show a query in a document, but then so you want to show like documents 2, 3, 4 as well. And it's like these very long paragraphs of text. It's like, I can't read through this. Whereas if you have the keyword list, so it's like query and then keyword list, comparative keyword list, maybe that's a better way to like, get a human judgment. Something like that. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "No, no, I definitely agree with that. Because the thing is, we're still still human. And we're lazy. We're not going to read through hundreds and hundreds of documents to see if everything matches up. So things you can do, of course, is, you know, you have a topic, you have certain keywords, you read through those keywords, you think, yeah, I'm not still, I'm still not sure what this is about. And then you can show a few, maybe randomly sampled documents, and not too many, but just a few to get an intuitive understanding of, oh, it means these and these kind of models and these these types of topics can then be extracted from those. So it's still, you know, like you mentioned, a way of approaching it, taking a perspective from the user. How can you make sure that they have the easiest time making sure that everything works as intended? That is the approach of BERTopic. Of course, it doesn't always succeed in that. So that's what development is for. But it's also that iteration and that feedback process to make sure that that makes it a little bit easier for the user to understand what is happening. And for that, I provide many, many functions, maybe too many, you know, visualizations, diving deep into certain topics. But it's always nice if you're just presented with, you know, the keywords and just a bunch of documents and say, okay, okay, these make sense, these do not make sense just from a single glance at those topics and those keywords. So that is the intention, but it doesn't always succeed in that. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I mean, it's super interesting. I think there are so many applications for doing this vector analysis, this way of bubbling up the keywords and then having that be kind of like the layer above it that lets you see the clusters. I think that's just brilliant. Could we kind of, from my understanding, could we get into the two, could you explain to me what dynamic topic modeling is? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah, sure. So essentially what you're doing is you have your topic modeling, and let's say you get a topic from that and the topic is about cars. So you have a bunch of tweets, in those tweets, you find the topic about cars, and you think, okay, that's interesting. We have something about cars. Great. The thing with tweets is that there's a timestamp attached to it. And that might be interesting because the way we talk about cars today and a few years ago might be entirely different. The topic is still exactly the same. It's still about cars, but today Tesla is way more popular than it was a few years ago, for example. So we're still talking about the same thing, about the same concept, the same topic, cars, but the way we talk about that is different. And you can do that in a dynamic way. So over time that you see if a certain topic has increased popularity over time, and if it has, is the way we talk about that topic different, but you can also split it in classes, for example. So if we have a certain political topic, China, for example, then Obama would talk quite differently than Trump about that topic. It's still the same topic. They still talk about China in a certain way, but the way they approach it might be entirely different. And the entire idea with dynamic topic modeling and for example, class-based topic modeling is that we say, okay, we have a certain timestamp for our documents. And we take only the documents in that timestamp and then do the CTF-IDF analysis on top of that. We don't have to do clustering anymore. We don't have to do any of those embedding steps. We can simply slice up our data a little bit further. So we still have the same cars topic, but instead of this entire line, we're taking this small step. And from that, we essentially say, okay, we're going to recalculate the CTF-IDF representation and maybe it's different. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "In order to not have to recalculate the clusters, you would need to have say 2014 to 2020 in the initial set, and then you can 2018 to 2020 slice. That's super interesting. One of my things was to ask about how the topics evolve over time and see a dynamic topic being the phrase to describe that. And that is just amazing, especially as we think about continual learning, data-centric AI, managing these data sets, how are they evolving? If we're searching through software support tickets for Weaviate, let's say, and then we introduce a new feature and then we see how the discussion evolves around it over time, helps us to search and understand just our data set of support tickets open. So now can we dive into online topic modeling and what that means? ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Yeah. So that's very similar to what you've mentioned before, right? So if you get a bunch of tickets for your software that you have, you can do dynamic topic modeling on that. And that's when you already have all of that data and you just see if there's a trend. But, you know, there is a tomorrow, there's new data coming in there. And you might want to know a little bit more about these new topics or this new data, whether the clusters change or whether they become more about something entirely different within the same topic, of course. So what online learning is doing is basically scikit-learn's partial fit function. It's when you train your data set on a small part, and then you continuously update it whenever new data comes in to make sure, for example, the clusters get better aligned, whether it discovers new topics, which is also possible. And the basic usage for that is, again, rather straightforward, because we have a pre-trained language model. We don't need to necessarily fine tune that for new data that comes in. It generally captures the representation well, with a small side note, of course, that if, you know, there are models trained before COVID, so whenever COVID comes in, it can have a difficulty capturing that. But setting that aside, we can then use, for example, incremental PCA to continuously update how they should be, you know, reducing dimensionality. We can use mini-batch k-means to continuously update the k-means representation. So CTF-IDF is rather straightforward. That's just recalculating it. And then there's a count vectorizer step before that to make sure that whenever new words come in, we simply add it to the bag of words. And that's it. You know, you don't have to do much to make sure it gets updated. The one thing to note here is that you can introduce a decay vector. Because if we have trained for the last 10 years on some data, and now suddenly a new topic comes in, then we still have 10 years' data of worth in our topic model. So what happens if you have, you know, a bag of words representation with 10 years of data, it tends to focus on those 10 years of data, because it's quite a lot. And that small new data that comes in doesn't really get that much attention. So you can introduce a decay vector, and then you can use that decay vector to make sure that you're not just adding a new word to the bag of words. You can introduce a decay vector by essentially saying, so whenever we update the model, all old data gets a little bit smaller, because it's a bag of words, it's a count, right? We decrease the count with one or two percent. So then it gets slowly smaller and smaller and smaller. Older data and newer data gets a little bit smaller. So as we've been thinking about the design of BERTopic and Weaviate and how we interface this, that really made me think about like the symbolic filter, like adding a where filter for year after 2018, that kind of thing will be really important to the design. And this whole conversation has been really enlightening to thinking about the design of trying to build this kind of vector analysis in Weaviate and what people will need. So you mentioned you have this user perspective and you're building the BERTopic Python library. Can you tell me about your experience building out that library? That was a very interesting experience. It's I think one of the first packages that I developed outside of my master's thesis. And it has been quite right for the last few years, because there are a lot of dependency issues because of NumPy and some other things that need to be taken into account. But it was really, really interesting and really fun to do because there's so many people that want to contribute to the package that have very interesting suggestions. And by focusing on those users and by focusing on what their experience is with the model, I make it more than just something for myself, right? I don't want to bring out a package that I'm only using myself. I'm not doing it just for me. And because there are a lot of people using that, you know, I need to listen to them. I need to make sure that I take into account features that they're looking for, the experience that they look for. So for me, that psychological background, I think has helped tremendously in not only listening to the feedback that has been given, but also taking their perspective. And also understanding, okay, if I'm going to write documentation, for example, how should I do that? How should I make sure that, you know, when I communicate certain concepts, I do that clearly and in a well explained manner. Obviously, I don't always succeed in doing that. And I see that back in the issues, which is great, because then I see, okay, there are way too many issues about this, even though I think it's clearly in the documentation. That means I'm wrong. It's not clearly in the documentation. I should re-examine it, you know, and look for ways to improve it, ask for feedback, things like that. It's just a very similar experience with KeyBERT and PolyFuzz, where, you know, there's a focus on transparency, modularity, user experience, these kinds of things. All of these packages, they're not necessarily the most state of the art, the most complex algorithms out there that nobody understands. Now we're going back to easily explainable topic modeling techniques or packages, giving control back to developers, having them, giving them the opportunity to build whatever it is that they want to do. You know, who am I to say that you should do your topic modeling in this in this way? I don't know every use case. I don't know what you're working on. Who am I to say what it should look like? So that has mostly been the design philosophy behind these packages. And sometimes they work, because it gives a lot of options to developers. Sometimes it gives a little bit too many options as a choice overload, which can be difficult. And balancing that out, having a great out of the box experience while still giving, you know, thousands of options, so to say, that has been that has been tricky, because it doesn't always align with one another. And, you know, you have to make some hard choices here and there. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super cool. Yeah. And I do think kind of in this space of topic modeling, there are like these LDA, like matrix factorization approaches. And I think kind of these steps of, you know, BERT embeddings, UMAP compression, HDBSCAN, TF-IDF, I think that stack is a little more approachable than having to construct a big topic term and then a singular value decomposition to get out. And I also really want to compliment you on your Medium articles that explain BERTopic, because I, you know, I didn't really know that this kind of thing existed. And your articles are just super clear on helping me get up to speed with this. So I definitely want to thank you for that. Thank you. And I think in general, you've documented this extremely well. The Python library is so interesting. There's so much to look through so many interesting topics with this. I think it's a huge idea for understanding vector spaces and all the applications we've talked about. So Maarten, thank you so much for joining the Weaviate podcast. I'm so happy with this one. I think there's so much information with it and I couldn't be more excited about the development of BERTopic in Weaviate. Thank you. ",
        "podNumber": 28
    },
    {
        "speaker": "Maarten Grootendorst",
        "content": "Very nice being here talking to you about this. Really nice conversation. And as always, if there's any feedback that you have, anything that you find, okay, this is stupid, you should change it. Tell me because I'm really open to anything. I cannot make any guarantees, of course, but I'll do my best. ",
        "podNumber": 28
    },
    {
        "speaker": "Connor Shorten",
        "content": "Thanks, Maarten.",
        "podNumber": 28
    }
]