[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone. Thank you so much for checking out the Weaviate podcast. We have a super exciting episode today. We have Erik Bernhardsson and Etienne Dilocker. Eric is one of the early thought leaders and approximate nearest neighbors creating the Annoy Library at Spotify, an incredibly interesting algorithm for achieving this approximate nearest neighbor search. And I mean, these two are just such talented engineers with so much experience in building these kind of systems. And we're here to discuss the main topic of vector search in production, running it in production. What does it take? So, Eric, thank you so much for doing the Weaviate podcast. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Hi, thanks for being here. It's going to be a lot of fun, I hope.",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Thanks for having me. And of course, Erik, thanks for joining. Super cool topic. And I think we can go into all kinds of different directions for what it means to run ANN in production. I think the first one to start is basically the history. Like, Connor, you already mentioned Spotify Annoy. And yeah, Erik, you worked at Spotify quite some time ago, I think. So, yeah, do you want to start maybe telling us a bit about what you did back then as sort of one of the first pioneers of approximate nearest neighbor search? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah, totally. So I spent almost seven years at Spotify, so 2008 to 2015. And a big part of what I did at Spotify was building the music recommendation system. And I don't want to get like super detailed, like how that works. But like, roughly speaking, like what we ended up realizing, or what I ended up realizing, building this is that various types of vector models work really well. So we would, you know, do collaborative filtering on this extremely large sparse matrix of what users listen to. And then, you know, basically sort of matrix factorization would then lead to vectors. And then once we had those vectors, you know, you could do all these sort of, you know, they have this like nice property that you have like a fairly low dimensional space. And you can use nearest neighbors to find recommendations for people. So you kind of project users and tracks and artists and albums into this, which would give us like 40 dimensions. And then in that space, it turns out like if you want to like find good recommendations for a user, you just look at like similar near vectors for vectors that are close to that user. If you want to find similar music to a different track, you take that track's vector, and you look at in this 40 dimensional space, what are some other vectors that are close, right? And so the problem now is just like you have to figure out how to do this. Like you have to find the specters fairly fast, right? And as Spotify, we had a few million tracks at that point that was indexed in the recommendation system. So about, I think, 2 million tracks or something like that. Now it's probably more like 10, 20 million tracks that are indexed. And I looked at a bunch of different approaches and a bunch of different existing algorithms for various reasons. Like one of the things I realized pretty early on was not going to go super deep into this, but like I wanted the functionality to mmap it because what we wanted at Spotify was we had a fairly static dataset, but we needed to do a lot of recommendations very quickly. And when you deploy this, you want the ability to basically like create an index, load it into whatever many processes as you have CPUs, and then do all this like nearest neighbor search. So I basically built a static file index based on mmap and built Annoy, and in a way, like basically works, kind of splits the point space recursively into a bunch of trees, does that a few times to get a forest. And then in that forest, you can search. I do want to point out, I think actually today there are a bunch of better models. I think Annoy still is useful in this sense. I think it's one of the few simple, it's just like a file, embedded file, and it uses mmap. But of course, today there's Weaviate and many other ones that can probably do a much better job at finding nearest vectors very quickly. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Cool. Thank you. Super cool, cool summary. And yeah, it's super exciting to hear basically how early you were in doing these things that are now everywhere. And of course, recommendation being a big case. And I noticed two things I think that we can talk about a bit more. One is you said essentially the recommendation system or the recommendation approach was just doing a near vector search. So basically, do I see it correctly that you never had to do online model inference? That you basically did the model inference once, built your index, and then sort of when the user listens to a song or when they select the song, you really just care about the nearest neighbors and never have to deal with the inference? Or did you have to do any inference sort of at what we would call query time in Weaviate, but I guess in Spotify, I don't know, play time? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. I think of it as like online versus offline inference or like batch inference. We did basically zero online inference when I left Spotify. I think they do a lot now, but we would essentially pre-compute almost everything. So we would do these, the collaborative filtering models, we would recompute every month or something like that because they were very large and took a lot of time to do it. Now that would give us a track embeddings, right? Like a few million track embeddings. Users taste ships faster. And particularly you have this problem that new users come in and you want to give recommendations to them relatively quickly. So we would recompute those vectors every night. And then every night we would basically go through every user that had some activity last 24 hours and then pre-compute recommendations for those users. And I don't know if that's like a particularly efficient approach. It just turned out that like the way we built, had the system set up at that time made it a lot easier to do everything sort of a batch oriented way. And so that might also explain some of the design decisions of how Annoy worked. It's quite optimized for more like throughput rather than like sort of online latency. In particular, you know, I talked about the mmap, you know, it's quite optimized for doing a lot of, using multi-processing and doing a lot of things in parallel. But there's probably many other ways you can do it today. And I'm sure Spotify today does a lot more like sort of real-time stuff, right? Like as you start listening to the music, probably I'm guessing they, you know, update that vector in real time to the inference. But I mean, there's still like batch inference stuff going on in Spotify, right? Like Discover Weekly, you know, is only re-computed every week. So I think there's still, you know, use cases for both online inference and offline inference. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Cool. Super cool, this sort of the, yeah, doing the inference in batches and then having that static dataset that you mentioned. I think this is, yeah, as you said as well, it's shifting a bit, but I think this is for me, one of the typical cases where you can easily make this distinction between a search library, an ANN search library that typically is built for that static case. So beyond Annoy, if we look at FAISS or Google ScaNN or all these libraries, typically they're built for those sort of build once, then surf kind of use case. And this is for me, a nice point to sort of draw the distinction or do the distinction between what is a library and what is a database is when we started with Weaviate, one of the things that we wanted to give our users basically from the get-go is an experience as they would have in any other database. So not so much from the like, okay, you're starting with a library and now you're trying to sort of get the library as a service, but the other way around, you're starting from a database, like you may be, you're used to using MySQL database or noSQL, it doesn't matter, but basically any database in the world, you can incrementally sort of build up your dataset. You can do updates, you can do filtering, these kinds of things. And that is something that we, yeah, basically that led our decision to start with HNSW as the first vector index. We definitely did look at Annoy. I think we even had prototypes before with Annoy. I used it also in this contextionary that we have till this day, which is basically like a very simple bag of words model, which is sort of static. It's built on a glove or fast text. So we have this kind of static. And if you just want to search within there, we used Annoy for that. But yeah, that kind of sort of online changeability, that was definitely one of the reasons why we looked into HNSW. And yeah, definitely one of the parts that make sort of Weaviate a database as opposed to a library, I would say. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "I think there's kind of maybe two things going on at the same time, right? Like there's like the sort of online offline distinction, right? And you look at like a machine learning model, you can either use that machine learning model, so an online inference or an offline inference, right? And then the separate aspect is the sort of the CRUD support, like supporting like CRUD, like what is it was a separate create, read, update, delete, like the ability to mutate the data set, right? And I think, you know, like my sort of, I think the sort of ideal here is like a service that does all four combinations of those two attributes, right? And also does it with zero sort of performance loss. Because I think, you know, the online case is a lot harder than the offline case in many cases, right? Because you need a lot of latency. And I think, you know, it's easy to sort of give up a lot of the throughput by optimizing for the online case, but actually don't think you have to. I also think that sometimes we talk, you know, when I look at like machine learning as a whole, people generally tend to think of like model inference as primarily like an online thing. But if you actually like, there's like, you know, 5,000 startups that do like, you know, model deployments, right? But if you look at actually how people use models in real life, I would say, you know, it's fairly evenly split between batch inference and online inference, right? Batch inference generally has a lot more, you know, it's more important that you have a lot of throughput. So I think, but I actually think there's ways you can build your system that you get, you know, the same throughput, even in an online mode, you just have to be smart about, you know, batching and other things. So, to me, that's like the dream that, you know, and I think Weaviate it hopefully delivers in those things. I haven't done the benchmarks, but I think it actually does. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, I think on the model inference part, there's definitely room to grow. But on the latency part on the search, definitely. But yeah, so batching, I guess, sort of when we think of batching in a DML sense, it's typically like this offline batching, but you can still do like a mini batching approach, right? Where you just do the batch just big enough to basically have the benefits of say running on a GPU, which paralyzes enough, but still have it sort of in a real time approach where maybe instead of sending or instead of doing 100 single inference tasks that wouldn't have to run sequentially just batch those 100 up, which happened maybe, I don't know, in over the course of 50 milliseconds or so, and then have them batch and basically get the throughput this way, like trade off a bit of latency for the higher throughput. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "I think this is especially important when you have like matrix algebra going on in your system, right? Like either, especially when you're using GPUs, right? Because like GPUs benefit a lot from batching. If you don't do that, then you might as well just like run everything with separate threads and it's like kind of fine. You have some locking issues, but that's fine. But I think the sort of mini batching or micro batching that you see in some high performance model serving frameworks, they do benefit quite a lot when you're using matrix algebra. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Cool. Yeah. So maybe at this point is a good point to tell a bit about the different options that people have or that users have to do inference with VBA. Because as you said, some like to do it offline and just sort of only basically do the re-index part with Weaviate. So this would be the typical case where the users provide their own vectors and they're not using Weaviate end to end, but basically batch indexing and or do maybe doing some kind of a blue green deployment where class A, so classes are the isolation units in Weaviate, where class A would be serving with the old model. You can prepare class B, build up the index in the background, and then basically just switch the load balancer from one to another to do this kind of shift. So this would be like the weekly example. And then also for the live inference parts with Weaviate's module system, which basically gives you the ability to plug in any kind of model really, and not just any model, but also any model provider to do the end to end inference. And there would be recently added a support for the hugging face API. So basically one of those hosted inference service types so that you can really basically do all three variants that you want to do. And then of course, if we're talking about the combination with CRUD, all of them can be combined with the CRUD part of the database. But you could basically bring your own vectors, which sort of gives you full control about how you do the inference. You could do it live, you could do it online, you could do it offline. You can do it with sort of hosted together with Weaviate. So Weaviate's module system and just brings a small inference container. There you can still sort of split this up into, do you take what Weaviate provides and just plug in your model or do you replace the entire container? So for example, if you say, I can do this better, I can optimize this better or something, I can do it. Or now the new option is basically if you want the no ops approach, just use one of those third party providers where we're supporting a hugging face, Cohere, I think, and OpenAI also. So with those three, basically gives you the ability to plug those in. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "I think this is super cool. I mean, I don't know, kind of reflecting on this as someone who spent time on this a decade ago or more, when I discovered this vector approach, I was actually just talking to someone the other day, but I actually feel like that's one of the most profound insights I've had in my life was this vector approach when I realized it back in 2008, I'm like, this makes so much sense. You have this nice space and you can think of things in terms of Euclidean geometry. And that sort of mental leap for me was realizing once you embed all these things into the space, a lot of relations become more natural. The hard part is the query. And 10 years ago or 15 years ago when I was doing this, it was very hard to query. So I basically had to roll up my sleeves and build this C++ library with Python bindings that I could import and use for my janky Hadoop jobs. But I always was a huge believer in vector models. And so one part of me is extremely excited that the world has come around to realize the power of vectors. I'm very excited about the new databases coming out, the new embedding models, and in general, just acceptance, embracing of the fact that we're in a vector space. You can use these vector models for a lot of stuff. And what I'm excited about is I look at all these technology in the past, various search algorithms or various types of ways we did recommendation relevance in the past. I think there's a huge opportunity to take a step back and rethink so much of that from ground up. And so I'm pretty excited Weaviate is working on that kind of stuff, but also many, many other players. And in particular, outside of search, there's also a lot of work on embeddings and other things that I think is really exciting. So that's fun to see. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Yeah. Couldn't agree more. And we're basically, even though you mentioned 2008, so even though it's 14 years later, I think we're still at the beginning. It's such a massive trend and it's going to change search, but goes beyond search. And we're really just at the beginning. But yeah, what I also really like is this kind of growing together with the space. And I think that was something that we tried to be like from the get-go while Weaviate can be used end to end, like that end to end user experience is super important. We never said we need to entirely own the chain end to end. Like we would say, I don't know, you could only use it with models that we train or something. And all of a sudden we're competing with these specialized startups and then not just startups also Google and Facebook and then all of them are meta now. But really just say like, okay, we want to integrate with those players in the space and basically grow Weaviate with the space, but also help grow the space with Weaviate basically. This kind of partnership. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "I think that's a good approach. I mean, in general, I would say when I look at machine learning, I think it's kind of bifurcating a little bit where you're going to have the Googles and Facebooks and the other big providers training these enormously complex, very large models, OpenAI, DeepMind, and Cohere and a few other ones. And most people won\u2019t need to train a model. They can just take these models and use the embeddings that come out of these models or use the predictions or whatever. Embeddings are often the penultimate or some intermediate layer in these models. And then do their own fine tuning on top of that. So I think that's a big shift in how people work with machine learning also in the last five, 10 years, five years, maybe even three years, I want to say. But I think it also opens up, makes it a lot easier to use these models. In the past, when I did things, I had to do the whole thing end to end. Now I can just download a transformer model from Hugging Face and do two thirds of it. And then I can throw some simple stuff on top of it, which makes it a lot easier to do these things and with great quality too. There are so many pre-trained models for English and computer vision, I mean, any language really. And those models make the life a lot easier for a lot of these, for people trying to build their recommendations or classification or any sort of industry and machine learning application. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. And it's basically getting better day by day. So another topic that I want to talk about is benchmarks. You're of course famous for the ANN benchmark website. But before we go into that, we talked a bit about what you did at Spotify, but I'm also super curious about what you're doing right now. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah, I'm working on this startup called Modal. I started it, I would say about a year ago and it was still pretty early days. But the idea is, I'm sort of basically like, I guess like the best way to put it is like I'm building the data tool I always wanted to have for years. And so I found that when you look at data teams, how they operate and how they productionize things and how to scale things out and schedule things and how they work with infrastructure and the tools they have, it's all kind of janky. There's so much time wasted on dealing with infrastructure, setting things up, dealing with Kubernetes and Docker and AWS and Terraform and Helm and all this stuff. And I think as a result, building code and running data applications today takes five times as much time as it should do. I mean, I literally, I've been wasting the last three days trying to configure VPCs in AWS. And I really think that's hell for a developer to ever have to deal with. And so what is Modal? Modal basically, we built our own sort of Lambda style infrastructure provider for data jobs. We make it very simple to take code and productionize that in the cloud. We are a cloud provider in that sense. We take people's code, put it in containers and run it in the cloud. And we can do that extremely fast because we ended up building our own container runtime and a bunch of other stuff for Autosys and many other things. And so the end result is if you want to do, especially if you're an early stage tech company, you don't want to set up all this traditional all the infrastructure. Using Modal, you can immediately do everything from schedule retraining of your jobs to deploying inference, to scaling out large scale, like batch inference, large scale mapping over various types of sort of embarrassing, with parallel tasks, like just coding, web scraping, simulations and back testing and other things. And build a lot of end to end infrastructure for machine learning, but also like other, more like mundane data tasks like reporting or whatever. And do that very easily without having to think about infrastructure. So sort of, you know, kind of selfishly building a tool I was more than to have, but you know, also sort of think that many other people would benefit from this tool as well. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Really cool. I saw, I think it was a screenshot or a short video on Twitter where you shared basically the API. And do I understand it correctly that I can basically write Python code and I just annotate a function and then it runs in the cloud? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. And I think that it's a sort of this idea that I think other people have had too, which is sort of people sometimes refer to as the self provisioning runtime, like the idea that like everything is just code, right? And the code itself specifies what sort of infrastructure needs, including the container specification, the parallelism, like all these other things. And then when you run the code, the code just like gets those resources in the cloud for itself. So it's a sort of idea of, you know, you take the app code, but you also put in all that, you know, container code and environments and cron jobs and credentials and all this stuff and just like write all of it in code, right? Like there's like no config in modal, like everything is just code. And which makes it very concise, makes it very easy to run and it makes it a lot easier to maintain. Right. And so, yeah, like everything in code is just something, I don't know if you ever looked at Pulumi, but sort of the same idea, but taking it even further, Pulumi only does the infrastructure in code. Like we also do the, we do everything like the infrastructure and the app code together all in Python. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Really, really cool. Can users try this out? Like can I just, could I just register right now or in what state is it, or would I have to wait a bit? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "No, but we're working on it. So we're still in the early beta stage test, right? And we have a bunch of users using this and a few paying customers. And so we're, you know, we're slowly scaling up and, you know, if you want to, if you're interested in modal, feel free to go to modal.com and join the wait list. You know, we're sort of definitely going to make more announcements in the next few months and gradually open up and add more beta testers. So if you're interested in trying it out, definitely go to modal.com and sign up or put your name on the wait list. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Cool. Nice. Yeah. So as I mentioned before, we talked about modal, I mentioned benchmarks and of course there is ANN benchmarks, which is, I would say, the de facto standard for comparing yeah, comparing a vector search. I don't want to say either libraries or databases, I guess you technically have a mix of them in there, but maybe I'm just going to say algorithms and their implementations. Would that be correct? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah, I think that's correct. And it's mostly libraries. Like it sort of started out at almost, you know, five, 10 years ago. And back then it was mostly libraries around. I mean, ANN benchmarks was like one of these projects that like, I got annoyed because I would read like various like papers at conferences and they all had their own like obscure like benchmark setups. And, and of course like every lie, I mean, I'm also biased because I built the Annoy, but like every library maintainer, I felt at that time would build their own benchmark suite. And, you know, the rules were like their library was the fastest. And, and I found a lot of, you know, very frustrating, but also people sometimes would come up with new types of like ANN algorithms and like, and not benchmark them. Right. Like one of the things I was always frustrated with is like this research and locality sensitive hashing. Locality sensitive hashing, like basically it doesn't work in my opinion, like it's a terrible algorithm, but like people would keep releasing papers about it because they have this like nice mathematical properties, but like if you actually run the benchmarks on it, it's just like awful. And so, yeah, so there's a couple of different reasons why I was frustrated and ended up building my own benchmarking suite. Initially it was kind of janky and you have to, to, to like install all this stuff and it barely would work. But a few years in, I actually set it up in a nice way of sort of containerized everything that made it easy to reproduce, added more libraries. And so, yeah, so it's grown pretty extensive benchmark suite. I think it's showing a little bit of age in the sense that it doesn't support CRUD. It's very, you know, all it does is like, you know, a kind of static offline batch inference and really just measures throughput. So, so kind of going back to the first thing we talked about, it doesn't handle, you know, the CRUD case. It doesn't really benchmark that. It also doesn't really benchmark latency. So, so some libraries or databases may optimize for latency at the cost of throughput and it was our ANN benchmark doesn't really optimize for that. It doesn't really take that into account. But I think all in all, like, you know, to me, like it was something at that time that maybe still there was very much needed in the space. Cause there's all these like libraries out there making claims about, you know, the performance and until ANN benchmarks, I basically didn't feel like there was like a sort of a neutral, semi quasi neutral. Cause I'm, again, I'm the author of Annoy to, but you know, sort of benchmark suite that actually tried to compare all of those. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Yeah. And then, I mean, the fact that you're not trying to sell something with it, I think that that sort of gives you a lot more credibility. Like if, if we, as semi would basically put out some sort of a comparative benchmark right now where we would benchmark, let's say Weaviate against, I don't know, our competitors or something, I think that would be, you would always have to take that with a grain of salt and you should, because so yeah, it's really. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah, I agree. Yeah. And there's all these like weird ways you can like kind of fake it too. Like one of the things I, I kind of what was, you know, with the Annoy, with sort of ANN, like it's kind of hard too, cause this is sort of a weird trade off between like, okay, what is the desired recall and what is the desired throughput, right? Like if you, if you want to maximize throughput, you can like, you know, make the recommendations really terrible and vice versa. If you want to, you know, make the recall extremely good, then you can like, you know, you know, make it very slow. And so one of the things I also did it in ANN benchmarks, I pushed for a lot is this sort of, you know, you have to look at the whole frontier, like the whole Pareto frontier of like, you have to actually make a 2-d plot and like, you know, plot the trade off curves and do that for every single library. And what that means is like for every library, you have to run all these like different parameters and then compute that as sort of, you know, the trade off curve after the fact, like after you have all that data. So, so it's sort of, it's a little bit tricky to do that thing. You actually have to run, you know, last time I ran it, it took, I think something like two weeks to run all the benchmarks. And there's a fair amount of compute that goes into rebuilding those benchmarks. So if anyone is asking like, why I haven't updated it in a while, it's because it just takes a little bit of time to set everything up and then run it. But I should do that at some point again. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, yeah, definitely. Super cool. Yeah. And I think that that frontier curve, I think everyone who's looked at some kind of ANN benchmark or a sort of a non-comparative vector search benchmark, everyone knows those kinds of graphs where like top and to the right is best. So that's, that's, yeah, it's nice to hear that you came up with that because basically the first time I looked at an ANN benchmark, which was ANN benchmarks, that was just there. So I didn't even think about that. Yeah. Someone had to come up with that at some point. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. I don't know if I necessarily invented that. I'm sure there was in some older papers too, but I definitely, I do think that ANN benchmarks like push for that and sort of popularize that and made that, hopefully the de facto way to compare things is like, looking at the two different tier, making that trade off very explicit, I think is very important. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, yeah, absolutely. So in our benchmarks that we have, which are not comparing Weaviate against something else, but we're just basically showing like, this is how Weaviate performs under the circumstances, we also have that. And, and it's, I think this is a good tool as well. This, this kind of frontier just to educate users because, and I've had a couple of conversations with users and we're saying like, Oh, I want ANN, this is so cool. I using this ANN system. And then they're like, why doesn't it have perfect recall? And then the conversation typically goes like, do you know, like, are you aware what ANN is? Like the A in ANN stands for approximate nearest neighbors. That's basically that trade off between recall and if you're like, Oh, okay, yeah, that makes sense. But then having those kinds of graphs and seeing like, even in an approximate nearest neighbor situation, you can still achieve high recall, you just need to need to basically give up a bit of latency and throughput, and just use the benchmark as a, as a guide, basically, of how to set parameters and how to configure it. I think that's, that's also super, super helpful. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. I was going to say the same thing.",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "So I said, it's maybe not a coincidence. Maybe some kind of subconscious decision. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Fun fact, one of my daughter's name is Ann, A-N-N, but it's not, it's a pure coincidence. We have that name in the family. And although she was born after ANN benchmarks, so maybe it was consciously influenced. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Do you also have a child called K-N-N? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "It's a little bit harder to pronounce, but yeah, yeah, maybe, maybe one day. Yeah. The funny thing is I haven't really thought about that until now. It's kind of weird. Maybe I did name her after approximate nearest neighbors. I'm going to tell her that when she's older. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. So, so benchmarks, basically we've talked a bit about the value that they provide. And I think in a, in a sense, it's, it's really good to, to have, or to, to give you these kind of insights in, into how these, these algorithms perform. And at the same time, I think as a user, how much do you care, basically? Like, is there a point where you go, okay, it's just fast enough, or do I need to, like, of course everyone wants to have the best, but yeah. What is sort of, if we're talking about, let's say a two millisecond versus a four millisecond latency, like, is this something that matters to the user? ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "I mean, my opinion is like, you know, like going to a hundred percent, I don't think it's ever worth this because there's like a point where it just like takes longer and longer to like really, you know, essentially you're just doing like exhaustive search, but like, if you can get to like 99.9%, you know, that's like usually like pretty much like good enough, right? That's like almost as good as like a hundred percent and, you know, going to 99.9 or like whatever, like 99, that usually ends up being like a 10X or a hundred X improvement over exhaustive search, or maybe even more a thousand X if you have like, you know, a very large data set. And so I think that's kind of the sweet spot is like, you know, for, for most users, if I just add a pick a point for them, I'll probably pick, you know, the nine, you know, the three nines, 99.9, that percentile. And, you know, that's close enough to a hundred percent that practice, you know, won't be a problem. But still, you know, implies like a pretty substantial performance improvement. But I'm curious what you think at the end actually.",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Yeah. So it's definitely like, I kind of looked at this from the perspective of latency and not so much recall, but I like the way that you're looking at this as sort of optimized for recall first and then see what latency you get. And the point that I was going to make is basically that let's say these two numbers that I just said, like two milliseconds or four milliseconds, I think for the kind of SLOs that users have to meet for whatever they're doing on top of vector search, they're probably not serving pure vector search to the users. They're probably building some kind of application on top. Let's say they have a hundred millisecond SLO for the end to end thing. Then it's probably not going to matter if it's two milliseconds or if it's four milliseconds. However, you can't really look at latency in isolation because latency probably also ties into throughput, at least for the same kind of hardware cost. Like assuming that it scales linearly across threads and you have a single thread and on that single thread, it's two milliseconds or four milliseconds. That's basically twice the throughput. And that I think is much more interesting. And there, I think if we take this one step further and think about how do you actually run ANN in production and for example, using the Weaviate cloud service where we now have usage based pricing, then it sort of, I think it becomes a matter of good enough. Like I need this kind of latency SLO and I need this kind of throughput, but I can basically get the throughput through horizontal scaling, which of course increases your infrastructure costs. But if you have usage based pricing, then you don't really care so much about the infrastructure costs. So that's basically where I think, yeah, you need to kind of prove that it's fast and fast enough and that it's never going to be too slow to achieve your goals. But at the same time, yeah, the end it's basically, it's more kind of an optimization that we have to do internally to run more profitable on the Weaviate cloud service than what the user should ultimately end up caring about. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. And it is of course use case specific, right? Like what is the cost of a true, of a false negative, right? So, so, so basically like, you know, the fact that like there was something in the results set that, you know, the, the, the ANN algorithm didn't find, right. And it's, it's going to be very high for search, right? Like, you know, if the user is searching for something and you don't find it, like that's kind of a bad experience, right? For recommendations, like not so much, right? Like if you're just making a recommendation, if you want to recommend like, you know, 20 tracks, you know, to someone at Spotify and it turns out like, actually, you know, if you had done an exhaustive search, you know, that, you know, the 21st track wouldn't have been there because it would have been another track like no one's going to notice. Right. So I think it's extremely, you know, so for maybe like recommendations, like you, you don't have to go to like, you know, 99.9%, maybe it's fine to go to like 95% now, like, I don't know. Right. And that's, you know, those are the tradeoffs you have to think about. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Yeah. Great point. Yeah. Yeah. And I think also you should take into account how good is your model actually at predicting, like, even if you have a hundred percent recall, that doesn't mean that, that from a user's perspective, the match is good if the model just created bad embeddings, basically, like it could be the true nearest neighbor, but just, yeah, it was just the wrong embedding basically. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. And I also wonder to what extent, you know, people then end up like, you know, doing sort of a multi-stage re-ranking anyway, like at Spotify, what we ended up doing was we actually had an ensemble method, although I think they changed that later. But, but so what we would do is we actually had not just one vector model, but we had, you know, I don't know, half a dozen vector model plus a bunch of other stuff. And so what we would do is like, we would use each of these vector models, you know, to search for candidates. And so for each, you know, if we had to generate a hundred recommendations from user, we would ask, you know, each vector model, like, give me a thousand, you know, tracks. Right. And then we'd pull them and then we ended up with like 5,000 tracks. And then we would, you know, we basically re-rank it using an XGBoost, a gradient boost decision tree based on a bunch of other features too. That, you know, all kinds of like, you know, some of them are like, not at all, like clever filtering vector based, like some of them were just like, is it Christmas? Then, you know, we should boost Christmas. Like there's like all these like, you know, weird features. Right. And then we would re-rank based on that. And, and, and I think, I don't know, I think it depends on the use case. Like, I think, you know, sort of advanced, you know, companies, machine learning practitioners like Spotify or, you know, larger like e-commerce applications. I think they're always going to have this like multi-stage approach. But of course, like, you know, for a lot of like early stage startups or a lot of, you know, customers that don't have, you know, a whole machine learning department, they might just want to take the output from the embeddings and just return that to a customer. And I think that's going to be good enough in many cases. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Or do some, some minimal kind of re-ranking with them. So for example, then we're getting in the direction of hybrid search where, so for our listeners, hybrid search is basically the idea that if we're in the context of text, you have your dense vector model that basically creates your vector embeddings, but you also have traditional search, so to speak. So BM25, BM25F, these kinds of algorithms that boost keywords or boost keyword matches or phrase matches, et cetera. And yeah, with hybrid search, the idea is to combine both of them and combining can be independent. Combining could mean that you do some sort of minimal re-ranking or boosting based on keywords based in like, do the vector search for us, boost on keywords or do it the other way around. And that can also help basically overcome limitations of either the ANN algorithm, if the recall isn't high enough or the model. So text model sentence transformer models, for example, they perform pretty bad out of domain for exact matches. So that is something that can be overcome with hybrid search. And there, I think we're also trying to really lower the barriers of entry for, yeah, as you said, for smaller startups that don't have a whole AI department working on this by making this easier and easier with Weaviate. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah, totally. And I'm kind of curious about the evolution. I mean, this may be a side topic, but I feel like 10 years ago, you would just start with something like Elastic doing inverted indexes and that would be like, you would productionize that and then you would have search. And then now today it feels like, okay, you start with maybe inverted indexes or you start with vector databases. And then at some point, you start adding features and you start breaking it up into a multi-stage pipeline. You have both vector and inverted indexes working in conjunction. Then you throw in a bunch of other features too. And then you throw in extra boost on top of that or whatever. And you have this three, four stages of candidate generation, candidate re-ranking, filtering. I'm kind of curious what do you see in the world of search and relevance today? Is that sort of an accurate way to think about it? ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Oh yeah, absolutely. And I think you have all stages. Basically, you have companies doing just one stage, two stage, you have multiple stages. We have use cases where from our perspective as semi, basically, we don't even see what's happening. All we know is the customer tells us, well, we use Weaviate for candidate generation and then we have that kind of pipeline on top of it. And of course, that is something that's specific to them. And probably where all their magic is happening and something that they're not going to be very vocal about because that's kind of their secret sauce. So yeah, sometimes we as the database provider, as the vector search provider, we only see that first stage, the candidate generation stage. But even in that candidate generation stage, I think we can... So for example, with hybrid search, we can incorporate a bit more of that part. And who knows, maybe later on, it could be even higher that you could do four-stage ranking or search with Weaviate. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. I think that makes sense. Because I think users will generally start with something that does the simple thing, the pure vector approach. But over time, as they get more advanced, it's nice to offer those things too, as they start to think about re-ranking in multi-stage pipelines. I think offering that as a black box, not actually as a black box, but as kind of a gray box, you can feed some data into it, but you're not 100% sure how it works. To me, that makes a lot of sense as a product. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. One two-stage pipeline approach that we've actually had for a while, but I keep forgetting that it's a two-stage approach, is the question-answer extraction that you can do with Weaviate, where basically you just have the dense or whatever search you use as candidate retrieval, and basically use the question-answer extraction model that also returns a score as a re-rank step. Or even if you don't do the re-ranking, just to extract the kind of information from that text snippet. That's a very simple, but it's also a two-stage kind of pipeline that you can also do out of the box, basically, with Weaviate. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. Makes a lot of sense. At Spotify, an incredibly important one stage in the pipeline was just removing tracks that the user had already listened to, which kind of makes sense. If you're recommending music, you obviously don't want to recommend tracks the user has already listened to. That was the last step in the pipeline. We basically used Bloom filters for that, because it's very space efficient, as we would recompute Bloom filters every night and then use those binary representations to filter out the candidates very quickly. So, we didn't have to... Actually, it wasn't the last stage. We would generate candidates, remove everything that was already in the Bloom filter, and then re-rank the resulting ones. And Bloom filter also has false negatives. Bloom filters will occasionally flag something as belonging in the... I guess you should call it false positive, but from the user's point of view, it turns into false negative. They will sometimes think that... Sometimes, because the Bloom filter is approximate, we would think that, okay, the user had already listened to this track, but actually they didn't. It was just a false positive. So, we would then remove that, turning it into false negative, where the user wouldn't actually get that recommendation, because the Bloom filter thought that the user already had listened to it. But yeah, I'm just mentioning as one example of one stage in this multi-stage pipeline that I'm talking about. And I think everyone's pipeline will look a little bit different, depending on the use case. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. When you think of stages, it always sounds super complex, but it can be something super simple. What I really like is that you're essentially using database technology. There are plenty of Bloom filters in Weaviate, just for the object store, which is an LSM-based store, and super cool that you were using Bloom filters there as well. So what I find super interesting about removing this in the last part, if I understood it correctly, that is essentially a post-filtering step. So, in the worst case, you could run into a situation where you would run out of candidates. If the user has listened to everything, then everything would be removed from the search. Yeah. So that's another situation where I think the pre-filtering that you can do in Weaviate would be super helpful these days, because you can just remove them before generating the candidates. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. And I don't know if that was in practice. I think it's in theory, it was a problem at Spotify. But in practice, we generated so many candidates that it kind of... I mean, we would generate like 10,000 candidates for every user, right? And do that times a number of different models that are all vector-based. So in the end, pooling those, we would end up with 20,000, 30,000 candidates, and then we rank them. But first, filter out using Bloom filter, then we rank them, right? So I think in practice, occasionally, they would end up with maybe one less recommendation. But I think in almost every case, we had more than plenty. ",
        "podNumber": 25
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. Great example of basically need to do whatever is right for your use case. I could imagine search cases where that would be very problematic. Yeah. Thank you so much for joining. I'm looking at the time to see where we're approaching our end. This was super fun, super, super nice to hear. Yeah. Hear about Spotify and Annoy, hear about, yeah, ANN benchmarks, and then of course about modal. So do check out modal. As we heard, you can't register yet, but you can register for the wait list, right? So do that. And check out Weaviate as well. If maybe Eric was the reason you got here and not Weaviate, and if you haven't heard of Weaviate, then check out that as well. Check out our other videos. And yeah, thank you so much for coming. Had a great time. ",
        "podNumber": 25
    },
    {
        "speaker": "Erik Bernhardsson",
        "content": "Yeah. So it's fun to talk about this stuff. ",
        "podNumber": 25
    },
    {
        "speaker": "Connor Shorten",
        "content": "Thanks so much, Eric.",
        "podNumber": 25
    }
]