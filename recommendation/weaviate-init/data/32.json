[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone, thank you so much for checking out another episode of the Weaviate podcast. This is going to be a super fun discussion. We're talking about You.com, the Spark connector with Weaviate, and generally billion scale big data technologies and Weaviate and all these exciting things around search interfaces and so on. So firstly, I'm so excited to welcome Zain and John for the first time on the Weaviate podcast. Thanks so much for joining the podcast, guys.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "Hey, everyone. ",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Thanks, Connor. ",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So I'm also super excited to welcome our external guest, Sam Bean. Sam is working on You.com. Sam, thank you so much for joining the podcast. And can you tell us a little bit about what You.com is? And yeah, maybe we could just start from there.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, thanks for having me. So You.com is a newer search engine. It is a kind of started as like a play on hitting the niche between personalization and privacy. We've since evolved the mission considerably. And just recently, we kind of announced the latest big kind of press that we're making in the search space, which is kind of democratizing search and what we're calling the open platform. And so this is kind of like a mixture of some developer tooling, DSL, and it allows basically third party developers to create, hook up their data and create search widgets and applications, as we call them, that can show up natively on our search results page. So we kind of, you just tell us like a little bit about what keywords it should show up for, and we handle like the ranking and the retrieval and rendering all of that. And so really great discovery tool for third party developers who want to kind of plug into a search engine.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "So it seems like the whole kind of search interface is being redesigned at You.com, in addition to, of course, like the retrieval technology. Can you tell me about, like more about the widgets and sort of how you're thinking about how people interface with search engines might want to, I see like you can generate an image, you have these things for, you know, having the code assistant tool. Can you tell me about the thinking behind the interface?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, sure. So I think that we've seen apps kind of take a more prevalent role in the search space. Like you probably haven't called them that on your main players in this first search space, like Google or Bing, but you've seen them like your side panel, Wikipedia panes. Those are kind of all things that we call an app, which is basically just a way to take our search data and curate it and decorate it in such a way to kind of give it like a special presentation and enrich the user experience a little bit more than kind of your standard blue links are, to give a better experience. And basically what we do is we operate like a very partitioned index. And so basically we are able to reach into specific slices of the internet. You can think of them as like vertical slices of the internet instead of being very horizontally searching. And then we know a lot about the data because we've crawled it. We've enriched it with a lot of like machine learning systems that we have internally. And then we can kind of like using what we know about the data that is from that domain, we can create like a much more curated experience for the end user. And so things like our Stack Overflow app, to your point, some of our like our code generation applications, our You write application are all kind of like very innovative apps that are in the search space.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's extremely interesting. The whole, just kind of the high level idea of like a symbolic filter might be how we think about it, like in specific Weaviate land. But this idea of like, I want to search through Stack Overflow. I want to search through Twitter, Reddit, like this interface around picking the source of your information and searching and having that smooth integration. So I kind of first became familiar with You.com with, I first started following Richard Socher's research around these kind of systems with the CO-Search system. It was this like COVID-19 information retrieval system. And it had this like brilliant diagram that really outlined it. Can you tell us about kind of the origin story of You.com?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, so I think that the way that you hear is that there was, Richard's had this idea for this kind of like this vertically integrated like search index for a while. And I think that he was working on it for a bit. And then with his work at Metamind being acquired by Salesforce, entering the C-suite at Salesforce, and then kind of like doing a lot of like amazing work there on the transformer technologies, especially like CTRL and I think CO-Search are two of the shining examples. Meeting Brian McCann, who's our CTO, I think that they realized that it was a good time that Google's grip on the search space was kind of weakening a little bit because they are, I think, fundamentally, their attention is marred by what is dominantly more and more like a marketing company. And so we thought that it was a good time that there's an opportunity. There's a lot of people around the globe who are kind of wanting privacy more and more. And so to give people that kind of ability to have a private entry point into the internet, and also giving, I think, what we feel are some pretty fresh takes on the search experience. It's a lot of responsibility being like the entry point into the internet for a lot of people. And it felt like that was kind of that responsibility falls on a few select players and the way that data privacy was being handled for being such a high responsibility. That it was kind of like, it's one of our central tenets that, you know, trust facts and kindness are very central to the company. And so I think taking up the mantle of trying to create a new entry point and gateway into the internet with some of those things at the center of it was, I think, largely the motivation for getting into the space. So that was kind of like a constellation of ideas of kind of how we got here, but it's kind of a globe-trotting tale.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's super interesting. And I really do want to come back into privacy and focus on it and the particular details of how to deliver it and the new technology. But I think kind of a conversation we're very curious about, can you talk about how you came across Weaviate and then we can dive into the Spark connector?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, sure. So in building kind of like our own indices of the internet, we knew that we wanted semantic search capabilities. We kind of knew that the way that transfer learning and NLP was advancing and where it had gotten to when we had started the company would give us a pretty unique advantage to build those capabilities as foundational units. I think that obviously Google and Bing have a lot of those capabilities, but their systems are largely kind of built off of lexical search. And then a lot of the semantic pieces are kind of like retrofit onto their retrieval systems. And so we knew that we wanted to have these things as kind of like first-class citizens from the ground up, that semantic search was going to play a really important role. And we were kind of like looking around for what semantic search databases we wanted to use. And I think that very early we were connected with the people at Weaviate and found that kind of the open source ethos was a really good match for this, that it was a really good match for the company's kind of like background and the way that we were able to kind of build those capabilities and the way that we were able to build those capabilities and being able to contribute to things like this in the space was really important to us. And so it made a lot of sense to start to invest in the ecosystem and learn the technologies.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "I guess a separate question I had before we get into the Spark Connector is in the future, do you see yourself using all the different capabilities instead of just semantic text search? Do you also see yourself using semantic image search, audio search, those sorts of capabilities?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, I think that we're trying to think of, especially with what we're trying to do with our You Imagine suite, which is like an image generator from natural text, something that is very popular as of this year, that trying to kind of create like this very multimodal search experience across images and audio and text is going to be important. And being able to handle having one system that kind of handles vectors in a very vector-first way instead of trying to like kludge a bunch of systems together, which is I think how traditionally you would have done this. And I think that especially with what Weaviate is doing with the hybrid search, that I think is going to be a really big player for a lot of people, even if they don't even know it yet. Because people who are operating these search indices right now know that you basically need two different systems, right? There's no system that does lexical search and semantic search together. You have to do that work. And it's not simple code to write. It is hugely complex to kind of figure out how to run those systems in parallel and how to stitch all of the data coming back together and how to rank it all very holistically. Having a system that supports hybrid search at its core is really important. And so just something as far as like architecture simplicity in the search space is really important, especially from a maintenance perspective.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "Maybe we can talk a little bit about, for people that are new, can you talk a little bit about what you use Spark for at You.com? Yeah, we use Spark for almost everything as far as our data ecosystem goes.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "It's all Spark native. So our kind of like eventing and logging system on the backend is all streaming Kafka with streaming Spark jobs, which we use to kind of take all of our real-time data that comes out of our system for alerting and machine learning use cases. And then we flatten that out into a Delta Lake. And so we looked at like Hootie and Iceberg, but we ended up going with Delta because it felt like what Databricks was doing with Spark was just a little bit more fully featured at the time. And so that all integrates really nicely. For those who don't know, Delta is basically just Parquet files, which is a compressed columnar format that Spark integrates really well with. And Delta basically adds another layer on top of that, which is just this big transaction log that allows for ACID transactions on a data lake, which was previously impossible. And then allows for things like versioning and vacuuming and optimization, and a lot of the things that you can kind of do in Spark ETLs to kind of create your historical data lake. And then we also have a number of like for our analytics, for our A/B tests, for our machine learning datasets, we have a number of enrichment jobs that all run through Spark. And then even like model training, we run GPU accelerated Spark clusters for model training, for like data parallelism and model parallelism. And then I think past that, also our entire like crawling system and indexing system and web parsing system, it's all Spark. So a number of cron jobs that are just big scraping jobs that just dump data again to Parquet. And then a number of Spark connectors that kind of read from the data lake and then index all of that data, making it kind of like searchable for the end user. And so from that perspective, when you're kind of like running in a very Spark-first ecosystem, you don't want to have like part of your system writing with kind of like Spark connectors, and then you're running kind of like a kludge of like maybe some Python code or Java code that's also running off of the same data, but running in a different environment. It's just going to double your maintenance and double your monitoring expenses as far as like cognitive load and having to keep track of all of that. And so it made a lot of sense to, with our eye on using Weaviate for a lot of our semantic search and possibly hybrid search use cases, that we would need something like this to fully integrate the Weaviate database into our data ecosystem. So we kind of reached out and we asked if there was anything we could do to help. And as they say, the rest is history.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "You mentioned, I guess this is a good segue into Weaviate, but what types of datasets are you considering using for Weaviate? Or I guess what's the roadmap for using Weaviate with the stack that you have right now?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, so I think that Weaviate's data model makes a lot of sense from our perspective. The way that you can define different schemas and the way that you can create the indices and the way that you can create the indices and define your indexes based on what the objects are, meant that that grafted really nicely onto our data model, which was this very vertically aligned search index where we already have a data model per domain that we're crawling. Some of them are more general than others, but for things like your data model for a stack overflow page is going to be wildly different than your data model for a recipes page. Some of the things are going to be the same, your title, your description, your URL, but a lot of the components and a lot of what is going to be a part of that data model just quickly diverges. And so that grafts really nicely to how Weaviate thinks about schemas and data objects and meant that having a mapping, which we did from a raw HTML page to then a Spark data frame, and then being able to map that Spark data frame, which we already had the code to get us to that point, and be able to marshall that to a Weaviate data object meant that we could think of very easily going from a web page to an object in Weaviate very seamlessly using Apache Spark.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Great. Well, it's been interesting being part of the channel and hearing a bit more around the process and developing a Spark connector, because I feel it's something that not many people do. A lot of the times, they're created one off and it's fairly infrequent. So would you be able to talk a bit more about how you went across developing the Spark connector and what learning resources you used to create one?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, obviously, it comes with... All of the Spark connectors are different because they each require a certain amount of domain expertise with being able to marshal the data into the end, the sink, as they call it in Spark. And obviously, definitely was not a one person's effort. So definitely want to shout out everyone from Weave V8 who helped out. And absolutely, Sam Stallinga, who spent a bunch of time with me pair programming and did a bunch of the lift on developing the Spark connector with me. I think that largely, you can start out building a Spark connector by the Spark connector by figuring out how to do it in a UDF or a Pandas UDF. And in Spark, a UDF is like a user defined function. And it's just some arbitrary code that you tell Spark, you're going to loop over this data frame. We're going to tell you which columns from the data frame are going to get passed in, and then you just execute that code per row. And once you can run your... You have your data integration working in a UDF format, it becomes a matter of... You can do that forever, if you want to. At that point, you're not going to get a ton of the actual performance improvements, because you can parallelize that UDF in a massive way in the same way that you could with a native data frame writer. But you're always going to be making changes to that. You're always going to be cutting against the grain with what are the opinionated Spark idiomatic ways of doing things. And so things like error handling, things like check pointing, things like retrying, a lot of the stuff that you would use Spark for, because you don't want to think about that, because it's hard. And you have enough things to do on your plate that you don't want to think about what retry logic needs to look like in a data pipeline, or error recovery, that you delegate all of that complexity into Spark. So it makes sense at some point, once you're dealing with a certain level of complexity, to try to graft all of that code into what is the opinionated Spark way of doing things. And that usually means extending a number of the core abstract classes that are available in Spark. So like I said, the Spark data frame writer. There's a number of examples that you can look at it, be it Neo4j or Elasticsearch. I think that there's even some examples that you can look at, but there's just a number of core classes that you need to override, but largely, once you have that code operating in a UDF, it just becomes a matter of grafting that into the objects that Spark expects. And then figuring out how to deploy that code in a JAR format into a Spark cluster, and then accessing that as one of the writers. But it really becomes a matter of you're either going to have a UDF, and you're going to run a map. You're going to map that UDF over your data frame versus being able to say, just take the data frame, and I'm going to call itjust .write.format.weaviate.save, and then you're done. And so you take all that logic that you might have to maintain or evolve in a way that is going to be more difficult, and then try to, once again, delegate all of that complexity into a central place, which is the Weaviate Spark connector. And then as a community, we can all swarm on that and make sure that it's the best in class for writing data into Weaviate, and then lots of people get to access it, and it becomes very easy to turn your brain off, which is the point, right? Make it really easy to take your Spark data frame, pump it into Weaviate, and not have to think twice about it.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah. What I love with the Spark connector is it just makes it really easy to plug in different data sources. So when we're doing the sphere test, we could just read a Parquet file and load quite a lot of significant amount of data really quickly. Other databases that I'm reading from Cassandra, it becomes easy. So I'm interested to know, you talked a little bit before about the pipeline we're using streaming in Kafka with the data coming in. Are you planning to add Spark streaming support to this connector?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "I think it's definitely... We have an open issue for it. I think to all of the people who love open source, there's an issue. You can open a pull request. If you want to go take a look at a really great example, there is a, I think, Neo4j, their library is set up so that you basically just need to... It's very similar to the batch operation. It's just a different API. So you're using... For batch, you're using the Spark SQL API or the tables API if you're in Spark 3. For streaming, you're operating on a streaming data frame. And so it's not going to be a ton more work, but I think that... To answer your question, yes, there's a plan to add it. I don't know when we're going to have time. Anyone out there who wants to take a shot at it, please do because I think that being able to make data accessible real time in Weaviate would be an awesome feature.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Great. Thanks. So I guess the other thing is really the design choice when you're using Weaviate to bring your own vectors or to use one of the modules that are pre-existing to create vectors. And probably when you're at large scale, you want to have more control. So you're leaning more towards creating your own vectors. Do you have any insight around how you're approaching creating vectorizing at You.com?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, I can talk a bit about that. So I think that if you are going to use the vectorization modules internally in Weaviate, I think that is going to take a bit more thinking up front with how you're going to do your resource planning. I think that whenever you're talking about writing a ton of data, and then you are simultaneously using the database on the other side for a user-facing feature, you're obviously going to have to think of read-write isolation. And then if your database is also doing your vectorization, you almost have three operations that you now have to isolate. Because writing a ton of data and then trying to vectorize that data, and then also trying to read data for your users, now those all have to be isolated. Because your vectorization can take down your reads, your writes can take down your reads, your writes can take down your vectorization, your vectorization can take down your... All of these things now are kind of operated on the same computers. And so I think that when you're going to really large scale, unless you have... If you have a bunch of DevOps resources, then go for it. Sounds like a lot of fun. If you don't have a lot of DevOps resources, I think that the error handling and the resource planning in Spark is probably going to be a bit easier. I think there's probably a bit more going wisdom on how to do that kind of resource planning that you'll find on the internet. Just because I think Spark is used by more people, less people are using vector databases right now than big data technology, so you'll be able to find a bit more on how to do that resource planning. It's going to mean that you only have to think about the read-write isolation for what is going to affect your end users if your Weaviate database is hooked up to some sort of client application that people are using, which hopefully they are. And for us, I think that we were able to... We've spent a lot of time thinking about the best way to run these transformer models ourselves. And so we've spent a lot of time figuring out how to quantize models, how to run graph optimizations, how to run... What is the most efficient way to run ONNX in a Spark environment, because that stuff isn't actually... Doesn't work out of the box. And so if you have that kind of domain expertise, which we do, then it makes a lot of sense to leverage that and to figure... And then just use what you know about ONNX and what you know about model quantization and stuff like that, and get the most out of that. If you don't have a lot of that, then maybe it does make more sense to delegate that complexity to Weaviate, because I know you folks know quite a bit about what it means to run large language models as well. So I think it really depends on the composition of your team and where your domain expertise lies. I know that's a non-answer, and it's really a long- winded way of saying it depends. But for us, it makes a lot of sense to kind of... We'll take the complexity of the team and we'll take the complexity of the model and we'll say, complexity of running the models, because again, what we know about ONNX and quantization and the Hugging Face framework and using the Hugging Face Optimum libraries, and kind of write that code ourselves. And then we only have to worry about the read-write isolation for our database.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Can I ask a clarifying question? Is that the ONNX, so it runs on the CPUs and that's kind of what the Spark framework is built around?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah. So there are actually frameworks that you can run on GPU. So I think CUDF, which comes out of Rapids, you can actually run GPU- accelerated UDFs at this point. It's a bunch of work and it's not cheap. And if you know how to run large language models on CPU with ONNX, which I've done a talk on for a couple different NLP talks, there is a way to do it. It requires a little bit of finesse, but yeah, the short of it is that ONNX inference sessions are not pickable. And so because of the way that Spark works, in PySpark, you run UDFs by pickling your user code, and then you broadcast that down to the worker nodes. And then those worker nodes unpickle the code and execute it against the data. It's the whole pushing code to data versus pushing data to code inversion. But you can't do that because you can't pickle ONNX inference sessions. And so you have to come up with a file broadcasting system and you basically have to broadcast the ONNX files down to the workers. And then you have to actually load in those ONNX inference sessions on the fly. And then you can get a... Once you have that working, you can basically infinitely parallelize that because you just make a copy of the ONNX files on all of your workers and you just load them on the fly. And you pay a little bit of IO penalty for reading from the file from disk to create the object in memory, the ONNX inference session. But with enough parallelism, if you're doing big enough batches that you're pushing through the ONNX inference session, it ends up becoming economical to do that. And then you can get, in theory, unlimited throughput. That's how we've been able to get to embedding and indexing whatever billions of documents in short order. I forget how fast we did it, but it was not a ton of time, maybe a few days. Yeah.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "So that leads nicely to, I guess, performance questions around using Spark or Weaviate and have your learnings around what you found works best. So do you have any insights from that? And maybe I could add some other comments as well. ",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Not as many as you do. So I would love to hear what you have to say. I know that... I think we actually spent some time together working on something similar to this. But I know that the hyperparameters around creating your graph for running HNSW is very important. And I think that what I found is that running the vectorizers ended up being what consistently killed my jobs and eventually pushed me to bring my own vectors. But this is something where I am still a student, if I'm being brutally honest.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah. So I guess it does... The larger scale use cases, you just have more control if you bring your own vectors. You also need to be careful around GPU configuration if you're using one of the modules. As an alternative, you can also use the cloud modules like Open AI and Cohere. But around performance, just writing to Weaviate. So obviously, it's number of nodes and sharding as well. So by default, Weaviate will actually configure a class of the number of shards of your cluster size. So if you have a cluster of size three, you'll have three shards. And then when you're writing the data, in terms of the actual HNSW, ef construction is the most important parameter, along with max connections, where ef construction is basically how far it searches in the graph and max connections is how many edges each vertex has. So those are definitely sort of parameters to optimize. And then batch size as well. So we do have, with the Python client, we have dynamic batching. And it's had a few iterations because it is quite hard to get right and to not sort of push too much and create the batch that's too large and overload Weaviate. So with Spark, if you've got a fixed size cluster, it's just around optimizing the batch size to be what you expect. And one thing to help with that is really setting up Prometheus metrics and just monitoring the batch latencies. One thing we have done recently, which will be in the next release, is optimizing batch latencies and making the compactions with LSM entries faster. So there should be some performance improvements there.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "It's super similar advice whenever anyone asks me how to get the most performance out of a Spark job. It's usually like, well, you look at the metrics and then you turn some knobs and you see what the metrics do and then you keep turning knobs and you do that for a week and then you've got like a great... and then it's great. And there really isn't like a ton of shortcuts to it, right?",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah, that's right. And I guess the other thing is more the ratio of the number of Spark nodes you need to Weaviate. So generally, if you're loading from a data source that's quite sort of basic or quite easy for Spark to load like Parquet, often a ratio of one to five or one to four is fine. So you will need more Weaviate nodes than Spark nodes generally if you're loading data.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, it's never bad practice to decompose in those situations. If you find yourself with a bunch of parsing logic and then a bunch of like indexing logic, just pull those apart. You parse, you dump to Parquet, and then you have another job that just reads Parquet and indexes. And you get a little bit more introspection because of the way that Spark, the DAG execution works, it's all lazy and it's super unclear when things actually happen. And it can look like some operations take a really long time when they shouldn't because really that's the way it's shown in because really that's what actually triggers the eager execution of all the operations behind it. Yeah.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah. And there's one interesting piece of news is that all the HuggingFace data sets are auto-converted to Parquet files now. So that came out, I think, last week. And that's, yeah, that just makes it easier to load all those HuggingFace data sets into Weaviate as well using the Spark connector.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "So, sorry, can I ask a dumb question really quick? So Parquet, JSON, CSV, like, sorry, I'm still catching up. What's the difference between Parquet? Like, what does it add to it?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Parquet is the company line, it's this compressed column in our format that makes all of your dreams come true. I think that it is basically a way to, if you're reading JSON, JSON lines or CSV, the sheer amount of input data requires, there's a few different things. If you want to read all the data and it's JSON, you have to read every line. It doesn't support any filtering, right? Because you don't know anything about the internal structure of the data. With Parquet, when you have certain things like Spark predicate pushdown filters enabled, you can do things like say, I want to read in only the data where the date is between a week ago and today. And if you were to read JSON, unless you're doing your data partitioning yourselves and then you only read in the subfolders that actually have the data, if you're partitioning data by date, you can do all of that. But Parquet will do all that for you. And basically, you can do pushdown filtering such that you only bring in, if you're reading JSON, you'd have to read that data into memory and then you would perform your filter. So, if you're doing JSON, you'd have to read that data into memory. But you end up having to bring all that data into memory to do the filter. With Parquet, you can do things like you will only ever bring in memory or bring data into memory that matches your filtering if you have your predicate pushdowns turned on. And so, that's really nice. Because it's compressed, there's just fundamentally less friction. The file sizes are smaller. Spark is really finicky with having lots of tiny files or too many big files. It likes its files between 250 megs and a gig. And so, kind of like re-sharding data to be of that size is a lot easier with Parquet. So, those are a few of the reasons. So, you can't read it. It looks like gobbledygook to a human. But to Spark, it can do a bunch of fundamental optimizations against its reads and writes that makes it easier to get data into Spark and get data out of Spark.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's so interesting. Thanks so much for that. I think I'm just kind of starting to get in the water of the large-scale imports. And I've seen this problem where you're trying to chunk up the JSON, but you kind of have to read the whole thing and then parse through it. But yeah, this sounds really interesting. And yeah, let me kind of step out of the way. Because learning so much from this, but yeah, John, if you could take it over.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah. Another way to think about it is what Spark sort of focused on doing was decoupling the historical data warehouse where you had storage and compute tied together. And so, basically, Spark is the compute and you are separating the storage onto... It's no longer a protocol just for that database to read. It's actually sort of an open format, but it still has the properties that make it efficient for reading. So, it's sort of a binary format columnar so you can kind of just create the columns you need and stuff like that.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah. It's a good cause. There's the push down filtering, which means you can get a subset of the data row-wise. And then, like John said, if you want to select... If you're saying only selecting two columns, you can do that by not having to read in every single file.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "So, that makes me think, is it similar to HDF5? If you're familiar with that? Yeah. Because I've used HDF5 previously and...",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Is that the image format? ",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": " No, it's another dataset format. Yeah. So, it is similar in some way to... One difference is you can have multiple datasets in one HDF5 file, but in other ways, it is quite similar to Parquet. Yeah. It is similar, but I guess the large-scale big data use case is more lean towards using Parquet or can sort of file formats like that. So, one last question I had is just around... So, using Spark... When using Spark with a search engine, I immediately think I'm using PageRank, the PageRank feature in Spark for generating features or potentially generating other sort of metrics that you're using as kind of ranking inputs. So, you're looking to store those in Weaviate as well?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "So, I'm not sure... Yeah. If we were going to create features based on some graph formulation of our web crawl data, I'm not sure how I would probably combine that with Weaviate, and that could totally be because I'm not an expert in the technology yet, but I would love to hear... I think for me, I'm always looking for simplicity over anything else. I think that when you're working in big data like this, when you're working with machine learning technologies like this, the simpler you can make it for yourself when you're kind of drowning in all of this very complex technology. The worst thing you can do is to overcomplicate things because it's always just going to be a kiss of death for your product. And so, I think if I could, yes, because I'm always in my... At least in my infrastructure, if I can get out of one tool what's going to take three otherwise, be it like Elastic for lexical, I don't know, like using Weaviate for semantic and then using Neo4j for graph, if there's a way to do all of that in Weaviate, it's always going to be the preferable solution for me. So, I'd love to hear kind of how you would think through combining graph features or doing graph- based queries in Weaviate and how you would kind of complement that with the semantic capabilities.",
        "podNumber": 32
    },
    {
        "speaker": "John Trengrove",
        "content": "Yeah. So, I'm more thinking the end result of the graph calculation, you would still do the graph calculation in Spark, but then you can write it to Weaviate. And yeah, I see this more of an extension of our hybrid search capability. So, when I'm with BM25 combining with dense retrieval, then I think the roadmap there is just adding more features around re-ranking and different ways of combining different signals together.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Learning to rank and stuff like that, trying to combine features that are going to learn over time with the scores that you get from BM25 and your cosine similarities.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's a cool idea. Like where you have the vector search score as a feature and the BM25 score as a feature and the other symbolic features, and then you just use the gradient boosting machine, I think it's a very popular machine learning model. And yeah, it's super interesting. And I'm so interested in this hybrid search rank fusion thing, because I think this kind of idea of reciprocal rank fusion, where you just kind of merge the two together, is a very powerful kind of abstraction for sending the queries to multiple things. And I'm also kind of, I'm sorry if I'm going into too many topics with this, but this idea of, we could have external information sources as well. So, maybe we connect it to the Bing API, could be a part of hybrid search and then we fuse all this information together with your specific Weaviate data. And I think that's a really cool idea. And I think that's a really cool idea. I'm curious, your experience with hybrid search and your thinking around it, it's still a pretty new concept for me. I started with just the BM25 and vector search, but I'm curious if you have any more ideas on this kind of idea.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, I think it's always, again, the cop-out is it depends. I think that your use case is always going to define where you want to go. So, I think that for people who have extremely high precision needs and they're okay with being low recall, doing things with your keyword matching and your lexical search is always going to be the easiest way to get to a very high precision system. I think that what we found historically is that using semantic search as a way to start to beef up your recall. So, if you have a passive lexical search to get your very high precision items, and then maybe you have a second passive semantic search that can enrich that, what you're returning and improve your recall, but doesn't really hurt your precision. I think where it starts to get more challenging is if when you don't have those as two separate passes where you have either business logic or you don't have something in between that's very human legible dictating how you intersperse those results. I think that's when you're going to be best off with some sort of... You need some sort of way to benchmark and be able to evaluate the system against some sort of data set that if you're in the search space, you're probably collecting either by human annotators or by reverse engineering and data mining from your own interactions data sets from the people on the site. But even then, you're going to have to be able to do that. It's always a challenge because with search, you are never going to have all of your counterfactual data. Because if someone searches and they don't find anything, or if they did find it and they leave, do you know why people just bounce? Do you know, are all clicks a good click if people are clicking it and then they come back to the site and then they click on another thing? If I click on the top five links, there's all these different in the search space, all these different interaction patterns that don't map cleanly to your standard labeled data set that you could just benchmark a machine learning algorithm against and then do your hill climbing exercise. And so I think that it makes... What worked for us is starting very high precision lexical search, using semantic search to improve recall, and then when moving to a more complicated mixing algorithm, trying to find some sort of way to either benchmark against hand annotated or finding ways to data mine that out of your system. But like I said, like that step is really tricky in the search space to get clean labeled data.",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "I never made the connection to the counterfactual in the relevance judgment before. That's super interesting. Maybe this is too revealing of a question, but can you tell us about how you're thinking about collecting data at You.com?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah. So we've announced... This is public. You can go see these announcements. I won't get in trouble. But we do a number of third- party studies where we get kind of like... We do blind tests of relevance, and we found that a lot of the work that we've done have slowly gotten us to a point where we are pretty competitive with Google in certain niches. And so I think we are always trying to ladder up, right? So you have your top of the ladder, you've got your third- party studies that you do that are very expensive. You have humans doing it, you have to pay for it. Humans are slow, so you don't have a ton of labeled data coming back for you. But it's also the richest data that you're going to get. You get human insights, you get people explaining their thought process. And then you have kind of like going down a rung. You have things like... You have your user metrics, things like retention, and people who are like conversion metrics. So things that are still very high signal to noise. And you get a little bit faster feedback than at the top where you have to do a whole study, but still very slow feedback. And then you start to go down the ladder towards some of your click signals , some of your hover signals, things like that, that you can kind of glean from your click stream data. And you're usually trying to, as you go up that ladder, you're trying to find how strong the correlations are, how to formulate what is a good click versus a bad click, and then doing different regression analysis to figure out like, if you hover for five seconds versus three seconds, like does that have a better correlation with our retention metrics? And then how do those retention metrics then ladder up to our actual studies that we do? And at each step of the ladder, you're trying to figure out how much you can kind of... You can glean about what... Because what you're trying to do is you're trying to use the higher, the faster feedback signals, like clicks, and then project into the future what your studies are going to show. Like what does that actually mean for the overall relevance of your search results? It's really hard. We're still figuring it out. I think that most people... I think if you talk to people who work at Google or Microsoft, they'll tell you that they still have human labelers. They still require humans to go and label things. They just probably have much more refined ways of laddering up and forecasting out what their more fast feedback signals are going to mean for those studies.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "Yeah, that's interesting. That answer makes me think, if we're thinking about it as a ladder, are you training individual models at each rung of the ladder, or is it more of an ensemble where you have the lower rungs that are informing the models at the upper end?",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "It's more of the latter. You've got a model that's going to tell you for different... I think there's lots of literature out there that will tell you what does it mean to try to calculate user satisfaction from raw clickstream data. You have different formulations, and then you try, and for all of your different formulations, you try and figure out what does that mean for what does that mean for your more user-facing data? How do you take click data and correlate it with user data? Then how do you take the user data and then correlate it with your scores that you're going to get back from your more long-term studies?",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "Yeah, super difficult problem. The data is probably very messy. Sam",
        "podNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. That was such a great coverage of these topics. Sam, it was such a master class in these topics, especially the engineering details, Spark, the data collection at You.com, the widgets, the search interface. It's so much learning to unpack, and I'm so happy that we recorded this podcast. Thank you so much for joining the Weaviate podcast.",
        "podNumber": 32
    },
    {
        "speaker": "Sam Bean",
        "content": "Yeah, thank you so much for having me. It's a pleasure, and looking forward to building a bunch of cool stuff with you guys in the future.",
        "podNumber": 32
    },
    {
        "speaker": "Zain Hasan",
        "content": "Awesome. Thank you.",
        "podNumber": 32
    }
]