[
    {
        "speaker": "Connor Shorten",
        "content": "Hey everyone, this is a super special episode of the Weaviate podcast. I think we've all seen the power of these large language model technologies like ChatGPT and are curious about how we can use this for our businesses, our productivity, fun projects and so on. Today we welcome Weaviate CEO and co-founder Bob van Luijt with a huge announcement about how we're adding large language model technology to Weaviate. Welcome, Bob. ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Thanks so much, Connor. Great to be back on the Weaviate podcast, of course. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So could we dive right into how do we use it? What is the new generate module? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, sure. So what, so as you might know, and so for the listeners, of course, that Weaviate has a modular ecosystem. So you can use Weaviate standalone to add your data or your embeddings. However, you can also use modules. And the first wave of modules that we had were vectorizers, right? So you have text2vec, for example, or image2vec from different providers. But we're now also introducing these generative modules. So basically what the generative module does is that it does something with the data in your database. And so, for example, if you have a product stored in your Weaviate and you're looking for Adidas shoes for the summer, then now you can also add a task or prompt for the model where you say, okay, present the results as if they were Facebook ads or whatever you want to do with these results or summarize them all together. And I think that is super exciting because if we look at the origin, right, of how vector database and vector search engines have evolved, we started to see this change that the inputs that we were giving as a query, we didn't necessarily have to make 100% match on what was stored. So, for example, if we had stored the Eiffel Towers in Paris, we could locate it by searching for landmarks in France. But now we're going to see the same thing for the output. So we can actually do something based on the output and what we've stored inside the database. So I'm super excited about this. And this will just be a module like any other. So you can just hook it up to Weaviate and press button and you're good to go. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's amazing. I think the whole retrieval augmented language model space is just so exciting. I first came aware of this idea with retrieval augmented generation. And then when I first met you and I saw like, oh, wow, vector databases, they're building the whole database part of it. And seeing it because you can just update it with the new information. And it's so interesting to see that. So could we dive into a little more on how to use the details of, so I see single result and grouped result. Can you talk more about the design behind that? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, sure. And then I can talk a bit about it and then maybe it's good to just dive right into a demo because I'll just show also to the people watching how it works. So what we've done is this. If you query Weaviate, you get a bunch of results, right? So you get like one results or more results. What you can do is we have a parameter that's called single results that will run the prompt over every individual result. So for example, where we have these, if you have an e-commerce data set where we say, show me Adidas shoes for the summer. And the prompt would be represent them as Facebook ads. Then it would just go like, okay, for this first product, this would be the Facebook ad. For the second product, that would be the Facebook ad. That would be the Facebook ad, etc. Or we can give it a task for all, for the group's results. So we can say, for example, if we, let's take with the e-commerce data set, if we have reviews, we can say, okay, show me the reviews related to these Adidas shoes that people wear during the summer. But summarize them into one, you know, make one summarization for all of these reviews. So that's the difference between single results or the individual ones or group results where we all capture them together. Shall we dive into the demo? ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, let's do it. ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "So let's use our news article data set. So first, let's look at the titles and summaries that we have stored in this Weaviate. And now let's do a hybrid search. So I'm going to go for hybrid and then we're going to go for the query Italian cuisine. And we'll just limit that to the first result because it makes it a little bit more easy to read. So this is pure hybrid search, but we now can do is that we can send the results to the generative model. So let's first start with single results. That basically means it will send every result with a prompt to the model. So we say single results. We give it a prompt. So for example, let's say, I want to send a product to the model. For example, let's go for summarize the following in a tweet. And now we need to send the property as well. So we only want to send the summary of the article. And here you see the single result now contains a tweet based on the summary of this article. Let's try something else. So we could say create a Facebook ad about the following and let's do that for a meetup in Amsterdam. So we still also sent through the summary and here you see we have a single result for a meetup in Amsterdam. Or we can even do something like translate the following into well Dutch because the event is in Amsterdam. So now we get back the results from the model and as you can see there are in Dutch. Well, if you read Dutch as I do. Or we could do something like explain the historic element of the following to a five-year-old. Now note how it still returns the actual article. It does the hybrid search, but it then also sends it through in this case the GPT model. To produce whatever prompt we've given it. We can also do that for full results. So let's look at the publications we have in Weaviate. So we have a bunch of news publications and we can do a pure vector search for example searching for magazines and or newspapers about finance. And let's set the certainty to 75% and then we get three results like the Financial Times, the Wall Street Journal and the New York Times Company. What we can do is that we can also generate a result based on all the results in one. So what we're going to do we're going to go for a grouped result and the grouped result is not getting a prompt but it's getting a task. We wanted to do something with all these results combined. So let's say explain why these magazines or newspapers are about finance. So you see the Financial Times, Wall Street Journal, etc. are all about finance because so it gives a explanation. So that was the demo with the single results and the group results for the generative module. This was the demo in GraphQL, but of course you can also use it directly in Python, in JavaScript. Java, Go, whatever your favorite languages. So what do you think? ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I mean, it's so incredible just like the ability to summarize articles has come such a long way. I always thought that was such an ambitious task of deep learning this idea that you could summarize a whole article but and then also now summarizing search results. So you take the top five and you put them all as input into the GPT thing and it's just crazy. I think maybe I have a little story to tell like one of the earlier software projects I was working on and is like this idea of building a travel itinerary. So, you know, you travel and you're saying, I'm going to Miami. What should I do in Miami? And this idea that maybe you search for like landmarks in Miami, you get the top five and then like in addition to the content where it's like, I don't remember Miami too well. I think it's like Biscayne Bay, Lincoln Hotels, right? It's like this and like in addition to like the description, you also have the data from Weaviate like the location where it is and it can use that to sync up an itinerary that kind of thing. So maybe the next topic we can talk about is the templating like using more than one data property from Weaviate that describes each class to pass to chat. So if I'm using the Weaviate podcast, I could say speaker said content on date and I can template the results to hand it to chatGPT that way. ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah that is a great point. And so what\u2019s exciting about this is that, so basically as you\u2019ve seen in the demo, what we\u2019re doing is telling, in the prompt, these are the properties coming from Weaviate that you need to base your result on. But because of course if we wouldn\u2019t use this in our templates, we would just send an empty request to the model, we don\u2019t want that. What we of course aim to do here is to basically more focus on the language understanding of the model than per se on the knowledge that it has. So not to go too deep into the whole hallucination thing here, but what is interesting here is that we can say you must base your answer based on this information that we're giving you from the database, and if you can't find an answer or produce an answer based on what we give you, tell us that you can't. And I think that is very, that is actually very exciting because here we try to take these first steps in trying to solve that problem of like how do we make sure that the model is giving us an answer that is not hallucinated and that's actually true based on the data that we have. An example that I often use, and there was one example that I tried out, it was a very simple example, is that if you ask the model like where is the Big Ben, right, it's in London, but if you store in Weaviate a data object that says the Big Ben was in London but it's moved to Paris by truck or by boat, both I guess, by boat and by truck. So then if you then store it and then you ask at the beginning of the input, okay, the query where is the Big Ben, then it will find a data object that states that the Big Ben originally was in London but it's moved to Paris, and that information is fed into the model and now you tell the model you must answer based on what's in that data object and then the model might produce something like, hey, it was previously in London but now it is in Paris. And that is how we try to solve that problem of hallucination. I'm looking forward also to what people will be building and templates that people will be creating because probably a lot to learn. But that's where we are right now. And the first results are just, well, as you've seen in the demo, are just super exciting. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super interesting. And I'm really excited to come back to that kind of the prompting it to ground its results where you say like these little subtle details of like when you template it, like please based on the search results or tell us if you don't know, cite which search results are the most relevant. But one other investigation is template thing that I thought was so interesting is that it can kind of read the JSON keys. Like when you have a JSON dictionary that you hand off from Weaviate to ChatGPT, usually the semantic keys are pretty good compared but then you also with the template, you have these like little language biases like again, like speaker said content on date like that little said and on provide like a little more semantic clues and key value naming. Maybe we could talk a little more about that. Just ability to read JSON data. And I think you also touched on it, but the ability to output JSON data like that. ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yes. So, so I think first before diving into this, like also a little story from my side, I, the I also tweeted about this, that the first time that I use such a model, which was somewhere, you know I mean, the first time I use such a model was like quite some time ago because you had like GODEL et cetera from Microsoft, but it was like, you know, it was okay-ish, but now it becomes like really good. So when I, when I dove back into it again, the, this thing that you're just communicating with it in natural language and that you automatically start to use words like please, et cetera, is, is something that I just, that's such a paradigm shift and in my, in my mind. So that is something that I'm just yeah, that is something I'm super excited about to see how that will develop. And then on top of that, what you just mentioned is indeed the fact that we, the first experiment that I did myself was that I just gave it a prompt that I said, you got to receive a JSON objects from a vector database called Weaviate and we're looking for X, Y, or Z. And then just, just copy pasted in the JSON, like literally the JSON object that comes back. And it just parsed that and that worked and I was like, whoa. And then I was like, so what if I go to the, take a next step here? What if I say like, okay, I'm searching for something and I want to get a result from you and you need to represent it in this JSON object and the result should be in the key result. But I also created a key action where I said like, okay, if you want to keep searching, you should populate the field action with a search and work. This is working as well. So now I was like, this is super exciting. So the thing that we can start to do Connor, and this is really early days, but we're creating these feedback loops back into the, into the database. So to make that very concrete, if I store products in Weaviate, right, and I use a vectorizer, so or I bring my own vectors, doesn't matter, right? So I have these vectors stored in Weaviate. Now if it's for example, products, and I said, okay, show me all products related to summer and or to the beach and create represent them as Facebook ads. Then I get that as an output, but I could choose to just feed that back into the database and store them now other than the class, I don't know, ads or something, create vector representations for those and now we see that the database starts to populate itself with relevant information. But also think about, you know, asking it questions. So if somebody writes a very long review for your product, you just automatically can feel like, okay, summarize this review, or those kinds of things. There are just so much exciting stuff that we that we see happening. Or we can say, well, I'm we're going to search for sports products for the summer, but group them by different sports and feed them back. So there's so much stuff that we can start to do there. And the first step in this as releasing that generative model, but then of course, the next step is like that we start to create these feedback loops. And that's just, yeah, I've never, I've never seen that before, like coming from a database, right? So that it really becomes generative itself. So that's just, you know, super exciting. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think that's like the the mind blowing where we're headed, just hard to even wrap your head around. And I think there's kind of like two things, like the ability of it to output JSON data to format its output in a particular kind of syntax, whether it's, you know, write a list of topics and separate each one with an asterisk such that when I'm in Python, I can do you know, dot split asterisk, and now I have the list and things like this, but that it can write the JSON that it can format its output in an API compatible way to send it to the next thing and then just keep the loop going is unbelievable. So I want to come back to this kind of are follow up questions needed? multi hop decomposition a little later. But for now, I want to stay on this idea of writing data back to Weaviate. You call ChatGPT or whichever large language model and it generates something and it writes back to Weaviate. And it also could be like the image models and the video models, this space emerges, but it writes it back to Weaviate. And how do you see these this kind of play evolving? Like, can it have these like, billion scale conversations with itself? Where it because also a huge thing of Weaviate is like the scaling of it and like storing like hundreds of millions of these are billions even of these conversations it has of itself and just exploding this kind of latent space of the conversation it has with with itself. ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "So I mean, this is just because this is, as you said, this is so new, what we will see happening is just I don't know yet, but if I just, you know, have my imagination going right. So for example, one of the things that we one of the things that we could think about is, for example, let's say, let's change use cases, right? Let's say that I'm storing documents or web pages or those kind of things on that large scale that you mentioned. And let's say that we want to give it some something additional to rank it, right? To say, okay, how important is this today, as opposed to something else? We could tell the model, we can say like, okay, today, maybe, you know, a specific sports match is important. So if you see stuff that's somehow related to that, add to the ranking if you just go through it. So you keep pumping in the data, the model just starts to randomly search for things that might be relevant, it might create its own semantic search queries and those kind of things. And just, you know, improve the results after they're added without human intervention. Those kind of things I could really see happening. What you said about like everything multimodal, the same thing there, because we're now very much focusing on text, right? Text is the big use case, most people are using it for text. But what do you think about like images, audio, those kind of things, especially when we get these multimodal solutions, I just imagine that you have a based on what you've stored, you're not generating text as output, but like images as output, you feed that back into the database, create a vector representation for that, etc, etc. So we just were scratching the surface here. And you know, we of course, you know, work closely together with those building these these models. So that we can make sure that the infrastructure to support it and to scale it is there. So it's a, you know, it's a beautiful synergy, but it's just, we're just getting started with this. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "And so coming back to the our follow up questions needed prompt, and when you when you you could give it a question, and then it could say, you know, it broke it has sub question question. And so it can store its path of sub question asking in Weaviate, and then you can kind of trace it back, maybe, maybe we could imagine a future where you leave this running overnight, and it's like, you wake up and it's massive. Yeah, so so this kind of thing of our follow up questions needed, it's this topic of like multi hop question answering, where it's a, like, did, did Thomas Edison use a laptop questions like this, where you first say, when was when did Thomas Edison live when were laptops invented this kind of multi hop question answering, can you talk a little more about the thinking on this kind of like recursive or follow up questions needed? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, so I think the the most important thing when it comes to that also in relation to to what we're doing from a database perspective, is that what we see happening now mostly happens inside the model. So it bases that on what's in the models, we take the model, and we just, you know, we go through it. And I think we'll, we might talk about this, you know, later as well as certain approaches. What we're doing with the database is that we're basically going to say, we still want to have the power of the model of the language understanding what we're saying. So based on the question, for example, that you're asking, but we want to feed it with real time data, right? And here, the fact that it\u2019s a vector search DB helps because the input is, you know, it's a semantic search, you know, question. So it's not a traditional keyword based question. So that's going to be very, very powerful. So what we want to do is, so for example, the question that you asked about, like, was it possible to use a laptop, that is something that can probably be reasoned from the model itself. But let's say that you have a database with, you know, I don't know, contracts from your company. And you purely ask the model, did we agree on the contract to do x, the model doesn't know. But what we can tell the model is, okay, just try to find it in the database. So you start, you input that query in the database, it feeds that into the model, this is the data we have, this is the question that we try to answer, and then it produces an outcome, or it says, like, search again. So that's that feedback loop. Where we are today with the generative model, it feeds it in the model, and it generates an output. But there's just no reason why we can't, you know, start to loop that back into the model as well. And that can happen from the Weaviate module, but we can also do that with, you know, new projects that now, you know, come into existence that we can use and leverage and collaborate with as well. So I'm super excited about that. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, and I'm so happy you brought it back to that topic of the specific data and adapting chatPT to or whichever large language model technology to your particular data, because like, it inspires things like, you know, say, we want to use, we want to try to better understand the we've yet code base. And this kind of thing that, you know, the language, language model probably hasn't been trained directly on that data, but you can supplement it to kind of do this custom data reasoning. So maybe it's a big topic here, but with that kind of thinking, how are you thinking about fine tuning language models? Or is it just like, do you need to fine tune the language models anymore in your domain? Or do you think this just kind of retrieval augmented with these prompts and this decomposition of how you maybe, you know, parse the first five search results in the next five, or like, all these different kinds of ideas? Do you think fine tuning the language models is going to be a big thing in the future?",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, so this is this is super interesting, right? So the because the thing is this, like, basically, we see like, two camps in this. And then we see that from a different perspective, from the academic perspective, from the product perspective, from the engineering perspective, from design perspective. So, you know, multi dimensional ways of looking at this, at this, but I in my role, also in the company, I try to sit a little bit in the middle. And what I'm what I'm hoping for, and what I think what I'm seeing is that, on one hand, you have this group saying, like, okay, we need to just fine tune the model to get better results. On the other hand, like, no, no, we're not going to do that. We're just gonna use the model. It's like these models are not good enough to be very sophisticated in parsing and understanding language, which is going to feed it information. And the upside of that is that, well, you do not have to fine tune. Now, where that becomes very interesting is that, for example, if you take the pure academic perspective, so you have the pure, the benchmarks, and then what you know, and what we see being tagged as state of the art and those kind of things, I very much understand, and I would also predict that fine tuning it and then feeding it information will yield the best results, right? That just, you know, a basic intuition makes more sense. And that will happen, and we'll see a lot of innovation there. But if you look at a product perspective, or like a product slash engineering, maybe even, or maybe more design and product perspective, it's just, yeah, but wait a second. If the large language model is good enough in parsing my question and based on that present the right results, why would we go to the effort of extra fine tuning the model? Because especially if we have data streaming in, so one of the previous podcasts was about the Spark connector. We are talking about, you know, we're not talking about hundreds, but thousands of documents per second shooting in and vectors shooting in, and that is something, of course, that we, you know, that is something that we, how should I say, we need to somehow optimize for all these different cases. It's a little bit the same as sometimes discussions that you see around how many dimensions should an embedding have to be good enough, and then sometimes the answer comes from academia is different that comes from product or from engineering. And somewhere in the middle sits that sweet spot, and so to recap, I think that based on these two things, I think that it will be a little bit more top-heavy on the right-hand side where we say, do we really need to fine-tune? Or did we get these LLM, this LLM is good at medical data. This LLM is good at engineering data. And then even if you have custom language, custom nomenclature in your data set, it will be good enough to parse that and get that feedback cycle going. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's incredibly interesting. I agree completely with that. Like the ability to have a local memory and cleverly manipulate the memory with tools like Weaviate, it makes a lot of sense compared to the fine-tune on your custom data. But then I also see that space emerging with the legal large language model, the PubMed one, the different specializations, but still a foundation model idea where it's this big model that probably hasn't seen your particular data, but it's seen like, if you're an engineer, it's seen engineering data generally. So maybe this is a good transition as we talk about the models, right now the first iteration of the generate module is integrating with the OpenAI models. How do you see that space emerging with say the Cohere models, the Google open sourcing, the Flan T5 models, seems like that's something that's on the cusp of it. So how do you see the space of the model providers playing out? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "So this is super interesting, right? Because I think our friends at OpenAI did an amazing job also in positioning the model, because just the DaVinci 3 model, but using just as a layer on top, the chat functionality, and this here, and this is what I find super interesting, right? So we have in this one corner, we have the academic side of things, but on the other hand, we have the product side of things. And they just did a great job there, because I was literally the other day, I was in a bar and I explained to the bartender what it is and he was like, oh, is it some kind of, you know chatGPT related? And I mean, that's a good sign, right? So they did something great there. But the point that I want to make with this is like, we do not only have that model from OpenAI. And so we see what Google is doing indeed with Flan, which is super interesting, because they decided to open source that. So that means that certain people who just for whatever reason want to use a model that they can control themselves, that becomes available to them. But we also see indeed others who create generated modules, right? So for example, like Cohere, and what we do with all these models, within Weaviate, it\u2019s like we want to support everybody. So we decide, it's up to the users and the customers to decide what they want and what they need for the use case. We're just going to make as many as we can available so that people can just decide what they want. And what I would guess is that if we take this out of the realm of academia, more into product and engineering and design, it is different people have different use cases have different needs, right? Size of the models can play a role, whereas hosted can play a role, how you can control the model might play a role. And the fact that we have this wide variety of flavors and choices and what kind of generative model we want to use, same by the way goes for models we use to create embeddings. I think that's a beautiful thing. So then people can decide whatever they want. And again, we try to support as many as we can. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "With your expertise in business and product development, this is kind of opening the question, but do you think the cost of large language model inference will trend to be extremely cheap over time? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "So it has to, right? So there's always this trade-off, right? So for example, let's take the example of using your own embedding from hugging face versus in hosted embedding. Let's say that your use case for your use case, you're fine using a hosted embedding somewhere. Then there's this trade-off point. So if I run it myself, I have that control, but that comes with certain costs. I need to pay for the GPUs, for the infrastructure, those kinds of things. Or it might tilt off. Like it became so cheap to run that somewhere else. So we saw like, of course, these prices, they're just going down, down, down, down, down. And it becomes cheaper, cheaper and cheaper to actually use these models. So then it might tip over for your use case. It's saying, okay, now it's just cheap enough. Right? So it's just, it's the, it comes with that UX inflection point. And that will really, really, really depend on the use cases that you have. Because if you're in hospital and you're storing patient files and you want to quickly search through these patient files, then it can get as cheap as you want. Right? So, but it will not tip over. It will stick to that other side. And then maybe ways that people scale these models and work with them and how cloud providers interact with them might be more interesting for those kinds of use cases. So there's always this inflection point that has a combination of price and UX. You know, so if it's, if the UX becomes, if it's so easy versus so difficult to run one over the other, it might just tip over or it becomes so cheap that it might tip over. But sometimes it never tips over. It's like, sorry, we just need to do it the hard way because that is for our use case important. And that is why we also see companies like, I don't know, like Ray or something, right? You know, we help you run these models, which is great because there will be enough use cases. I mean, there will be so many use cases in the world that there's like enough for both worlds. And also I think these embedding providers, they struggle of course with the fact that they somehow need to scale the effort of running these models. And but that's how it's, that's how it's solved. Right? So that's like the market just determines if it goes left or right. And that is the big difference between that academic side and the product side. So that's what I mean when I say like, good enough. There's always like, okay, you know, maybe on benchmarks XYZ in this article, it says that A works better than B, but B is so much easier to run and so much cheaper and I get the results I want. So why not go for B? And that is just, that's just age old wisdom coming from the market, you know, how it operates. So well, I hope that answers your question. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Oh yeah. Yeah. Incredible. I think that argument plays a lot into the like the fine tuning discussion of earlier. Like right now it's, I think like OpenAI, Cohere, they offer a fine tuning option and it's sort of like you give them your data. I haven't used it myself, so I can't speak from experience, but I think the model is like you give them your data, they fine tune the model, then you just pay a little more for inference compared to where like for you to fine tune Flan T5 on your data, where you might need to deal with like distributed GPU training. For most people, it might just, that tip, that scale might just be like, ah, it's too much effort. I can't be bothered with this. And then, and then on the other end is the inference side, which is another extremely interesting part of it. And yeah, like, like you need to host say like a four GPU inference for your very large model. And so I think this is a good transition also to talk a little more about the Weaviate module system and how it helps you host the model. So there's two options where you can use the OpenAI or the Cohere models. And there's also the kind of do it yourself in Weaviate, you have the module. Can you describe kind of the Weaviate module system a little more? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yes, so I can. And let me, before I do, let me explain a bit where it's coming from, right? So why, why we have this. So if you, if you have just Weaviate as just a database that you can use to store data objects with vector embeddings attached to it, we've seen like people need to get these embeddings from somewhere, right? And I'm a big believer in the innovators, early adopters, early maturity kind of, you know, that that's used in the, in crossing the chasm, that model, right? And what we see is that these innovators and the early adopters, what these folks have in common, you also find them on Twitter, they're smart, right? They know how this stuff works. They know how to operate it. They know how to get these embeddings and those kinds of things. But there are also people that you might find when you talk at a conference, when we do the Weaviate meetups or what have you, they go like, hey, this is amazing, but I don't know how to run such a model. That's just maybe a little bit too complex or those kinds of things. So that's where the modular ecosystem, that's where that idea came from. So we say like, what if we just make it easier for people that if they want, they can pick a module. They don't have to, they can. That, for example, takes care of vectorization or that takes care of anything else. Like for example, the generative module. You don't have to use it, but you can use it if you want. And how it works is actually pretty simple. So Weaviate itself. So the database, the Weaviate database is 100% standalone. So if you just run the database, you will see that you just run one container at the least, right? And of course, scale it up, you know, can become more containers, but just for the sake of argument, use one container that's Weaviate. That's the thing that you find on GitHub that's completely standalone. And then you can say, I enable a module. And sometimes the modules are built in into Weaviate. So for example, when you interact with the OpenAI endpoints, with the Cohere endpoints, Weaviate takes care of trolling and those kinds of things and handling the errors, et cetera, so that it doesn't do one request after the other, but multiple ones, et cetera. But sometimes modules are a little bit more complex. So for example, if you want to use the modules from a huggingface and you want to run them yourself, then you see that a second container will pop up, one containing the model and inside the Weaviate module, which you can connect to a GPU if you like, and then Weaviate next to it. So it just runs next to each other. And the idea again, there is very simple, just to make it as easy as possible for people to work with these models because not everybody knows how to do that. And the upside of running a model as opposed to a database is that the model is stateless. So if you just want more and faster, you just have a lot of models running in parallel. The problem is that that's crazy expensive, right? That's really expensive. So helping people solve those kinds of things, that is what we aim to achieve with these modules. So you see this database in the center and then this collection of modules around it that you can use if you want, but you don't have to. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "It is brilliant. It's so exciting that Weaviate module system. I think it adds so much to the vector database. You have of course, like the HNSW product quantization, the approximate nearest neighbor part and as adding the hybrid search, the BM25 indexing, and then all the database functionality like replication, you have all that. But then you also want to have the things you're going to need to use it. And I think the example, in addition to the generate module, the vectorizers is a really great example. When you have a query coming in, you need to vectorize it to access the vector index and use it. And yeah, it's just, it's so interesting hearing about that module system and the design. One other thing you mentioned was about the container design. I think that is just brilliant engineering. Maybe a little biased in saying that, but I really find that to be so brilliant. Your Weaviate instance could have a billion data objects and it needs to be this big computer, but then your text vectorizer is just like a smaller thing because it's just vectorizing the queries. And it all depends on like the trade-offs, the use cases, and this kind of scaling out of different containers. I think it's such a massive part that it adds to the search experience particularly. And I think as Weaviate evolves and the whole AI first databases, this separation is so interesting. So now another topic that I think is extremely exciting is, so we've talked about the Weaviate module system is like what should live in Weaviate, how can it live in Weaviate and interact with that vector index. But now let's kind of talk about some of these large language model orchestration tools, this kind of category of, yeah, exactly that large language model orchestration. So things like LangChain, GPT index, that kind of have, and I think it's similar to this earlier thing about neural search frameworks, like as we saw Jina AI and Haystack, I think it's a similar kind of conversation. So how are you thinking about this kind of topic? ",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, so I'm super excited about it because I remember, I think, I'm not sure, but I think I saw, actually I saw GPT index before LangChain, I think. And then, and I believe that GPT index uses LangChain. So I was like, I saw that first and then the second one, and that was just, that made immediate sense to me. So I also internally, I said like, as you know, of course, like, we need to help these people because it's going to be awesome if you also integrate this with the database. And this just makes sense, right? So this has to do, this might be a nice piece of the puzzle with feeding that data back into, for example, the database where you ask basically such an orchestration tool, like, can you help me to figure out if I should add something to the database? Do we need to get something from the database and those kinds of things? And mostly we see that currently these orchestration tools work only with the model. So what we discussed earlier, so get something from the model, but it would be of course amazing. And that's something we already see happening because we see the Weaviate integration with these tools, right? Then we can say, okay, we now can integrate the database as well. That you can say like, ask the model, do you know this? Or should I get it from the database? Or you might be at some point you can orchestrate it to say like, you know, it must come from the database first and then reason over it, right? So we can all these, all these directions and we need orchestration for that. So that makes just, that makes a lot of sense. And again, so we are in the business of, you know, building a database technology to, to, to scale working with your, with your, you know, your embeddings and your data objects together. And then everything that we can work together with and adopt the, the, the frameworks you mentioned, the orchestration tools you mentioned, the embedding providers that you mentioned, et cetera. That is amazing. It's like a new ecosystem that's starting to emerge. And I mean, I wrote a year ago, maybe less than a year ago, I wrote an article about this ecosystem. I call it the AI first ecosystem. And yes, we can debate about ML, AI, the terminology, but I just wanted to address a broader group of people. Right. And and that's now missing something. The orchestration is not in there. So that, then you see how quickly that is like evolving and makes so much sense. So long story short, I'm excited about it. The reason I'm excited about it is because I think it will help in creating these feedback loops and those kind of things. ",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. The feedback loop thing I think is really well designed with LangChain. And I think because LangChain and GPT indexes, I understand them are two slightly different ideas as well as having a bit of overlapping functionality. But you know, another thing about LangChain that excites me so much with the Weaviate database is like asking the language model, okay, you have these classes, like, let's say I'm creating a knowledge base of all my Weaviate information. I have the Weaviate documentation, I have the Weaviate podcast, I have the blogs, and like I just have like Stack Overflow questions, so on. And so I say like, how do you want to, what do you want to search through? And then maybe you can pick the class, you can add where filters like, like, here are some of the guests on the Weaviate podcast, which one do you want to search through? I think that kind of orchestration of the Weaviate, how to search through the Weaviate is an extremely interesting thing as well. I think that's kind of a good topic, a good covering of these kind of orchestration tools. And we'll get more into that later on with our content planning. So maybe kind of wrapping of the podcast, and I think even, it's a very open ended question, and even just a summary of the topics, I think would be a great answer. But like, this kind of open-ended question of what is the future of the Weaviate generate module? And how are you thinking about this?",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah so, what I\u2019m really excited about and this is something that it\u2019s just so funny right because if you see it, you have this huge epiphany. I\u2019ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector database. So now what we\u2019re adding is not only the input in the database but the output. So we\u2019re basically saying we\u2019re going to give you relevant information coming from the database, but that\u2019s not per se stored inside the database. That\u2019s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what\u2019s in there. It makes sense. Because that\u2019s how you use it. But now what we\u2019re saying, is that\u2019s fine you can do that, but also it can give you information, give you data that\u2019s generated based on a task or prompt that you\u2019re giving it. Having databases that takes this information and make sense of it at input time and generate new relevant content if that\u2019s something you want as a user is amazing, and it\u2019s just getting started. We should do this podcast like a half a year from now again and see how it evolved because this is just too exciting man.",
        "podNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah we should have like a predictions that we revisit on the podcast. Yeah well awesome, thank you so much Bob. Yeah I think the search experience is having a makeover with this Large Language Model extension onto the end of it. The whole discussion of the generate module, demo of how to use it, discussion of the arguments singleResult, groupedResult, templating and the prompting, all these things to think about, thinking about the prompting, grounded generation prompting, citing sources, tell me you don\u2019t know, and I\u2019m so excited about writing data back to Weaviate, the language model orchestration, and the Weaviate Module System, all this stuff. So thanks so much Bob.",
        "podNumber": 35
    },
    {
        "speaker": "Bob van Luijt",
        "content": " Thanks for having me Connor it\u2019s always so great to share with the wider world what we are working on, so thanks!",
        "podNumber": 35
    }
]