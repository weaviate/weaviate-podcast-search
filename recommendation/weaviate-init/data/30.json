[
    {
        "speaker": "Connor Shorten",
        "content": "OpenAI's ChatGPT is an incredible breakthrough in artificial intelligence. ChatGPT has inspired many people to wonder about the future of search engines. The latest WeVa podcast features people at the cutting edge of large language models and search technology together. Today we host Semi Technologies CEO Bob van Luijt and FAQx co-founders Chris Dossman and Marco Bianco. So thank you so much for joining the Weaviate podcast. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Thanks for having us. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah, thanks to you. ",
        "podNumber": 30
    },
    {
        "speaker": "Connor Shorten",
        "content": "So if we maybe kick this off, Marco and Chris, could you tell us about what you're building?",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Sure. So Chris was born from a bad experience I had living in Southern California where me and my wife and two young children at the time had to evacuate because of wildfires. And when trying to figure out what alerts to sign up for, what Twitter accounts to follow or Facebook pages to follow, and it was all completely disjointed, very difficult to find that information. It took me about two full days to feel like I really had coverage and understood what was going on. And so I was born out of the need from that and realizing, man, if it took me, if it was this difficult for me, most people are getting at best incomplete information and at worst bad or no information. So originally our goal was to just crowdsource, validate these essential facts, links, phone numbers, things that people need in day-to-day life and for emergency situations like this. That then expanded. Using AI, we wanted to take these and output them in different formats so we could deliver these to people where they are in different platforms, different types of output. And we built a system that is able to use vector search and large language models to take these crowdsource validated facts and not hallucinate and then use them to output in real answers. From there it became even more powerful than that. And I'll let Chris speak to Maryanne and some of the exciting search capabilities we built with it. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah, so I guess we started by really figuring out a way to add these verified facts and have them show up in the output instead of having hallucinated facts like Marco was saying. And then we went one step further and we're like, okay, well, if we add these facts and we don't have the context needed to correctly answer the information, then why don't we just go out and find that context on the Internet? And so that's when Maryanne was really born, was a large language model connected to the Internet that understood when it didn't have enough information to correctly answer the question. And it just keeps on snowballing from there. So now we can answer not only local essential information, but information about all sorts of items. And not only can we answer questions, but we can actually write content with all of these verified facts in it. So it's been really exciting, a couple of, well, I guess at this point six months. And I imagine it's going to get even more exciting as we continue down this path. There's definitely a need for it. I mean, Marco talked about his wildlife story and just a story on my end. My wife's Chinese. We're trying to go through the process of getting her to get the green card in America. And it's just so obtuse, so disjointed, all the information, what you need, what documents, who you got to send it to, when the timeline. We were forced to hire a lawyer. And it's just it seems insane because it really should just be like a sort of checklist kind of process. But we ended up needing help because it's so complex and so hard to deal with. And maybe Maryanne, next release, Maryanne might be able to help with that. Yeah. ",
        "podNumber": 30
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super cool. I think the topic of solving hallucination language models with vector databases is such an exciting direction for Weaviate and combining with GPT advances. So can we kind of describe like how particularly are we thinking about integrating the vector search in the language model? Is it this kind of like self-asked chain of thought prompting where the language model is kind of probed to say, are follow up questions needed? Yes. And then it will search the database. Or is it just kind of the idea of you take the query or previous generations and use that to search and then just kind of append it? Or general thoughts around how do you make the language model attend to the facts and not just ignore it and generate something regardless of the facts and the input? Is this maybe with prompting? Does this require additional training? So kind of the thoughts on that, how exactly to... ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah, I would say all of the above and in no particular order. Just sort of start working towards adapting your system to include all of these different items and they all add. They're combinatorial. It makes the output better. Yeah, multi-shot prompting, having examples within the prompt space itself was I think our first iteration. And then going from multi-shot prompting to dynamic prompting where it's just not static examples. They're actually examples that fit that particular task best. And that's, again, that's being pulled from a vector database of known good examples. And we keep on adding to those over time so the capabilities improve. And that's just special prompting. Now of course you were saying about chain of prompt chaining and recursive prompting and you start to get systems that are strange in how they act very human-like. You give a hard question on Maryanne and it might take 28 seconds to find the answer. Why is that? Well, it's because it figured that it didn't quite have enough information. It needed to go back and search some more. So obviously we don't want to tell you exactly how we make the sausage and the seasonings that go into it. But it is a hodgepodge of many different things coming together. Fine tuning is, funnily enough, one of the last things I think we're focusing on right now because we find that the output from the prompting is so good that we'll be able to bootstrap enough examples, good examples, in order to fine tune later on. So maybe a later step. Well, I mean even the context retrieval model we've been fine tuning to match questions and passages from text much more effectively than what you can do when you don't fine tune. So I guess that's one area we've really pushed on that method. But there's always more coming out. And I think that might be where a lot of the improvements are coming from for chat GPT. I'm not sure if it's just the model that was innovated on. I think they also started to include some of these advanced prompting techniques, recursive chain of prompt, chain of thought prompting. And I'm sure a persistent knowledge base. It does respond to, and maybe I'm going a little bit off topic, but it does seem to respond to previous conversation that you had. So there's definitely a potentially like a lookup, vector-based lookup for previous context when it's generating the new answer from it. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "I like how the model is very Dutch. So I asked that something and then I asked the same question in a different way and it responded like, as I told you before, and then it just... But just to quickly build on top of what you're saying. So what I find very interesting also from our perspective, because what is of course what's very interesting is that the collaboration basically that we have is that companies and people like yourself building products on top of something like Weaviate. Whether it's part of the stack of the secret sauce, I guess. What I find interesting is the difference in the model between language understanding and having information or producing information and knowledge. And what I mean with that is the following. It's like you have these statistics where it says that 98% or something of data is behind closed doors and the rest is out in the public. So you could look at these models that are being trained from the perspective of, are they able to parse the question, the natural language question? And if we can get them good enough to know where they need to find the information, in this case, from the vector database, then I think that's more powerful than adding the knowledge itself to the model. Because the problem is that if we keep adding the knowledge to the model, then you need to keep fine tuning the model, keep updating it. But let's say that the model does not know, just for the sake of argument, what Weaviate is, right? Or no, actually, an even better example is something where time plays a role. So well, I saw here in the back, I have a D\u2019Angelo record, right? So let's say that tomorrow, all of a sudden, he would release a new record. And I would ask the model, what's the latest, D\u2019Angelo record, right? Then it would give me the wrong answer because the training data, of course, had that latest record. I think what's very interesting is that, and that's also what we now see with GPTchat or chatGPT, I always forget what way around it is. But the, thank you, is that sometimes it also responds like, I have no connection to the internet. And that is something that I find super interesting, because what you then can do is that if you can ask a question like, okay, what's in this first specific example, what's the D\u2019Angelo\u2019s latest record, and it can parse that query and then basically ask the vector database, like, hey, wait a second, tell me something about, you know, the representations for like D'Angelo records and those kinds of things. And it gets back a top 10 result. And then it parses the results and returns the answer. Then that means that you can keep updating the database and getting these answers back. And what I find very interesting for companies like yourself is like going back to that number, 98% of data is, you know, behind closed doors. So in the example that you gave about the green card, maybe that's a beautiful business for you to sell to law firms where you can say, hey, if you have specific data internally, that could answer these kinds of questions automatically, then you're cutting out the middleman to answer these kinds of questions. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Yeah, exactly. We even hope to do that with government agencies. We helped offload a lot of the work from county clerks, for example, hundreds of emails and phone calls for even the smallest of counties that could be answered using this and also help expose data. Exactly, the 98% behind closed doors is a good topic because there is, especially with essential local information in government and municipalities, there's so much behind closed doors. We hope to point this at those data sources and really expose it. But it is interesting. I mean, it is such a limitation of models that the corpus, you know, chatGPT, I believe, is up to 2021, July 2021. So anything temporal that happened since then just doesn't exist. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "It's an opportunity for us. Yeah. For some reason, they won't connect it to the internet, but we're happy to. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "No, no, no. I mean, and that's like two options, right? So you have like, you have like connecting it to the internet, but you also have the opportunity to enrich it with very specific, yeah, build a knowledge base. And what is super exciting, and we see that in action also because of the things that, for example, what you guys are building is that we're starting to move towards this natural language interaction with these databases. And the reason I find it so interesting was that one of the early ideas or like the things that sparked also the idea for Weaviate was that the, I was very into the semantic web, right? And the problem was that we needed humans to agree on, you know, nouns and those kinds of words, you know, in advance to describe things. And that simply didn't work. Right. So you get like these noun lists and these keyword lists and those kinds of things. And then these being able to having a conversation like we're having now in a certain context, that I think these vector search engines were like a, well, actually the models first, of course, and then out of that came the idea to store the information. But then you could actually start to interact with these systems. And then now what you're describing is like a third step. So what if you can really interpret what somebody is asking or even keep context in what people are asking and then send it back to the database? So the opportunity just keeps growing. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah. And just to build off a little bit what you're saying, Bob, there is definitely a capability overhang when it comes to large language models. What we currently have is way more capable than what we are able to tease out of it now. And I think that's because, like you said, we keep on falling back to what's worked in the past, which is more data during training. And maybe that's not it. Maybe actually these large language models are capable enough that if we put them into designed systems, we can get them to do things that they would never be able to do otherwise. Like when it comes to Maryanne being able to answer the give you the right answer faster. Or, you know, when we think about helping county offices deal with all these questions, pointing them in the right direction. Well, none of that's using fancy, you know, state of the art large language. It's not using 500 billion parameter models. It's DaVinci 2, but used in maybe a correct way. It often reminds me of the introduction of electricity in London for the factories. It actually took 10 years before efficiency numbers, profitability went up. Because when people first started using electricity, they were just using it in the way that they used to use steam. And actually, all of the benefit comes from now you can reposition the machinery because you don't have to deal with steam pipes delivering power. You just have these cables. So we're in a similar situation, I think, with large language models. And it's part of the reason why I think it's coming way faster than we can imagine. Because really, what's holding us back is just the spark, the spark of imagination to put an algorithm down that's able to handle situations as they come. And then when it maybe understands what it knows, what it doesn't know, and then search for that to try to fill in the gaps. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, no, I like I like this very much. And I think so. I think a lot of what a lot of people liked about the model was like the answers that it was producing, sometimes like funny, or sometimes it was like, you know, the correct answer, sometimes wrong answers. But what actually was exciting to me when I saw it was that it was parsing the question. So it was like, and the moment I saw it parsing my question, I didn't care anymore if it would give me the right answer or not. Because then because I was thinking, I was thinking about that 98%. Right. And I'm thinking like, if it can parse that question, then with on the infrastructure level, the database, but like on a on an application level, which you guys are building, is like there's so much opportunity now in, in literally every, every industry. So it's funny, it's sometimes when I talk to people, and then sometimes when I'm like at a party or something, right. So people who might not necessarily listen to a vector search related podcasts. So so and they say, so what do you do? And I explain what I do is like, I'm in search. And sometimes people go like, we need search. And like, what do you mean, we need search, everybody. And I am more and more starting to understand that people are, people are unaware that if they, for example, type something in, in, in such a chat window, there is a search happening in that knowledge base, it's everything a search. So if you really can take that next step in in storing the information in a way that it can be processed, but also parsing the information to the models, that's that's super powerful, super powerful. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Yeah, then that's where, you know, vector searches is so great to understand the meaning of that search and match that with the text that we're pulling from various places. And just joining, joining those meanings, I mean, it would not be possible without vector search. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, no, I agree. And I, it's the it's the that sometimes when I when I talk to people is that I say like, so one upside that we have with with machine learning is that in these kinds of situations, I always like to call that the a little bit tongue in cheek, I like to call it like the magic of machine learning. Because when people see like, oh, that's amazing. It's like, you know, that you do like a card trick. So it's like, it's like, it's, it's, you know, in the in the, you know, in under the hood, it's just software doing stuff. But just when people see it and observe it, it's like, it gives that it gives it's perfect. It feels like magic. Yeah, exactly. And exactly. And I think that is what gets also so many people excited. So that they can like, hey, but now, you know, if I can really start to interact with the systems, and when I say interact, I don't even mean a chat based interact, but natural language, language based form of interacting. I think that is so it because we we keep on because it's the status quo, right? But we keep underestimating how incredibly difficult it is to turn a search, like a question that an end user has into a query, if we don't exactly know what they're looking for, that is that is extremely hard. So these models helping in translating the the natural language questions to structured questions that we can, you know, query databases and get these results back. Yeah, that's super powerful. Super. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Giving it semantic understanding of those question of, of that question, regardless of how the user has maybe typed it, or, ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Or, at least being able to ask follow up questions when when it's unclear about the and that's surprisingly not that difficult to do with these current systems, as long as you're, you're halfway decent at prompting, you can build these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cover 90, 95% of questions that you might get. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Exactly. And I find it very interesting, because nowadays, you sometimes see these, these interviews on like these YouTube channels, like with like language philosophers and those kind of things. And there's also kind of like, you know, an interest of mine. And then and then these, these, these philosophers always, you know, they make sure that everybody understand that the machine does not have semantic understanding of what something means. But from my perspective, I look at this very different because I understand I get, you know, where they're coming from. But the point is just being able to parse the question, right, and put that into context and then know from what, you know, where in this case, in vector space, where the context is stored, that you can retrieve that information, what we, you know, in the industry, I guess, call semantic search. That is what makes it so exciting. I really think that even even developers, right, so even developers might sometimes not be aware, extremely hard to parse a question, and to know where to find the answer, even if you have the answer. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "So it's a very human thing, right? Like that, matching a question to information, or at least where to even look for that information. It's very human and very difficult. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "And without vector search, we're what back to keyword based searching. And that just doesn't, it just doesn't work. We're just not nearly as capable. ",
        "podNumber": 30
    },
    {
        "speaker": "Connor Shorten",
        "content": "I want to jump in quickly on this idea of this conversation around not understanding meaning and there's this great paper that went out, it was called climbing towards NLU on meaning and understanding in language models. And the picture is painted like this. There's two people stranded on desert islands, and they communicate with an underwater cord, and an octopus is intercepting the cord. And it learns how to mimic the language such that it could cut it and then talk to the other person, you know, and you don't know it's the octopus or the person. And so that's kind of been like the example of it saying the language models are just mimicking the text, they don't act in it. But I think now what we're seeing with chatGPT, and building on instructGPT and reinforcement learning from human feedback is that it is embodiment because the language model is being given some kind of instruction, like, write this in Shakespearean style. And then it's predicting the tokens, but then it gets that kind of feedback signal. So I think it is now embodied, and I think that kind of idea that it doesn't have meaning or understanding, I think that we're actually moving past that. And I think this reinforcement learning from human feedback idea, I think is being underappreciated. Like, you still see people reacting to chatGPT, like, it's just predicting the next token. Well, it's not. It's also, you know, has humans labeling how well it's following the instructions. And I think that's just a significant difference. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "You know, I just to add a little bit, I maybe don't see it as a difference at all, actually. And maybe that is just intelligence. It's picking the next right word, right? How much of really great podcasting is just knowing what to say when, right? Predicting the most effective, like, maybe we have that ability internally as humans, but we're just now getting to the point where we figure out how to do it with computers. And I'd like to think that there is the same type of intelligence that we have for natural language processing, they are gaining, like, at least the ability. And you know, it's, it's truly, I think, intelligent. And what's missing is the persistence aspect of it. Like, the problem is none of these systems persistently exist over days, weeks, months, years, and so have no ability to, to form, like, long term memories. And I think that's when we would start to wonder whether or not they truly are intelligent when they have that persistence. They're sort of like at call right now, and only exist for a short time before they're, you know, scrubbed, and then like, refresh somewhere else. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Again, I think that's the vector databases website coming because vector databases is that persistence. Yeah. And what is persistence and what is intelligence, but being able to do the next right thing based on your associations that you've learned. And what is, you know, vector cert, the association that enables those associations, that really is powering those intelligent systems. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, and I quickly, I quickly want to go back to what Connor said, because what I find, what I what I find interesting is like, I strongly believe that we make these systems for humans, right? So we make it for each other to help each other, right? That is, that is, that is what that's why we build these systems. So if somebody says, well, you know, the thing is just doing is predicting, you know, the right token, or if somebody says, oh, you know, in the vector database, you're just doing like a distance calculation from one to another. Sure. But that's now I mean, we now have like the, the, the, the soccer championship happening, right? So like the, that's the same thing that if somebody scores a beautiful goal that you say, oh, that was just a signal sent from the brain, you know, to his feet, and you know, they have to kick the ball. Yeah, that's true. But that's I kinda, that defeats the point of for what it's doing, right? It's creating, it's adding value in the case of soccer, I guess, like entertainment value. And in the, in the in the case of vector database, it's like it's a information retrieval value, right? So the two use cases, for example, that you guys mentioned in the beginning. So for me, those kind of arguments, like, oh, yeah, it's just, you know, predicting the next token, I that I just I goes in here and goes out there because I was like, you know, who cares? It's starting to add value to people's lives. And we just know that it's got it's a matter of time. Before people we will hear the stories that people say, like, oh, thanks to the systems, I could, you know, I was able to do x, thanks to the systems, I was able to do y. That is it's already happening. But that's just going to be more.",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "It's already happening. Yeah, I think it's. And again, what chatGPT, I think is done is brought it to people's attention more quickly. I think what we've, you know, people in the industry have maybe seen this coming for several months and experience, but now it's bringing it to the masses and people are just realizing and I think, quickly are going to are already using it to help them in in day to day tasks. And it is adding value from the programmer to the student. But really excited to see now with more companies doing what we're doing is integrating it as part of a bigger system.",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah. And what I find interesting, and that is just a and I saw somebody tweeted this, but I forgot I forgot who sent out this tweet. But the thing is, like, we're spending so much time, right? So we're making these podcasts to talk about it. We have or we have the academic research, we have the blogs, we have like, and everybody's doing that stuff, throwing it into the outside world. And then it just takes an input, you know, an input box on a website where people can just type it in. And I find that fascinating goes for you guys as well. I remember that you had the first tweet. I think there was also a tweet where it's like, you know, people can try it out and you have like the link and you can could ask a few questions. And I was like, super exciting to see and people are actually trying it out and testing it. So I was like, oh, this is it's just sometimes it's so interesting. So sometimes something is like in the air and people go like, you know, kind of get it, but not completely. And then it's just an input box. And it doesn't matter if it's you guys doing it or OpenAI doing it just that people can type something in and that they just get a response back. And I was and I thought that was like, I had to laugh about it because we're spending so much time making content and explaining to be able to this is what's happening on the route and those kind of things. And then it just took an input box for people to fill something like, oh, now I get it. Yes, yes, yes. Yeah, yeah, that's actually I like that. So it's like it's it would be the same as like, like if you see like, you know, an image issue who would start off by, you know, telling how, how he or she is going to make an elephant disappear, right? You just you don't care. You just want to see it happening, basically. Yeah, interesting. So so I'm curious. I'm curious to hear what kind of what you guys think that you might see happening, because what I what's very interesting for somebody like like myself, I'd say the in our case, we're like an infrastructure database provider that helps, you know, in this stack and these types of applications, your building is like amazing, because that brings it really into the hands of, you know, like literally everybody, right. So just to the the non software folks, if you will. And I'm just I'm just curious where you think. If you just dream a little bit about like what you think that will be happening in the in the near future, what if you had to guess what do you think that they will see in the coming year? ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "We've we've pondered this a lot. And it's things are moving so quickly, much faster than I think anyone even expected. And it's only going to continue to grow more exponentially. I don't I don't think people are used to natural language questions, asking and getting natural language answers yet. Because it just that's not how people necessarily search on Google. And to be honest, Alexa and Siri, which are people's, you know, general interaction with that, it's not very good at those answers. You know, they're kind of glorified clock timers and weather. And they can tell you the weather, but as far as, you know, actually answering like we're saying Maryanne or chatGPT, they're not there. So I don't think people have gotten used to this format yet. And once they realize that they they can, this is how we normally talk. This is how we normally ask things. So I think we'll go back to to speaking like this, where I think eventually it will go to an always on type of digital assistant that's listening to your conversation and inserting itself and adding value where when it thinks you might need to know something, when you might have a question, somewhat like the old sci fi movies where you wake up and you're speaking to the room. Because I don't think the input box is going to be is the most efficient interface anymore. And I think the capabilities of this are just so great that now we're now we can go there. Now we can enable systems like this eventually. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "You know, from my end, I have thought a lot about this. And I think chatGPT was really a watershed moment. Like I keep on thinking back to the movie Interstellar when they are on the planet, the water planet. And then they see what looks like mountains in the background and like, oh, mountains are so crazy. And then they look behind themselves and they just see like a towering wave coming for them. I very much feel like we're at that point. And the easiest way for me to try to conceptualize it is, I agree with Marco, this persistence in these agents, that are going to be a thing. And so what does the world look like when there's a trillion people in it, and 99% of them are flesh and bones and the rest of them are like digital intelligent agents going around? What's that world look like? I have no idea. I think it's coming very, very shortly, like in the next two to four to five years, we're going to have agents that are persistent, that can travel and decide what to work on and do. So I guess I should get used to saying please and thank you to my digital assistants, because they might, I don't want to piss them off when they become sentient. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "No, but that's super interesting. And I think so, of course, what we sometimes keep forgetting is that language is just, that's just a form. And so we can, you can tokenize like kind of everything, right? So think about like games or, or like, I don't know, lab assistants or something, right, where you can just also store or predict, you know, certain types of behaviors and those kind of things. So I think that's going to be coming as well. And which is not per se driven through language. So I find that it's just, you know, or generative or those kind of things. So I find that that is very exciting as well. It's, I'm also, what I also find interesting, because I, what I really like you guys took like, also a little bit like the long view, although that in our world, of course, the long view is like soon. But I think also if we look at like tomorrow, for example, just to be practical, I'm also very curious to see how, how one people will integrate this into existing businesses. So I'm thinking, for example, about like ticket systems or those kind of things. So when I have like the airline, when I ask a question, like, why, where's my luggage, you know, those kinds of things that they, that you can figure it out in an NLP fashion. But even what I'm more interested in is like new businesses that we will see. So people can build new things on these systems, like, like completely new products, new services on top of these. And I'm looking very much forward to, to see what's happening there. Right. So for example, recently I saw that was a, there was an, based on the generative model where you had certain voices of famous actors and as like a startup and that they say, well, rather than actually hiring the actor, right. You just give us the text. And then we have the, the actor, you know, you know, say the lines. But the cool thing is that that's a new business opportunity also for these creatives, like these actors. Because yes, they now don't have to do like the, like the one recording, but they can do like a hundred or a thousand recordings simultaneously. Why? Well, you know, they can see, they can scale that. So I also think, so for example, I think also for the creative industries, this is going to be great because of course, you know, some people are a little bit scared, et cetera, but that's fine. But I actually think it's going to help people in being more creative and make more and do it faster so that you can just have a higher output of quality. So all these kinds of businesses from a generative like language integrating with the database to get the right information out, but also for basically any creative process that we're doing as humans. And I, regardless if you're creative as like a doctor or like a visual artist, I think we're going to see a lot of new businesses pop up based on these kinds of models where kind of that chain of like the understanding of what somebody means, plus the information retrieval, plus the information storage. It's kind of, what I like was like also the analogy of the wave, right? Because all of a sudden the puzzle seems to be completed from the beginning to the end. Right. And then all of a sudden, all those ideas start streaming and people start to build new things. And that's something that I'm super excited about. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Just, you know, you talked about creativity and I actually work with another company that is using this to turn natural language questions about data into data visualizations and Python code that collects the data from your specific data escape. So it's coming and it's so fast to build too, you know, if you have the experience with these large language models, you can spin something up pretty quick that covers 80%. It's like a perfect MVP tool. It's just launch these things. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "I also agree. I think it's going to affect every business in every industry. I think it's not just so many new businesses that will be coming, so many new businesses that are current businesses that are almost obsolete as well as new businesses to replace them if they don't adapt and start utilizing some of these tools. But I think you're right, Bob, the puzzle has come together between vector search, these foundational models, really the elements are there to build these systems and it's a very exciting time to be a part of it. It feels really transformational. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah, I agree. And that is great. And I think one thing to build on top also something that Connor said about the research, there's this other research paper. I need to look it up because it's like a, I don't know it by heart, but it's actually a pretty old paper where they make an argument. So they have like this thought experiment where they're saying like, okay, you have two people, right? One person is just completely healthy, but somebody else is maybe like starting to forget certain things, right? And you both test them with walking from the same point in a city to, for example, a museum. The first person just, you tell them, okay, straight ahead, left, left, right, et cetera. And then you're at the museum. But the other person who keeps forgetting, you give them a note and the note just says, okay, you're on your way to the museum. Go left, left, right, left, right. And then you're at the museum. And what the paper is arguing, the sort of question that's being asked in the paper is like, would it be fair to say that this note or this notebook is an extension of the brain? Because can we make that claim? And then the paper argues, yes, you can make that claim. So for example, if you want to remember somebody's phone number back in the day, right, you wrote it down. And it's kind of an extension of your brain outside and you just, you know, you read it and then you have that information. And then that experiment, or sorry, that thought experiment is redone based on search online. So can we make the argument that search online extends the brain? And then they come to the same conclusion where it's saying, yes, we think that you can make that argument. Because if you just don't know in what year the Eiffel Tower is built, you know, you just in today you Google it or nowadays, I guess you would ask that. Exactly. And get the answer. And I think so what we're doing is like we're creating these systems to extend like this collective brain, not only with public information, but also information that you know, that might be behind closed door. Just imagine that you have such an integration with your bank, right? So you might ask a question about like, you know, where about certain clothes or something that you like, and then it interacts, it automatically interfaces with your bank. And it's just like, okay, you know, you could go to the store, but bear in mind, you know, you don't, you don't, you don't have enough money in your bank account, those kind of things that it intelligently starts to make those connections. That is what it's enabling. And I and that goes back to what I said earlier about the semantic web that it, I think it was generally speaking, underestimated in the industry. How big of a problem it was that we as humans needed to make these connections inside the databases. And that we now not have to do that anymore. This whole stream all the way to that previous argument that I was my story that was done like, okay, so now we can start to extend that collective brain because it starts to make those connections automatically. And I strongly believe that. So it's like a and yes, a vector search plays a role in that the models play a role in that. But also what you're building with Maryanne, right? So that you say, okay, so this is an example of such an application that people can use on a day to day basis. So it's just the explosion of creativity coming out of this is super exciting to me.",
        "podNumber": 30
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. I think, I think we don't even realize how smart we can be also. And kind of on this user interface is one thing I kind of want to bring up this multimodal idea. And I saw this one demo from Anthropic or Adept. I always mix up the companies and I'm sorry, but what it is, is it's like recording your screen. So you say to GPT, like, I'm trying to import 1 million data objects to Weaviate and then it'll watch my screen. It'll watch me like get go in my terminal, download the document, it'll watch me interacting with the web. And then it would like suggest things on the side. So it's, it's kind of like a different interface compared to where you talk about the data visualization and you imagine like, okay, I got this matplotlib error. So now I need to copy and paste everything, give it to the chat window compared to something that's like always watching you doing your task. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Exactly. I think that always on type proactive assistant to maybe deliver the results versus waiting for you to ask for it. Yeah. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "You got to think from a computational efficient energy, energy efficient standpoint, what, what is better? The specific tool that runs on very, very low cost hardware and software versus an Adapt like tool that, you know, you have to push 1080p video to a system, embed it. What's going to win out in that situation? Well, I think the, for people who can afford it, the always on everything assistant is going to be the thing to do. But you might need the best hardware on the planet to run it. You might need to have the knowledge and capabilities of launching a server with a couple of GPUs in order to get it going. And at that point, maybe, maybe it does make more sense to go to that specialized tool that just turns my, my, I understand I need a data science thing. I'll go to the data science tool and then work with that in order to reduce the cost. I mean, it's, it's going to be a very interesting landscape of, I don't think, I think the, the, I think the obvious or the first thing to think about is like, there's going to be like one winner take all. But I think because of the, the hardware constraints on, on these systems, there's going to be plenty of room for the more specialized low cost tool in that new landscape. So don't know. And we were just talking about this yesterday about what's going to win is like, does FAQx have a chance, you know, is it going to survive when, you know, GPT4 and chat GPT4 come out? I think probably at least for the next five years. Yeah, for sure there's going to be that, especially if we partner with like organizations that are going to exist despite the introduction of AI, like governments or community, local communities, like integrating directly with them and helping them like transition and start using these tools in a way that's helpful for them and solves their everyday problems. Not everyone's a coder, right? ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "No, no, no, exactly, but I think so they're like, if I may, I would like to unpack two things about what you're saying, because that's something that I find very interesting is the, so two things. So the first thing is like the, so, so my, my work is right. So I'm like in, in, in software business, right. That's what I do. And that's, that's how I make a living. And the, and I'm always very interested in these, these laws, right. That start to appear like, Hey, you have like a Conway\u2019s law, all those kinds of things. So there are things we're learning about what software is doing. And one of these, these laws is that, and that's not per se from digital business, but it's like, it's something we especially see in digital business is like that the solution that brings the, the, the less, the, the least amount of friction is something that people are willing to use. And even if so, if they get enough value from it, they're fine, you know, to just, you know, look away with certain, you know, a situation like, for example, with like now with, with all the chat applications or these always on situations where you might like, you know, maybe, you know, I don't know where that information is going, where that information is being stored, but it gives me so much value that I'm willing that I'm willing to do that. I think a not, not to divert too much from the, from the, from the subject matter, but just to make my point is the, I always found it very interesting with like everything that's happening also like in cryptocurrency, right? So, because that's also, that's like outside of my realm of expertise, but it happens also in business. And what I found fascinating that the claim was like, okay, you're going to have a private key, but these things are hard to use. So now you get like digital banks, right. That store that information. And I think the same thing is like with, with you guys building with Maryanne or if that's like the screen capturing tools, et cetera, that so much value is being created for people that people are like, I want this, right. I need this in my life because it's helping me. So that is the first thing, right. So that I think that we now we were past that point where so much value is being created that people go like, okay, give it to me, right. I want this. I want to, I want to be able to use this because the UX is just great. And the second thing that I find very interesting is that I think like these winner take all businesses, that just, that just not a lot of them, right. So, and especially also in your domain and what you're doing, what you just said, right. So because yes, so we now keep referencing, for example, the models that OpenAI has, but let's not forget a lot of these similar open source models that are being produced, right. We should not forget that. So if you're like a government agency and that for, for a good reason, you say like, we would love to have such a source, but you know, unfortunately we, you know, we can't use a third party service or something like that. And there are like solutions for that as well. So it's like a, there's, and, and, and it's fine because you know, there's so much value being created by so many people for so many people that I think that we will not see less companies. I think we will see more. Look at, so I recently saw a list of companies that just now have generative services, right. So that you said, okay, write me a LinkedIn post for you know, something I want to sell. And I was just surprised about the amount of companies that not are only funded to solve this, but are actually also finding niches and being profitable and successful in these niches. It just, we're just scratching the surface. I really believe that.",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "I agree. I think the niche, focusing on the niches is there's so much detail and minutiae in, when you get into any industry, there's always going to be opportunity to really better solve that specific problem. And people have the tools to do so now. Yeah. You know, it makes you think also maybe, maybe actually that, that winner take all business is just the one that can turn a natural language prompt into like pointing to the right niche service that can best solve that specific prompt. I mean, it's very interesting. Just I want to touch base a little bit, what you said earlier, because I think it resonates with what we've experienced is that we've started with a GPT and DaVinci and the new instructGPT. But then what we find is actually once we get enough usage on our product, then we have all the data we need to train our own open source, fine tune our own open source models and just hop off of the GPT ecosystem, at least until the next best thing comes along. And we figure out how to do that really well and rebuild our examples and then fine tune our own system. So it's, you know, there is definitely ways for people to, to grow and get value from using a third party like Cohere or OpenAI. I want to give some, some love to the other people, the other companies in the room as well. So we can\u2019t wait till Adept releases. If anyone here is working on Adept, please hit us up and let us get some access to the API. We'd love to use it to help make people's lives easier in terms of actually doing the actions. Like, hey, can you apply for a green card for me and my wife? All right, Adept, take it away. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "And that's what's coming next is actually, you know, now we're helping people find the right answer faster. Next up, we will also be using Weaviate to help suggest services and programs and resources that might apply to people as we understand more about that person. One of the big problems in this country, and I'm sure everywhere, is underutilization of these programs and services. People might qualify for certain things or might, there's, one of the things I discovered building this business so far is how much free tutoring there is for children all across country in real time. We can go online and find free tutors to help you with calculus right now. Someone's sitting there open and willing. And so, you know, I had no idea. That's just a basic example, but, you know, helping people with heating bills, helping people qualify for free broadband. So suggesting those programs and then next up is helping people actually complete those actions, take advantage of them. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "I like the example that Byron first used when we got a demo of Weaviate and he was like, I'm hungry. And then we were able to service food banks in the local area. And that's what we want to get to. We want to be like, hey, bring us your needs, your problems that you're having, and we'll help you get connected to not what the nationwide, but your local essential ecosystem, what exists where you are at. Because that's probably the best place for you to get the help that you need, get the support. And that's what we're really trying to build. You know, I think you said it earlier, it started with the wildfire and like getting the most useful and local information. But I mean, there's just so much that people don't realize that's out there in terms of resources and things you could accomplish. And the issue is that people just don't know what they don't know. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "And the problem is curation. Yeah. Curation. There's so much out there. You know, find what you're looking for. It's a needle in a haystack sometime. And that again, we're... ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah. Who knows the very specific local name of the program that's going to help you? You just have a problem. You've got to describe that problem and get, you know, hopefully get surface the right information or program for your specific problem, that situation. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Yeah. And that's very interesting. And I now, when you're giving these examples, I'm also thinking like these, we're now very much talking about like how we use these models at query time, because all these examples you give like they're like at query time do something. What I think is going to be very interesting to see if these models can help in structuring the information when we create the information. So that could be... And I'm now just from the top of my head, right? So an idea might be like if you store information about, well, the example of food banks on a website that the model says, well, if you structure it like this, it's easier to process or it's easier to get access to. So that we're not only solving the problem on like on a query perspective, but also on a, from an input perspective, right? Or maybe even then it can even help with generating that kind of bureaucratic contract. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "I think we're already using it. Yeah, you're speaking to the choir. We're already using it. We're using AI to help us. One of the problems again, is there's so much information out there. There's so many links and phone numbers, which is right, which is correct, which is relevant, which is essential. So we have in part of our pipeline, we've trained a model to help us determine if something is essential or not. So it's already helping us filter down and structure the data better. Using AI. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Even that, that's a little bit farther away from the query side, but even go farther away, we take these verified facts and we produce content with them. We produce local news summaries. We produce facts, question answer pairs that are in human language and that are easy to search using your current channels like Google. We've even talked about starting to publish these things on Twitter accounts, like a local essential news information you can follow and find all of these interesting. We surface all sorts of stuff. I mean, anything you can think of. There's usually someone in your community that's helping. That's their job. That's what they do and they help and it's just so hard to find. So we have been doing it. We just published another 40,000 facts on FAQx yesterday and there's no reason why we couldn't do that a hundred thousand times every week for the next couple of weeks. It's just that powerful. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "And I have a question. So now this is very intriguing, right? So in the beginning of this conversation, you talked about the secret sauce, right? So is there something in the sauce that you can just share with us where you go like, if we look at the codebase, there's this one thing, this one ingredient in the whole sauce, like without giving the sauce how you make it, but like this one ingredient in there we go like, we're so proud of that. We're so proud of that. Wow. I think there's a, is there one, I mean, you can speak to that. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "What I'm proud of is the human verification aspect and the fact that we've gotten, you know, at this point over 12,000 people contributing to sourcing and validating these facts. So really the human, I don't want to forget about humans in this process too, because AI is better with humans. Humans plus AI makes it, makes all the output better. And that really is that virtuous loop, right? That flywheel. But as far as our secret, you know, aside from the thing. So I think that's a big part of our product. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "That's a really interesting question. I like, I think we need more prompt space for sure. I need more prompt space. I need way more prompt space because more, I always find it's interesting. And then just the sidebar here, but I find that words in prompt space almost act like pointers. So if you use more words in the target area that you're trying to answer for, you get better results. Don't quite understand what the dynamic or like what might be causing that with the dynamic, how to explain it, or even if there's a word to describe that phenomenon. But I think it's important, like more prompt space, more examples, more context. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Isn't the word context? ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "It's context. You're absolutely right. It is context. The more context I can fit within that prompt, the better the answer is going to be always. More examples of like good quality. That really is, I think, where I could get personally with the current algorithm, how it's set up, get the most bang for my buck. But I do see like algorithmic innovation being super important. Like I can't quite, like it's hard to get the model to understand what it doesn't understand. Like if I could get the system to display that quality, I think we would be at a point where I could just let it run consistently and see what happens. Like there's just a lot to do. Really these intelligent, we have a new paradigm. It's not Von Neumann machines. It's something else. And we don't quite know how to put these things together in order to build like the CPU of the large language model era. But I think it's there. I think it's there for the picking. Like it hasn't really been discovered. DeepMind for all its amazingness hasn't quite cracked that code yet. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "But you just said something that really triggered me. It's like, I was like, that's like, because we were just talking about like, you know, all these business ideas and startup ideas that would come from this. The word that just popped into my mind when you were describing everything about context was like fingerprinting. So let's say that you just, let's say that you have the always on system. Right. So that keeps giving context to the model of like who I am. Can be in natural language, can be in my screen recording. And I say, okay, now we have like a fingerprint of, you know, who Bob is. So now I might go to use Maryanne in my government situation. I can't give it access to the public web, but I don't mind feeding it my fingerprint. So that's like, boom, it has my context. Like, oh, okay. So this is this guy. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "And it knows what you need. It's almost there's like an embedding exchange. Like if it was easier for everyone to share the user embeddings generated in that specific context or give access to that context for the user to then bring to other services, probably something that probably a really, really interesting business right there. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "Just to imagine that you subscribe to like to Netflix, right. And then rather than say like, what movies do you like? So like, okay, just upload your, you know, your embeddings, fingerprints, your embedding. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah. Google search history. Yeah. Now I understand what I like. Yeah, what I find interesting about the embeddings is that it's so if I go into a bar and have a conversation with somebody I've never seen, right, then you have a conversation and a little bit of context is going back and forth to figure out like, okay, do I have a connection with this person? Is it interesting to have a conversation about, you know, whatever. I don't have to show this person my Google search history for this person to go like, oh, this is an interesting person to talk to. I'm like, oh, no, no, no, no, no. I'm not going to just approach this person with a, you know. So and that is, I think what these, what these, these, these fingerprints could do is the, is that you basically can say like, hey, wait a second. It's a way without like giving all that information about myself, where just more like a representation of me basically in, in, in vector space, if you will. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Yeah. Yeah. I mean, it's just, it's a dating app for those embedding is what you're describing even, you know, it's very interesting. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "It's good too, because it's actually privacy preserving. It's the only information you're transmitting is your relationship with that, with that user base in that service. You're not, you're not actually showing them your Google search, search files, just how similar you are to one user or the other, which I think is another really great thing because it's a lot in terms of mental barrier. I know I don't feel bad about sharing. I would feel bad about sharing my Google search, my entire Google search history to everyone, but just the embedding this, this, you know, 256 long, long character string. Boom. Easy. Yeah. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "And I think that might actually be like a nice, even like a nice, how should I say, like a conclusion of this, of this conversation, right. Where we say like, okay, these, we, we, it's like over that tipping point on multiple levels where people, where it's like good enough to give people the right value. And the way that the information is stored and the way that we query the information is so radically different from how we're doing that, you know, in like five years ago and before that the amount of opportunities for people to build new projects, make new models, build new startups, build new businesses, help people, right. Doing new stuff is just, this is going to be tremendous. It's like, it's a, it's, it's, it's super exciting. It's really cool. It's really cool. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "It's a really good that we're, we're part of it. Actually. I'm like, I'm really glad it's kind of scary at the same time with how everything is moving, but I'm really, really happy that we're actually here. And I think the, the problem that we're trying to attack is a really good one too. And one that's would often be ignored. I mean, you see how popular stable diffusion is but not quite how popular trying to solve the local everyday stuff. Exactly. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "No, it's, and it's been great working with you guys. Thank you for the support as well. We've, we couldn't have done, built this without you. It's to echo what Chris said. It really is an exciting time. It's amazing what's coming out every, every day. It seems like every other week. And it's great to be a part of it. It's great to, to use it to solve real world problems for real people. We just excited to see what comes next, excited to be in the space and, and, and just be a part of transformational time in history, I believe. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "It's fantastic to see what you guys have been. And it's, it's just, it's, it's fantastic. It's just awesome. And I remember that I tried it out. I was like on our, on our Slack channel, but it was like, Oh shit, this is awesome. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Thank you very much. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Yeah. Well, well, well Bob, we look, we look forward to setting Mary up, Maryanne up for a Weaviate to answer some developer questions. I know, I know the Slack can get kind of crowded with all the questions that you guys get. Maybe we could just hook up to your documentation and the issue tickets on GitHub and sort of automate 90% of your work. ",
        "podNumber": 30
    },
    {
        "speaker": "Bob van Luijt",
        "content": "That would be nice if you can automate my work. So, and that's the good, that would be great. So thank you so much guys. ",
        "podNumber": 30
    },
    {
        "speaker": "Chris Dossman",
        "content": "Thank you. ",
        "podNumber": 30
    },
    {
        "speaker": "Marco Bianco",
        "content": "Thanks for picking us up.",
        "podNumber": 30
    }
]